{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Online_Patient_Conversation_Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranildchandra/online-patient-conversation-classifier/blob/master/src/Online_Patient_Conversation_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wEK5O19MvqJy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Colab Notebook available @ \n",
        "https://github.com/indranildchandra/online-patient-conversation-classifier/blob/master/src/Online_Patient_Conversation_Classifier.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "grnSzC9KAlsI",
        "colab_type": "code",
        "outputId": "4a294206-04fe-42b9-a5b5-1ca81e0686ae",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1d47709-542f-47de-bff3-f105042c4a1f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c1d47709-542f-47de-bff3-f105042c4a1f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n",
            "User uploaded file \"test.csv\" with length 1174648 bytes\n",
            "User uploaded file \"train.csv\" with length 2378727 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GCpDU2Y977ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import all libraries/dependencies\n",
        "from tensorflow.contrib import learn\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aML-Cz5AtlAd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_dataset():\n",
        "\ttrain_data_df = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")\n",
        "\ttest_data_df = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "\tpositive_train_data_df = train_data_df[train_data_df['Patient_Tag'] == 1]\n",
        "\tpositive_train_data = positive_train_data_df[['TRANS_CONV_TEXT']]\n",
        "\n",
        "\tnegative_train_data_df = train_data_df[train_data_df['Patient_Tag'] == 0]\n",
        "\tnegative_train_data = negative_train_data_df[['TRANS_CONV_TEXT']]\n",
        "\n",
        "\ttest_data = test_data_df[['TRANS_CONV_TEXT']]\n",
        "\n",
        "\tfor index, row in positive_train_data.iterrows():\n",
        "\t\tfile_handler1 = open(\"patient_conversation-positive.txt\", 'a', encoding='utf-8')\n",
        "\t\tfile_handler1.write(str(row[0])+\"\\n\")\n",
        "\t\tfile_handler1.close()\n",
        "\n",
        "\tfor index, row in negative_train_data.iterrows():\n",
        "\t\tfile_handler2 = open(\"patient_conversation-negative.txt\", 'a', encoding='utf-8')\n",
        "\t\tfile_handler2.write(str(row[0])+\"\\n\")\n",
        "\t\tfile_handler2.close()\t\n",
        "\n",
        "\tfor index, row in test_data.iterrows():\n",
        "\t\tfile_handler3 = open(\"patient_conversations-test.txt\", 'a', encoding='utf-8')\n",
        "\t\tfile_handler3.write(str(row[0])+\"\\n\")\n",
        "\t\tfile_handler3.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rKl1C_qt8_mv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TextCNN(object):\n",
        "    #A CNN for text classification. Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    \n",
        "    def __init__(\n",
        "      self, sequence_length, num_classes, vocab_size,\n",
        "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        # Embedding layer\n",
        "        with tf.device('/gpu:0'), tf.name_scope(\"embedding\"):\n",
        "            self.W = tf.Variable(\n",
        "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "                name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # Calculate mean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyZhqN7h8oZe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_str(string):\n",
        "    # Tokenization/string cleaning for all datasets.\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\\\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\\\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\\\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wDmMFGvz8sM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data_and_labels(positive_data_file, negative_data_file):\n",
        "    # Loads data from files, splits the data into words and generates labels. Returns split sentences and labels.\n",
        "\n",
        "    # Load data from files\n",
        "    positive_examples = list(open(positive_data_file, \"r\", encoding=\"utf8\").readlines())\n",
        "    positive_examples = [s.strip() for s in positive_examples]\n",
        "    negative_examples = list(open(negative_data_file, \"r\", encoding=\"utf8\").readlines())\n",
        "    negative_examples = [s.strip() for s in negative_examples]\n",
        "    # Split by words\n",
        "    x_text = positive_examples + negative_examples\n",
        "    x_text = [clean_str(sent) for sent in x_text]\n",
        "    # Generate labels\n",
        "    positive_labels = [[0, 1] for _ in positive_examples]\n",
        "    negative_labels = [[1, 0] for _ in negative_examples]\n",
        "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "    return [x_text, y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wNqW-xYN8tLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "    # Generates a batch iterator for a dataset.\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJNsb9dV_Qdt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]  \n",
        "    print(\"\\nDeleting Following keys...\")\n",
        "    for keys in keys_list:\n",
        "        print(keys)\n",
        "        FLAGS.__delattr__(keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WAsQsnmUMHlb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]  \n",
        "    print(\"\\nCreated Following keys...\")\n",
        "    for keys in keys_list:\n",
        "        print(keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLnnEfpx94eU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "bfd3c048-f188-496a-8f8a-eaf6b80d83f0"
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Delete all flags before declaring\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "# Data loading params\n",
        "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
        "tf.flags.DEFINE_string(\"positive_data_file\", \"patient_conversation-positive.txt\", \"Data source for the positive data.\")\n",
        "tf.flags.DEFINE_string(\"negative_data_file\", \"patient_conversation-negative.txt\", \"Data source for the negative data.\")\n",
        "\n",
        "# Model Hyperparameters\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
        "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
        "\n",
        "# Training parameters\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "tf.flags.DEFINE_string(\"model_path\", \"\", \"Path where the model is saved.\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "\n",
        "# Print all flags after declaring\n",
        "print_all_flags(FLAGS)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Deleting Following keys...\n",
            "dev_sample_percentage\n",
            "positive_data_file\n",
            "negative_data_file\n",
            "embedding_dim\n",
            "filter_sizes\n",
            "num_filters\n",
            "dropout_keep_prob\n",
            "l2_reg_lambda\n",
            "batch_size\n",
            "num_epochs\n",
            "evaluate_every\n",
            "checkpoint_every\n",
            "num_checkpoints\n",
            "allow_soft_placement\n",
            "log_device_placement\n",
            "model_path\n",
            "h\n",
            "help\n",
            "helpfull\n",
            "helpshort\n",
            "\n",
            "Created Following keys...\n",
            "dev_sample_percentage\n",
            "positive_data_file\n",
            "negative_data_file\n",
            "embedding_dim\n",
            "filter_sizes\n",
            "num_filters\n",
            "dropout_keep_prob\n",
            "l2_reg_lambda\n",
            "batch_size\n",
            "num_epochs\n",
            "evaluate_every\n",
            "checkpoint_every\n",
            "num_checkpoints\n",
            "allow_soft_placement\n",
            "log_device_placement\n",
            "model_path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rg_V-uaUAD6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess():\n",
        "    # Data Preparation\n",
        "    # ==================================================\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    x_text, y = load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
        "\n",
        "    # Build vocabulary\n",
        "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "\n",
        "    # Randomly shuffle data\n",
        "    np.random.seed(10)\n",
        "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "    x_shuffled = x[shuffle_indices]\n",
        "    y_shuffled = y[shuffle_indices]\n",
        "\n",
        "    # Split train/test set\n",
        "    # TODO: This is very crude, should use cross-validation\n",
        "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
        "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "\n",
        "    del x, y, x_shuffled, y_shuffled\n",
        "\n",
        "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "    return x_train, y_train, vocab_processor, x_dev, y_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2kAtq3wNlPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def restore(sess_var, model_path):\n",
        "    if model_path is not None:\n",
        "        if os.path.exists(\"{}.index\".format(model_path)):\n",
        "            saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
        "            saver.restore(sess_var, model_path)\n",
        "            print(\"Model at %s restored\" % model_path)\n",
        "        else:\n",
        "            print(\"Model path does not exist, skipping...\")\n",
        "    else:\n",
        "        print(\"Model path is None - Nothing to restore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_EztpiM9_W6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
        "    # Training\n",
        "    # ==================================================\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        session_conf = tf.ConfigProto(\n",
        "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
        "          log_device_placement=FLAGS.log_device_placement)\n",
        "        sess = tf.Session(config=session_conf)\n",
        "        with sess.as_default():\n",
        "            cnn = TextCNN(\n",
        "                sequence_length=x_train.shape[1],\n",
        "                num_classes=y_train.shape[1],\n",
        "                vocab_size=len(vocab_processor.vocabulary_),\n",
        "                embedding_size=FLAGS.embedding_dim,\n",
        "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "                num_filters=FLAGS.num_filters,\n",
        "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "            # Define Training procedure\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "            # Keep track of gradient values and sparsity (optional)\n",
        "            grad_summaries = []\n",
        "            for g, v in grads_and_vars:\n",
        "                if g is not None:\n",
        "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format((v.name).replace(\":\",\"_\")), g)\n",
        "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format((v.name).replace(\":\",\"_\")), tf.nn.zero_fraction(g))\n",
        "                    grad_summaries.append(grad_hist_summary)\n",
        "                    grad_summaries.append(sparsity_summary)\n",
        "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "            # Output directory for models and summaries\n",
        "            timestamp = str(int(time.time()))\n",
        "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "            print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "            # Summaries for loss and accuracy\n",
        "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "            # Train Summaries\n",
        "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "            # Dev summaries\n",
        "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "            if not os.path.exists(checkpoint_dir):\n",
        "                os.makedirs(checkpoint_dir)\n",
        "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
        "\n",
        "            # Write vocabulary\n",
        "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "            # Initialize all variables\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            def train_step(x_batch, y_batch):\n",
        "                \"\"\"\n",
        "                A single training step\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                  cnn.input_x: x_batch,\n",
        "                  cnn.input_y: y_batch,\n",
        "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
        "                }\n",
        "                _, step, summaries, loss, accuracy = sess.run(\n",
        "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "            def dev_step(x_batch, y_batch, writer=None):\n",
        "                \"\"\"\n",
        "                Evaluates model on a dev set\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                  cnn.input_x: x_batch,\n",
        "                  cnn.input_y: y_batch,\n",
        "                  cnn.dropout_keep_prob: 1.0\n",
        "                }\n",
        "                step, summaries, loss, accuracy = sess.run(\n",
        "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                if writer:\n",
        "                    writer.add_summary(summaries, step)\n",
        "\n",
        "            # Generate batches\n",
        "            batches = batch_iter(\n",
        "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "            # Training loop. For each batch...\n",
        "            for batch in batches:\n",
        "                x_batch, y_batch = zip(*batch)\n",
        "                train_step(x_batch, y_batch)\n",
        "                current_step = tf.train.global_step(sess, global_step)\n",
        "                if current_step % FLAGS.evaluate_every == 0:\n",
        "                    print(\"\\nEvaluation:\")\n",
        "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
        "                    print(\"\")\n",
        "                if current_step % FLAGS.checkpoint_every == 0:\n",
        "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "                    FLAGS.model_path = path\n",
        "                    \n",
        "        #restore(sess, FLAGS.model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHpAN1gXIV77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model_path):\n",
        "    # Delete all flags before declaring\n",
        "    del_all_flags(tf.flags.FLAGS)\n",
        "    \n",
        "    tf.flags.DEFINE_string(\"test_data_file\", \"patient_conversations-test.txt\", \"Data source for the test data.\")\n",
        "    tf.flags.DEFINE_string(\"checkpoint_dir\", model_path, \"Checkpoint directory from training run\")\n",
        "    tf.flags.DEFINE_boolean(\"eval_train\", True, \"Evaluate on all training data\")\n",
        "    tf.flags.DEFINE_integer(\"batch_size\", 2, \"Batch Size (default: 64)\")\n",
        "    tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "    tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "    FLAGS = tf.flags.FLAGS\n",
        "    \n",
        "    # Print all flags after declaring\n",
        "    print_all_flags(FLAGS)\n",
        "    \n",
        "    if FLAGS.eval_train:\n",
        "        # Load data from files\n",
        "        test_examples = list(open(FLAGS.test_data_file, \"r\", encoding=\"utf8\").readlines())\n",
        "        test_examples = [s.strip() for s in test_examples]\n",
        "        # Split by words\n",
        "        x_raw = [clean_str(sent) for sent in test_examples]\n",
        "    else:\n",
        "        x_raw = [\"I think I am suffering from cold and flu\", \"I am really loving this problem\"]\n",
        "        \n",
        "    \n",
        "    \n",
        "    # Map data into vocabulary\n",
        "    vocab_path = os.path.join(FLAGS.checkpoint_dir, \"./../../\", \"vocab\")\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
        "    x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
        "    print(\"\\nEvaluating...\\n\")\n",
        "    \n",
        "    # Evaluation\n",
        "    # ==================================================\n",
        "    checkpoint_file = tf.train.latest_checkpoint(os.path.join(FLAGS.checkpoint_dir, \"./../\"))\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement, log_device_placement=FLAGS.log_device_placement)\n",
        "        sess = tf.Session(config=session_conf)\n",
        "        with sess.as_default():\n",
        "            # Load the saved meta graph and restore variables\n",
        "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "            saver.restore(sess, checkpoint_file)\n",
        "\n",
        "            # Get the placeholders from the graph by name\n",
        "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
        "            # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
        "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
        "\n",
        "            # Tensors we want to evaluate\n",
        "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
        "\n",
        "            # Generate batches for one epoch\n",
        "            batches = batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
        "\n",
        "            # Collect the predictions here\n",
        "            patient_tag = [\"Patient_Tag\"]\n",
        "\n",
        "            for x_test_batch in batches:\n",
        "                batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
        "                patient_tag = np.concatenate([patient_tag, batch_predictions])\n",
        "       \n",
        "    patient_index = [\"Index\"]\n",
        "    patient_index = np.concatenate([patient_index, np.arange(1,len(x_raw)+1,1)])\n",
        "    \n",
        "    # Save the evaluation to a csv\n",
        "    #predictions = np.column_stack((np.array(x_raw), patient_tag))\n",
        "    predictions = np.column_stack((patient_index, patient_tag))\n",
        "    out_path = os.path.abspath(os.path.join(FLAGS.checkpoint_dir, \"../../../../\", \"prediction.csv\"))\n",
        "    print(\"Saving evaluation to {0}\".format(out_path))\n",
        "    with open(out_path, 'w+', encoding=\"utf8\", newline='') as f:\n",
        "        csv.writer(f).writerows(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nCpdgHGzANDk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(argv=None):\n",
        "    generate_dataset()\n",
        "    x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n",
        "    train(x_train, y_train, vocab_processor, x_dev, y_dev)\n",
        "    test(FLAGS.model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwx9CPSnAOE6",
        "colab_type": "code",
        "outputId": "b127be79-fe51-406c-e54a-473e7dbd7426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4270
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    tf.app.run()\n",
        "    os._exit(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "WARNING:tensorflow:From <ipython-input-26-0300d2b7e644>:11: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "Vocabulary Size: 24377\n",
            "Train/Dev split: 1042/115\n",
            "WARNING:tensorflow:From <ipython-input-19-51e991986866>:72: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Writing to /content/runs/1548129893\n",
            "\n",
            "2019-01-22T04:04:57.599596: step 1, loss 2.73473, acc 0.5625\n",
            "2019-01-22T04:04:58.317157: step 2, loss 2.1865, acc 0.609375\n",
            "2019-01-22T04:04:59.000167: step 3, loss 1.54623, acc 0.78125\n",
            "2019-01-22T04:04:59.692981: step 4, loss 1.90169, acc 0.703125\n",
            "2019-01-22T04:05:00.376787: step 5, loss 1.71398, acc 0.6875\n",
            "2019-01-22T04:05:01.064323: step 6, loss 2.03938, acc 0.6875\n",
            "2019-01-22T04:05:01.756270: step 7, loss 2.04283, acc 0.625\n",
            "2019-01-22T04:05:02.439636: step 8, loss 1.55679, acc 0.65625\n",
            "2019-01-22T04:05:03.124992: step 9, loss 1.90697, acc 0.625\n",
            "2019-01-22T04:05:03.819561: step 10, loss 2.10046, acc 0.703125\n",
            "2019-01-22T04:05:04.506184: step 11, loss 1.23834, acc 0.75\n",
            "2019-01-22T04:05:05.193080: step 12, loss 2.46474, acc 0.6875\n",
            "2019-01-22T04:05:05.880886: step 13, loss 1.3825, acc 0.765625\n",
            "2019-01-22T04:05:06.566200: step 14, loss 1.15759, acc 0.765625\n",
            "2019-01-22T04:05:07.255991: step 15, loss 1.46589, acc 0.734375\n",
            "2019-01-22T04:05:07.946556: step 16, loss 2.25039, acc 0.703125\n",
            "2019-01-22T04:05:08.404238: step 17, loss 1.26625, acc 0.611111\n",
            "2019-01-22T04:05:09.090614: step 18, loss 1.15388, acc 0.765625\n",
            "2019-01-22T04:05:09.773682: step 19, loss 1.51069, acc 0.78125\n",
            "2019-01-22T04:05:10.460535: step 20, loss 1.02686, acc 0.78125\n",
            "2019-01-22T04:05:11.148463: step 21, loss 0.872535, acc 0.78125\n",
            "2019-01-22T04:05:11.833559: step 22, loss 0.821466, acc 0.796875\n",
            "2019-01-22T04:05:12.520918: step 23, loss 1.42225, acc 0.765625\n",
            "2019-01-22T04:05:13.207259: step 24, loss 1.52612, acc 0.6875\n",
            "2019-01-22T04:05:13.889899: step 25, loss 1.75106, acc 0.6875\n",
            "2019-01-22T04:05:14.573448: step 26, loss 0.966718, acc 0.8125\n",
            "2019-01-22T04:05:15.260601: step 27, loss 1.32063, acc 0.671875\n",
            "2019-01-22T04:05:15.947270: step 28, loss 0.750976, acc 0.796875\n",
            "2019-01-22T04:05:16.631511: step 29, loss 0.798983, acc 0.796875\n",
            "2019-01-22T04:05:17.316676: step 30, loss 1.15193, acc 0.671875\n",
            "2019-01-22T04:05:18.005682: step 31, loss 1.28846, acc 0.765625\n",
            "2019-01-22T04:05:18.694894: step 32, loss 0.967546, acc 0.765625\n",
            "2019-01-22T04:05:19.386867: step 33, loss 0.46545, acc 0.828125\n",
            "2019-01-22T04:05:19.654694: step 34, loss 0.772599, acc 0.666667\n",
            "2019-01-22T04:05:20.342306: step 35, loss 0.511434, acc 0.828125\n",
            "2019-01-22T04:05:21.027053: step 36, loss 1.31612, acc 0.828125\n",
            "2019-01-22T04:05:21.714161: step 37, loss 1.8214, acc 0.734375\n",
            "2019-01-22T04:05:22.398755: step 38, loss 0.920523, acc 0.796875\n",
            "2019-01-22T04:05:23.083108: step 39, loss 1.02018, acc 0.765625\n",
            "2019-01-22T04:05:23.768607: step 40, loss 0.811001, acc 0.890625\n",
            "2019-01-22T04:05:24.452469: step 41, loss 0.73886, acc 0.828125\n",
            "2019-01-22T04:05:25.132073: step 42, loss 1.03009, acc 0.84375\n",
            "2019-01-22T04:05:25.816736: step 43, loss 0.768295, acc 0.78125\n",
            "2019-01-22T04:05:26.500029: step 44, loss 0.44584, acc 0.828125\n",
            "2019-01-22T04:05:27.183784: step 45, loss 0.826508, acc 0.796875\n",
            "2019-01-22T04:05:27.867524: step 46, loss 1.17827, acc 0.703125\n",
            "2019-01-22T04:05:28.551863: step 47, loss 0.953223, acc 0.78125\n",
            "2019-01-22T04:05:29.237501: step 48, loss 1.05914, acc 0.78125\n",
            "2019-01-22T04:05:29.921821: step 49, loss 0.858457, acc 0.8125\n",
            "2019-01-22T04:05:30.608100: step 50, loss 0.604263, acc 0.78125\n",
            "2019-01-22T04:05:30.870744: step 51, loss 0.626087, acc 0.777778\n",
            "2019-01-22T04:05:31.556480: step 52, loss 0.851252, acc 0.8125\n",
            "2019-01-22T04:05:32.239320: step 53, loss 0.698756, acc 0.78125\n",
            "2019-01-22T04:05:32.925308: step 54, loss 0.396501, acc 0.90625\n",
            "2019-01-22T04:05:33.611475: step 55, loss 0.421119, acc 0.875\n",
            "2019-01-22T04:05:34.294984: step 56, loss 0.504734, acc 0.890625\n",
            "2019-01-22T04:05:34.979741: step 57, loss 0.740598, acc 0.859375\n",
            "2019-01-22T04:05:35.665790: step 58, loss 0.461571, acc 0.859375\n",
            "2019-01-22T04:05:36.352222: step 59, loss 0.634189, acc 0.796875\n",
            "2019-01-22T04:05:37.037209: step 60, loss 0.643892, acc 0.890625\n",
            "2019-01-22T04:05:37.726705: step 61, loss 0.715767, acc 0.859375\n",
            "2019-01-22T04:05:38.415903: step 62, loss 0.747289, acc 0.875\n",
            "2019-01-22T04:05:39.102635: step 63, loss 0.484676, acc 0.921875\n",
            "2019-01-22T04:05:39.789538: step 64, loss 0.298094, acc 0.90625\n",
            "2019-01-22T04:05:40.475004: step 65, loss 0.579211, acc 0.859375\n",
            "2019-01-22T04:05:41.161108: step 66, loss 0.478401, acc 0.859375\n",
            "2019-01-22T04:05:41.847166: step 67, loss 0.665961, acc 0.796875\n",
            "2019-01-22T04:05:42.110831: step 68, loss 0.128687, acc 0.944444\n",
            "2019-01-22T04:05:42.804564: step 69, loss 0.333823, acc 0.890625\n",
            "2019-01-22T04:05:43.492119: step 70, loss 0.616609, acc 0.859375\n",
            "2019-01-22T04:05:44.176179: step 71, loss 0.469222, acc 0.890625\n",
            "2019-01-22T04:05:44.867888: step 72, loss 0.446916, acc 0.890625\n",
            "2019-01-22T04:05:45.555482: step 73, loss 0.604167, acc 0.875\n",
            "2019-01-22T04:05:46.239485: step 74, loss 0.415653, acc 0.890625\n",
            "2019-01-22T04:05:46.932832: step 75, loss 0.551367, acc 0.875\n",
            "2019-01-22T04:05:47.621507: step 76, loss 0.687136, acc 0.90625\n",
            "2019-01-22T04:05:48.302126: step 77, loss 0.39185, acc 0.921875\n",
            "2019-01-22T04:05:48.993023: step 78, loss 0.423206, acc 0.859375\n",
            "2019-01-22T04:05:49.682972: step 79, loss 0.499364, acc 0.828125\n",
            "2019-01-22T04:05:50.368480: step 80, loss 0.333467, acc 0.90625\n",
            "2019-01-22T04:05:51.057827: step 81, loss 0.717508, acc 0.828125\n",
            "2019-01-22T04:05:51.741700: step 82, loss 0.498349, acc 0.875\n",
            "2019-01-22T04:05:52.427471: step 83, loss 0.290809, acc 0.9375\n",
            "2019-01-22T04:05:53.113082: step 84, loss 0.42967, acc 0.890625\n",
            "2019-01-22T04:05:53.376816: step 85, loss 0.647526, acc 0.833333\n",
            "2019-01-22T04:05:54.069764: step 86, loss 0.273584, acc 0.921875\n",
            "2019-01-22T04:05:54.757398: step 87, loss 0.142628, acc 0.9375\n",
            "2019-01-22T04:05:55.442277: step 88, loss 0.289979, acc 0.921875\n",
            "2019-01-22T04:05:56.134086: step 89, loss 0.588078, acc 0.875\n",
            "2019-01-22T04:05:56.819495: step 90, loss 0.661686, acc 0.859375\n",
            "2019-01-22T04:05:57.506183: step 91, loss 0.478271, acc 0.859375\n",
            "2019-01-22T04:05:58.198424: step 92, loss 0.373473, acc 0.890625\n",
            "2019-01-22T04:05:58.881549: step 93, loss 0.568642, acc 0.8125\n",
            "2019-01-22T04:05:59.571243: step 94, loss 0.31349, acc 0.921875\n",
            "2019-01-22T04:06:00.260550: step 95, loss 0.628494, acc 0.859375\n",
            "2019-01-22T04:06:00.944699: step 96, loss 0.380344, acc 0.84375\n",
            "2019-01-22T04:06:01.628639: step 97, loss 0.422746, acc 0.859375\n",
            "2019-01-22T04:06:02.318692: step 98, loss 0.305882, acc 0.890625\n",
            "2019-01-22T04:06:03.003786: step 99, loss 0.255962, acc 0.90625\n",
            "2019-01-22T04:06:03.687533: step 100, loss 0.301089, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T04:06:04.487937: step 100, loss 0.150834, acc 0.93913\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548129893/checkpoints/model-100\n",
            "\n",
            "2019-01-22T04:06:05.317992: step 101, loss 0.32272, acc 0.875\n",
            "2019-01-22T04:06:05.587928: step 102, loss 0.00167206, acc 1\n",
            "2019-01-22T04:06:06.274662: step 103, loss 0.557755, acc 0.921875\n",
            "2019-01-22T04:06:06.960616: step 104, loss 0.360857, acc 0.90625\n",
            "2019-01-22T04:06:07.644774: step 105, loss 0.493106, acc 0.890625\n",
            "2019-01-22T04:06:08.329907: step 106, loss 0.369742, acc 0.890625\n",
            "2019-01-22T04:06:09.015608: step 107, loss 0.737513, acc 0.796875\n",
            "2019-01-22T04:06:09.699253: step 108, loss 0.437326, acc 0.859375\n",
            "2019-01-22T04:06:10.384302: step 109, loss 0.386268, acc 0.84375\n",
            "2019-01-22T04:06:11.069356: step 110, loss 0.602557, acc 0.84375\n",
            "2019-01-22T04:06:11.757715: step 111, loss 0.625359, acc 0.890625\n",
            "2019-01-22T04:06:12.442652: step 112, loss 0.425409, acc 0.90625\n",
            "2019-01-22T04:06:13.128542: step 113, loss 0.285429, acc 0.90625\n",
            "2019-01-22T04:06:13.813829: step 114, loss 0.320029, acc 0.96875\n",
            "2019-01-22T04:06:14.497031: step 115, loss 0.715094, acc 0.828125\n",
            "2019-01-22T04:06:15.184169: step 116, loss 0.227918, acc 0.953125\n",
            "2019-01-22T04:06:15.867877: step 117, loss 0.118131, acc 0.96875\n",
            "2019-01-22T04:06:16.553694: step 118, loss 0.0663932, acc 0.96875\n",
            "2019-01-22T04:06:16.819555: step 119, loss 0.0107319, acc 1\n",
            "2019-01-22T04:06:17.503787: step 120, loss 0.456559, acc 0.875\n",
            "2019-01-22T04:06:18.189810: step 121, loss 0.492893, acc 0.890625\n",
            "2019-01-22T04:06:18.873078: step 122, loss 0.377818, acc 0.875\n",
            "2019-01-22T04:06:19.559554: step 123, loss 0.358499, acc 0.890625\n",
            "2019-01-22T04:06:20.243452: step 124, loss 0.253434, acc 0.90625\n",
            "2019-01-22T04:06:20.928670: step 125, loss 0.0978613, acc 0.96875\n",
            "2019-01-22T04:06:21.617628: step 126, loss 0.305646, acc 0.921875\n",
            "2019-01-22T04:06:22.298571: step 127, loss 0.351617, acc 0.90625\n",
            "2019-01-22T04:06:22.986336: step 128, loss 0.108495, acc 0.96875\n",
            "2019-01-22T04:06:23.671074: step 129, loss 0.494064, acc 0.859375\n",
            "2019-01-22T04:06:24.356784: step 130, loss 0.121243, acc 0.96875\n",
            "2019-01-22T04:06:25.042670: step 131, loss 0.287816, acc 0.953125\n",
            "2019-01-22T04:06:25.725997: step 132, loss 0.300557, acc 0.9375\n",
            "2019-01-22T04:06:26.412349: step 133, loss 0.112319, acc 0.9375\n",
            "2019-01-22T04:06:27.100043: step 134, loss 0.639472, acc 0.875\n",
            "2019-01-22T04:06:27.783198: step 135, loss 0.202078, acc 0.921875\n",
            "2019-01-22T04:06:28.051211: step 136, loss 0.505867, acc 0.888889\n",
            "2019-01-22T04:06:28.735637: step 137, loss 0.150786, acc 0.953125\n",
            "2019-01-22T04:06:29.422404: step 138, loss 0.323958, acc 0.9375\n",
            "2019-01-22T04:06:30.107114: step 139, loss 0.27761, acc 0.921875\n",
            "2019-01-22T04:06:30.791905: step 140, loss 0.318254, acc 0.921875\n",
            "2019-01-22T04:06:31.480060: step 141, loss 0.105465, acc 0.953125\n",
            "2019-01-22T04:06:32.164983: step 142, loss 0.0937811, acc 0.96875\n",
            "2019-01-22T04:06:32.847884: step 143, loss 0.343798, acc 0.921875\n",
            "2019-01-22T04:06:33.532553: step 144, loss 0.294731, acc 0.921875\n",
            "2019-01-22T04:06:34.219358: step 145, loss 0.453115, acc 0.90625\n",
            "2019-01-22T04:06:34.909024: step 146, loss 0.259931, acc 0.921875\n",
            "2019-01-22T04:06:35.595191: step 147, loss 0.136766, acc 0.953125\n",
            "2019-01-22T04:06:36.281966: step 148, loss 0.359643, acc 0.90625\n",
            "2019-01-22T04:06:36.968563: step 149, loss 0.0828249, acc 0.96875\n",
            "2019-01-22T04:06:37.656289: step 150, loss 0.310054, acc 0.9375\n",
            "2019-01-22T04:06:38.342267: step 151, loss 0.435229, acc 0.921875\n",
            "2019-01-22T04:06:39.025172: step 152, loss 0.078289, acc 0.953125\n",
            "2019-01-22T04:06:39.288068: step 153, loss 0.432132, acc 0.888889\n",
            "2019-01-22T04:06:39.977433: step 154, loss 0.324299, acc 0.921875\n",
            "2019-01-22T04:06:40.665771: step 155, loss 0.378382, acc 0.90625\n",
            "2019-01-22T04:06:41.352673: step 156, loss 0.51382, acc 0.890625\n",
            "2019-01-22T04:06:42.035789: step 157, loss 0.281805, acc 0.921875\n",
            "2019-01-22T04:06:42.717618: step 158, loss 0.235526, acc 0.953125\n",
            "2019-01-22T04:06:43.404589: step 159, loss 0.160166, acc 0.96875\n",
            "2019-01-22T04:06:44.089251: step 160, loss 0.655446, acc 0.828125\n",
            "2019-01-22T04:06:44.773835: step 161, loss 0.416416, acc 0.90625\n",
            "2019-01-22T04:06:45.463501: step 162, loss 0.0289981, acc 0.984375\n",
            "2019-01-22T04:06:46.147703: step 163, loss 0.244487, acc 0.90625\n",
            "2019-01-22T04:06:46.833916: step 164, loss 0.395883, acc 0.890625\n",
            "2019-01-22T04:06:47.521275: step 165, loss 0.322243, acc 0.90625\n",
            "2019-01-22T04:06:48.204682: step 166, loss 0.333601, acc 0.984375\n",
            "2019-01-22T04:06:48.891607: step 167, loss 0.206376, acc 0.9375\n",
            "2019-01-22T04:06:49.577938: step 168, loss 0.15859, acc 0.953125\n",
            "2019-01-22T04:06:50.262490: step 169, loss 0.144938, acc 0.90625\n",
            "2019-01-22T04:06:50.529385: step 170, loss 0.499797, acc 0.944444\n",
            "2019-01-22T04:06:51.214320: step 171, loss 0.0755914, acc 0.96875\n",
            "2019-01-22T04:06:51.898638: step 172, loss 0.403987, acc 0.890625\n",
            "2019-01-22T04:06:52.583803: step 173, loss 0.235276, acc 0.9375\n",
            "2019-01-22T04:06:53.268187: step 174, loss 0.237684, acc 0.9375\n",
            "2019-01-22T04:06:53.950544: step 175, loss 0.372912, acc 0.96875\n",
            "2019-01-22T04:06:54.636187: step 176, loss 0.0867265, acc 0.96875\n",
            "2019-01-22T04:06:55.318684: step 177, loss 0.169066, acc 0.953125\n",
            "2019-01-22T04:06:56.002802: step 178, loss 0.0508967, acc 0.96875\n",
            "2019-01-22T04:06:56.686506: step 179, loss 0.120201, acc 0.9375\n",
            "2019-01-22T04:06:57.373315: step 180, loss 0.0410125, acc 0.984375\n",
            "2019-01-22T04:06:58.060437: step 181, loss 0.525757, acc 0.921875\n",
            "2019-01-22T04:06:58.745804: step 182, loss 0.122378, acc 0.9375\n",
            "2019-01-22T04:06:59.427718: step 183, loss 0.248822, acc 0.9375\n",
            "2019-01-22T04:07:00.112245: step 184, loss 0.122309, acc 0.9375\n",
            "2019-01-22T04:07:00.799585: step 185, loss 0.154137, acc 0.9375\n",
            "2019-01-22T04:07:01.482629: step 186, loss 0.172171, acc 0.96875\n",
            "2019-01-22T04:07:01.758640: step 187, loss 0.284165, acc 0.833333\n",
            "2019-01-22T04:07:02.444566: step 188, loss 0.295928, acc 0.921875\n",
            "2019-01-22T04:07:03.129120: step 189, loss 0.180422, acc 0.953125\n",
            "2019-01-22T04:07:03.820981: step 190, loss 0.267393, acc 0.921875\n",
            "2019-01-22T04:07:04.506789: step 191, loss 0.239978, acc 0.953125\n",
            "2019-01-22T04:07:05.191814: step 192, loss 0.117669, acc 0.96875\n",
            "2019-01-22T04:07:05.880473: step 193, loss 0.186373, acc 0.9375\n",
            "2019-01-22T04:07:06.566261: step 194, loss 0.183276, acc 0.921875\n",
            "2019-01-22T04:07:07.251136: step 195, loss 0.444392, acc 0.84375\n",
            "2019-01-22T04:07:07.939305: step 196, loss 0.237502, acc 0.90625\n",
            "2019-01-22T04:07:08.622216: step 197, loss 0.117767, acc 0.953125\n",
            "2019-01-22T04:07:09.304210: step 198, loss 0.0869062, acc 0.96875\n",
            "2019-01-22T04:07:09.994121: step 199, loss 0.273485, acc 0.90625\n",
            "2019-01-22T04:07:10.676143: step 200, loss 0.062587, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T04:07:10.837213: step 200, loss 0.179504, acc 0.956522\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548129893/checkpoints/model-200\n",
            "\n",
            "2019-01-22T04:07:11.625196: step 201, loss 0.0978867, acc 0.9375\n",
            "2019-01-22T04:07:12.308681: step 202, loss 0.223171, acc 0.953125\n",
            "2019-01-22T04:07:12.998808: step 203, loss 0.445067, acc 0.875\n",
            "2019-01-22T04:07:13.262068: step 204, loss 0.0673905, acc 0.944444\n",
            "2019-01-22T04:07:13.948963: step 205, loss 0.0099684, acc 1\n",
            "2019-01-22T04:07:14.632854: step 206, loss 0.0100111, acc 1\n",
            "2019-01-22T04:07:15.315592: step 207, loss 0.145542, acc 0.9375\n",
            "2019-01-22T04:07:16.000098: step 208, loss 0.0920589, acc 0.953125\n",
            "2019-01-22T04:07:16.689763: step 209, loss 0.279231, acc 0.921875\n",
            "2019-01-22T04:07:17.371651: step 210, loss 0.243531, acc 0.9375\n",
            "2019-01-22T04:07:18.057846: step 211, loss 0.511076, acc 0.859375\n",
            "2019-01-22T04:07:18.741462: step 212, loss 0.0953403, acc 0.96875\n",
            "2019-01-22T04:07:19.438723: step 213, loss 0.240195, acc 0.953125\n",
            "2019-01-22T04:07:20.121286: step 214, loss 0.259595, acc 0.890625\n",
            "2019-01-22T04:07:20.808739: step 215, loss 0.283517, acc 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zCJW1smSJYvt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download prediction.csv file\n",
        "    files.download(\"prediction.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXxAEc6kf-YU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4b2e7e6d-dc75-4114-8fbd-8616eefc7d7b"
      },
      "cell_type": "code",
      "source": [
        "!ls -ltr"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 3144\n",
            "drwxr-xr-x 1 root root    4096 Jan  8 17:15 sample_data\n",
            "-rw-r--r-- 1 root root 1862769 Jan 22 02:42 patient_conversation-negative.txt\n",
            "-rw-r--r-- 1 root root  279872 Jan 22 02:42 patient_conversation-positive.txt\n",
            "-rw-r--r-- 1 root root 1058082 Jan 22 02:42 patient_conversations-test.txt\n",
            "drwxr-xr-x 3 root root    4096 Jan 22 02:43 runs\n",
            "-rw-r--r-- 1 root root    3908 Jan 22 03:21 prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IkqoRxP6Jcvc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}