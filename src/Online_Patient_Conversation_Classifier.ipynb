{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Online_Patient_Conversation_Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranildchandra/Online-Patient-Conversation-Classifier/blob/master/src/Online_Patient_Conversation_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "grnSzC9KAlsI",
        "colab_type": "code",
        "outputId": "a37ddf3e-7011-4ccc-91eb-7a243145938c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0707860a-0ac8-4c42-a4fb-0b50a4a535ba\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0707860a-0ac8-4c42-a4fb-0b50a4a535ba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n",
            "User uploaded file \"test.csv\" with length 1174648 bytes\n",
            "User uploaded file \"train.csv\" with length 2378727 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2osLkb5-L1cX",
        "colab_type": "code",
        "outputId": "bb8c01d6-97c8-4d3b-e039-de7c4f350688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "#Install required dependencies\n",
        "!pip install tensorflow==1.12.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.12.0 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.32.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.14.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (3.6.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.12.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.0.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.11.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.7.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.6.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (40.6.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GCpDU2Y977ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import all libraries/dependencies\n",
        "from tensorflow.contrib import learn\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aML-Cz5AtlAd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_dataset():\n",
        "\ttrain_data_df = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")\n",
        "\ttest_data_df = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "\tpositive_train_data_df = train_data_df[train_data_df['Patient_Tag'] == 1]\n",
        "\tpositive_train_data = positive_train_data_df[['TRANS_CONV_TEXT']]\n",
        "\n",
        "\tnegative_train_data_df = train_data_df[train_data_df['Patient_Tag'] == 0]\n",
        "\tnegative_train_data = negative_train_data_df[['TRANS_CONV_TEXT']]\n",
        "\n",
        "\ttest_data = test_data_df[['TRANS_CONV_TEXT']]\n",
        "\n",
        "\tfor index, row in positive_train_data.iterrows():\n",
        "\t\tfile_handler1 = open(\"patient_conversation-positive.txt\", 'w', encoding='utf-8')\n",
        "\t\tfile_handler1.write(str(row[0])+\"\\n\")\n",
        "\t\tfile_handler1.close()\n",
        "\n",
        "\tfor index, row in negative_train_data.iterrows():\n",
        "\t\tfile_handler2 = open(\"patient_conversation-negative.txt\", 'w', encoding='utf-8')\n",
        "\t\tfile_handler2.write(str(row[0])+\"\\n\")\n",
        "\t\tfile_handler2.close()\t\n",
        "\n",
        "\tfor index, row in test_data.iterrows():\n",
        "\t\tfile_handler3 = open(\"patient_conversations-test.txt\", 'w', encoding='utf-8')\n",
        "\t\tfile_handler3.write(str(row[0])+\"\\n\")\n",
        "\t\tfile_handler3.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rKl1C_qt8_mv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TextCNN(object):\n",
        "    #A CNN for text classification. Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    \n",
        "    def __init__(\n",
        "      self, sequence_length, num_classes, vocab_size,\n",
        "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        # Embedding layer\n",
        "        with tf.device('/gpu:0'), tf.name_scope(\"embedding\"):\n",
        "            self.W = tf.Variable(\n",
        "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "                name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # Calculate mean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyZhqN7h8oZe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_str(string):\n",
        "    # Tokenization/string cleaning for all datasets.\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\\\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\\\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\\\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wDmMFGvz8sM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data_and_labels(positive_data_file, negative_data_file):\n",
        "    # Loads data from files, splits the data into words and generates labels. Returns split sentences and labels.\n",
        "\n",
        "    # Load data from files\n",
        "    positive_examples = list(open(positive_data_file, \"r\", encoding=\"utf8\").readlines())\n",
        "    positive_examples = [s.strip() for s in positive_examples]\n",
        "    negative_examples = list(open(negative_data_file, \"r\", encoding=\"utf8\").readlines())\n",
        "    negative_examples = [s.strip() for s in negative_examples]\n",
        "    # Split by words\n",
        "    x_text = positive_examples + negative_examples\n",
        "    x_text = [clean_str(sent) for sent in x_text]\n",
        "    # Generate labels\n",
        "    positive_labels = [[0, 1] for _ in positive_examples]\n",
        "    negative_labels = [[1, 0] for _ in negative_examples]\n",
        "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "    return [x_text, y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wNqW-xYN8tLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "    # Generates a batch iterator for a dataset.\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJNsb9dV_Qdt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]  \n",
        "    print(\"\\nDeleting Following keys...\")\n",
        "    for keys in keys_list:\n",
        "        print(keys)\n",
        "        FLAGS.__delattr__(keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WAsQsnmUMHlb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]  \n",
        "    print(\"\\nCreated Following keys...\")\n",
        "    for keys in keys_list:\n",
        "        print(keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLnnEfpx94eU",
        "colab_type": "code",
        "outputId": "d8fa818e-44e0-4356-c18b-b387a292d5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Delete all flags before declaring\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "# Data loading params\n",
        "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
        "tf.flags.DEFINE_string(\"positive_data_file\", \"patient_conversation-positive.txt\", \"Data source for the positive data.\")\n",
        "tf.flags.DEFINE_string(\"negative_data_file\", \"patient_conversation-negative.txt\", \"Data source for the negative data.\")\n",
        "\n",
        "# Model Hyperparameters\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
        "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
        "\n",
        "# Training parameters\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "tf.flags.DEFINE_string(\"model_path\", \"\", \"Path where the model is saved.\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "\n",
        "# Print all flags after declaring\n",
        "print_all_flags(FLAGS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Deleting Following keys...\n",
            "test_data_file\n",
            "checkpoint_dir\n",
            "eval_train\n",
            "batch_size\n",
            "allow_soft_placement\n",
            "log_device_placement\n",
            "\n",
            "Created Following keys...\n",
            "dev_sample_percentage\n",
            "positive_data_file\n",
            "negative_data_file\n",
            "embedding_dim\n",
            "filter_sizes\n",
            "num_filters\n",
            "dropout_keep_prob\n",
            "l2_reg_lambda\n",
            "batch_size\n",
            "num_epochs\n",
            "evaluate_every\n",
            "checkpoint_every\n",
            "num_checkpoints\n",
            "allow_soft_placement\n",
            "log_device_placement\n",
            "model_path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rg_V-uaUAD6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess():\n",
        "    # Data Preparation\n",
        "    # ==================================================\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    x_text, y = load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
        "\n",
        "    # Build vocabulary\n",
        "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "\n",
        "    # Randomly shuffle data\n",
        "    np.random.seed(10)\n",
        "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "    x_shuffled = x[shuffle_indices]\n",
        "    y_shuffled = y[shuffle_indices]\n",
        "\n",
        "    # Split train/test set\n",
        "    # TODO: This is very crude, should use cross-validation\n",
        "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
        "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "\n",
        "    del x, y, x_shuffled, y_shuffled\n",
        "\n",
        "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "    return x_train, y_train, vocab_processor, x_dev, y_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2kAtq3wNlPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def restore(sess_var, model_path):\n",
        "    if model_path is not None:\n",
        "        if os.path.exists(\"{}.index\".format(model_path)):\n",
        "            saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
        "            saver.restore(sess_var, model_path)\n",
        "            print(\"Model at %s restored\" % model_path)\n",
        "        else:\n",
        "            print(\"Model path does not exist, skipping...\")\n",
        "    else:\n",
        "        print(\"Model path is None - Nothing to restore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_EztpiM9_W6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
        "    # Training\n",
        "    # ==================================================\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        session_conf = tf.ConfigProto(\n",
        "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
        "          log_device_placement=FLAGS.log_device_placement)\n",
        "        sess = tf.Session(config=session_conf)\n",
        "        with sess.as_default():\n",
        "            cnn = TextCNN(\n",
        "                sequence_length=x_train.shape[1],\n",
        "                num_classes=y_train.shape[1],\n",
        "                vocab_size=len(vocab_processor.vocabulary_),\n",
        "                embedding_size=FLAGS.embedding_dim,\n",
        "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "                num_filters=FLAGS.num_filters,\n",
        "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "            # Define Training procedure\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "            # Keep track of gradient values and sparsity (optional)\n",
        "            grad_summaries = []\n",
        "            for g, v in grads_and_vars:\n",
        "                if g is not None:\n",
        "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format((v.name).replace(\":\",\"_\")), g)\n",
        "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format((v.name).replace(\":\",\"_\")), tf.nn.zero_fraction(g))\n",
        "                    grad_summaries.append(grad_hist_summary)\n",
        "                    grad_summaries.append(sparsity_summary)\n",
        "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "            # Output directory for models and summaries\n",
        "            timestamp = str(int(time.time()))\n",
        "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "            print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "            # Summaries for loss and accuracy\n",
        "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "            # Train Summaries\n",
        "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "            # Dev summaries\n",
        "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "            if not os.path.exists(checkpoint_dir):\n",
        "                os.makedirs(checkpoint_dir)\n",
        "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
        "\n",
        "            # Write vocabulary\n",
        "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "            # Initialize all variables\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            def train_step(x_batch, y_batch):\n",
        "                \"\"\"\n",
        "                A single training step\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                  cnn.input_x: x_batch,\n",
        "                  cnn.input_y: y_batch,\n",
        "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
        "                }\n",
        "                _, step, summaries, loss, accuracy = sess.run(\n",
        "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "            def dev_step(x_batch, y_batch, writer=None):\n",
        "                \"\"\"\n",
        "                Evaluates model on a dev set\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                  cnn.input_x: x_batch,\n",
        "                  cnn.input_y: y_batch,\n",
        "                  cnn.dropout_keep_prob: 1.0\n",
        "                }\n",
        "                step, summaries, loss, accuracy = sess.run(\n",
        "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                if writer:\n",
        "                    writer.add_summary(summaries, step)\n",
        "\n",
        "            # Generate batches\n",
        "            batches = batch_iter(\n",
        "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "            # Training loop. For each batch...\n",
        "            for batch in batches:\n",
        "                x_batch, y_batch = zip(*batch)\n",
        "                train_step(x_batch, y_batch)\n",
        "                current_step = tf.train.global_step(sess, global_step)\n",
        "                if current_step % FLAGS.evaluate_every == 0:\n",
        "                    print(\"\\nEvaluation:\")\n",
        "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
        "                    print(\"\")\n",
        "                if current_step % FLAGS.checkpoint_every == 0:\n",
        "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "                    FLAGS.model_path = path\n",
        "                    \n",
        "        #restore(sess, FLAGS.model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHpAN1gXIV77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model_path):\n",
        "    # Delete all flags before declaring\n",
        "    del_all_flags(tf.flags.FLAGS)\n",
        "    \n",
        "    tf.flags.DEFINE_string(\"test_data_file\", \"patient_conversations-test.txt\", \"Data source for the test data.\")\n",
        "    tf.flags.DEFINE_string(\"checkpoint_dir\", model_path, \"Checkpoint directory from training run\")\n",
        "    tf.flags.DEFINE_boolean(\"eval_train\", True, \"Evaluate on all training data\")\n",
        "    tf.flags.DEFINE_integer(\"batch_size\", 2, \"Batch Size (default: 64)\")\n",
        "    tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "    tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "    FLAGS = tf.flags.FLAGS\n",
        "    \n",
        "    # Print all flags after declaring\n",
        "    print_all_flags(FLAGS)\n",
        "    \n",
        "    if FLAGS.eval_train:\n",
        "        # Load data from files\n",
        "        test_examples = list(open(FLAGS.test_data_file, \"r\", encoding=\"utf8\").readlines())\n",
        "        test_examples = [s.strip() for s in test_examples]\n",
        "        # Split by words\n",
        "        x_raw = [clean_str(sent) for sent in test_examples]\n",
        "    else:\n",
        "        x_raw = [\"I think I am suffering from cold and flu\", \"I am really loving this problem\"]\n",
        "        \n",
        "    \n",
        "    \n",
        "    # Map data into vocabulary\n",
        "    vocab_path = os.path.join(FLAGS.checkpoint_dir, \"./../../\", \"vocab\")\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
        "    x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
        "    print(\"\\nEvaluating...\\n\")\n",
        "    \n",
        "    # Evaluation\n",
        "    # ==================================================\n",
        "    checkpoint_file = tf.train.latest_checkpoint(os.path.join(FLAGS.checkpoint_dir, \"./../\"))\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement, log_device_placement=FLAGS.log_device_placement)\n",
        "        sess = tf.Session(config=session_conf)\n",
        "        with sess.as_default():\n",
        "            # Load the saved meta graph and restore variables\n",
        "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "            saver.restore(sess, checkpoint_file)\n",
        "\n",
        "            # Get the placeholders from the graph by name\n",
        "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
        "            # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
        "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
        "\n",
        "            # Tensors we want to evaluate\n",
        "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
        "\n",
        "            # Generate batches for one epoch\n",
        "            batches = batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
        "\n",
        "            # Collect the predictions here\n",
        "            patient_tag = [\"Patient_Tag\"]\n",
        "\n",
        "            for x_test_batch in batches:\n",
        "                batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
        "                patient_tag = np.concatenate([patient_tag, batch_predictions])\n",
        "       \n",
        "    patient_index = [\"Index\"]\n",
        "    patient_index = np.concatenate([patient_index, np.arange(1,len(x_raw)+1,1)])\n",
        "\n",
        "    conversation_data = [\"Conversation_Data\"]\n",
        "    conversation_data = np.concatenate([conversation_data, np.array(x_raw)])\n",
        "    \n",
        "    # Save the evaluation to a csv\n",
        "    predictions = np.column_stack((patient_index, patient_tag))\n",
        "    predictions_description = np.column_stack((conversation_data, patient_tag))\n",
        "\n",
        "    submission_file = os.path.join(FLAGS.checkpoint_dir, \"../../../../\", \"prediction.csv\")\n",
        "    predictions_description_file = os.path.join(FLAGS.checkpoint_dir, \"../../../../\", \"prediction-description.csv\")\n",
        "\n",
        "    out_path = os.path.abspath(submission_file)\n",
        "    print(\"Saving evaluation to {0}\".format(out_path))\n",
        "    with open(out_path, 'w+', encoding=\"utf8\", newline='') as f:\n",
        "        csv.writer(f).writerows(predictions)\n",
        "\n",
        "    out_path = os.path.abspath(predictions_description_file)\n",
        "    print(\"Saving prediction descriptions to {0}\".format(out_path))\n",
        "    with open(out_path, 'w+', encoding=\"utf8\", newline='') as f:\n",
        "        csv.writer(f).writerows(predictions_description)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nCpdgHGzANDk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(argv=None):\n",
        "    generate_dataset()\n",
        "    x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n",
        "    train(x_train, y_train, vocab_processor, x_dev, y_dev)\n",
        "    test(FLAGS.model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwx9CPSnAOE6",
        "colab_type": "code",
        "outputId": "9eea3c82-ad81-49ea-a75c-13e42a79ba1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119707
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    tf.app.run()\n",
        "    os._exit(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Vocabulary Size: 24377\n",
            "Train/Dev split: 2083/231\n",
            "Writing to /content/runs/1548136699\n",
            "\n",
            "2019-01-22T05:58:21.014785: step 1, loss 1.88718, acc 0.671875\n",
            "2019-01-22T05:58:21.715057: step 2, loss 2.25637, acc 0.734375\n",
            "2019-01-22T05:58:22.406838: step 3, loss 0.914202, acc 0.84375\n",
            "2019-01-22T05:58:23.093760: step 4, loss 3.15211, acc 0.671875\n",
            "2019-01-22T05:58:23.783852: step 5, loss 1.45498, acc 0.75\n",
            "2019-01-22T05:58:24.471676: step 6, loss 2.02851, acc 0.765625\n",
            "2019-01-22T05:58:25.159749: step 7, loss 2.03343, acc 0.71875\n",
            "2019-01-22T05:58:25.847648: step 8, loss 1.55283, acc 0.703125\n",
            "2019-01-22T05:58:26.537528: step 9, loss 1.26555, acc 0.765625\n",
            "2019-01-22T05:58:27.227826: step 10, loss 1.68106, acc 0.640625\n",
            "2019-01-22T05:58:27.915624: step 11, loss 1.09731, acc 0.78125\n",
            "2019-01-22T05:58:28.605687: step 12, loss 1.95123, acc 0.671875\n",
            "2019-01-22T05:58:29.297329: step 13, loss 1.80324, acc 0.578125\n",
            "2019-01-22T05:58:29.982330: step 14, loss 1.88467, acc 0.71875\n",
            "2019-01-22T05:58:30.676378: step 15, loss 1.01187, acc 0.734375\n",
            "2019-01-22T05:58:31.362929: step 16, loss 1.51854, acc 0.828125\n",
            "2019-01-22T05:58:32.050787: step 17, loss 0.878795, acc 0.8125\n",
            "2019-01-22T05:58:32.744794: step 18, loss 1.30988, acc 0.75\n",
            "2019-01-22T05:58:33.429071: step 19, loss 1.08818, acc 0.796875\n",
            "2019-01-22T05:58:34.113883: step 20, loss 0.805098, acc 0.84375\n",
            "2019-01-22T05:58:34.806499: step 21, loss 1.19834, acc 0.78125\n",
            "2019-01-22T05:58:35.486992: step 22, loss 0.9054, acc 0.8125\n",
            "2019-01-22T05:58:36.178834: step 23, loss 1.76163, acc 0.6875\n",
            "2019-01-22T05:58:36.872355: step 24, loss 1.26177, acc 0.734375\n",
            "2019-01-22T05:58:37.560949: step 25, loss 1.60956, acc 0.640625\n",
            "2019-01-22T05:58:38.248662: step 26, loss 1.16986, acc 0.703125\n",
            "2019-01-22T05:58:38.943502: step 27, loss 1.45974, acc 0.671875\n",
            "2019-01-22T05:58:39.628852: step 28, loss 0.940225, acc 0.71875\n",
            "2019-01-22T05:58:40.315332: step 29, loss 1.0345, acc 0.6875\n",
            "2019-01-22T05:58:41.014930: step 30, loss 1.0104, acc 0.78125\n",
            "2019-01-22T05:58:41.704246: step 31, loss 0.941857, acc 0.75\n",
            "2019-01-22T05:58:42.392512: step 32, loss 1.40534, acc 0.75\n",
            "2019-01-22T05:58:43.193703: step 33, loss 1.38314, acc 0.742857\n",
            "2019-01-22T05:58:43.895301: step 34, loss 1.09351, acc 0.75\n",
            "2019-01-22T05:58:44.587325: step 35, loss 0.794945, acc 0.796875\n",
            "2019-01-22T05:58:45.276092: step 36, loss 1.01913, acc 0.765625\n",
            "2019-01-22T05:58:45.967114: step 37, loss 1.44713, acc 0.765625\n",
            "2019-01-22T05:58:46.653970: step 38, loss 0.886757, acc 0.84375\n",
            "2019-01-22T05:58:47.339925: step 39, loss 0.939441, acc 0.734375\n",
            "2019-01-22T05:58:48.030359: step 40, loss 0.715362, acc 0.828125\n",
            "2019-01-22T05:58:48.713804: step 41, loss 1.41754, acc 0.65625\n",
            "2019-01-22T05:58:49.401150: step 42, loss 0.584695, acc 0.84375\n",
            "2019-01-22T05:58:50.085110: step 43, loss 0.591489, acc 0.78125\n",
            "2019-01-22T05:58:50.774738: step 44, loss 0.897054, acc 0.78125\n",
            "2019-01-22T05:58:51.460565: step 45, loss 0.949629, acc 0.78125\n",
            "2019-01-22T05:58:52.149610: step 46, loss 0.922712, acc 0.875\n",
            "2019-01-22T05:58:52.840016: step 47, loss 0.45885, acc 0.84375\n",
            "2019-01-22T05:58:53.522224: step 48, loss 0.950681, acc 0.84375\n",
            "2019-01-22T05:58:54.205284: step 49, loss 0.366301, acc 0.9375\n",
            "2019-01-22T05:58:54.894129: step 50, loss 0.276278, acc 0.859375\n",
            "2019-01-22T05:58:55.579873: step 51, loss 0.847579, acc 0.828125\n",
            "2019-01-22T05:58:56.264324: step 52, loss 0.923822, acc 0.859375\n",
            "2019-01-22T05:58:56.953735: step 53, loss 0.647941, acc 0.828125\n",
            "2019-01-22T05:58:57.639100: step 54, loss 0.320603, acc 0.890625\n",
            "2019-01-22T05:58:58.322503: step 55, loss 0.324977, acc 0.9375\n",
            "2019-01-22T05:58:59.012208: step 56, loss 0.509855, acc 0.890625\n",
            "2019-01-22T05:58:59.693371: step 57, loss 1.00907, acc 0.734375\n",
            "2019-01-22T05:59:00.378014: step 58, loss 0.650696, acc 0.84375\n",
            "2019-01-22T05:59:01.060009: step 59, loss 0.484239, acc 0.875\n",
            "2019-01-22T05:59:01.746112: step 60, loss 0.743328, acc 0.796875\n",
            "2019-01-22T05:59:02.429734: step 61, loss 0.779956, acc 0.859375\n",
            "2019-01-22T05:59:03.116444: step 62, loss 0.354216, acc 0.90625\n",
            "2019-01-22T05:59:03.801535: step 63, loss 0.780383, acc 0.8125\n",
            "2019-01-22T05:59:04.485762: step 64, loss 0.426446, acc 0.875\n",
            "2019-01-22T05:59:05.171509: step 65, loss 0.855429, acc 0.828125\n",
            "2019-01-22T05:59:05.589655: step 66, loss 0.235354, acc 0.885714\n",
            "2019-01-22T05:59:06.277736: step 67, loss 0.55363, acc 0.8125\n",
            "2019-01-22T05:59:06.957375: step 68, loss 0.39396, acc 0.875\n",
            "2019-01-22T05:59:07.645096: step 69, loss 0.155423, acc 0.9375\n",
            "2019-01-22T05:59:08.329843: step 70, loss 0.367576, acc 0.921875\n",
            "2019-01-22T05:59:09.016858: step 71, loss 0.52003, acc 0.890625\n",
            "2019-01-22T05:59:09.704537: step 72, loss 0.363853, acc 0.90625\n",
            "2019-01-22T05:59:10.388773: step 73, loss 0.465307, acc 0.90625\n",
            "2019-01-22T05:59:11.075063: step 74, loss 0.884938, acc 0.796875\n",
            "2019-01-22T05:59:11.759830: step 75, loss 0.224514, acc 0.90625\n",
            "2019-01-22T05:59:12.446179: step 76, loss 0.530249, acc 0.84375\n",
            "2019-01-22T05:59:13.135215: step 77, loss 0.399062, acc 0.90625\n",
            "2019-01-22T05:59:13.822330: step 78, loss 0.488572, acc 0.90625\n",
            "2019-01-22T05:59:14.506034: step 79, loss 0.691546, acc 0.828125\n",
            "2019-01-22T05:59:15.194026: step 80, loss 0.691422, acc 0.84375\n",
            "2019-01-22T05:59:15.883815: step 81, loss 0.577553, acc 0.8125\n",
            "2019-01-22T05:59:16.570177: step 82, loss 0.220265, acc 0.921875\n",
            "2019-01-22T05:59:17.256028: step 83, loss 0.359881, acc 0.921875\n",
            "2019-01-22T05:59:17.943733: step 84, loss 0.391912, acc 0.875\n",
            "2019-01-22T05:59:18.631747: step 85, loss 0.61374, acc 0.859375\n",
            "2019-01-22T05:59:19.316266: step 86, loss 0.305088, acc 0.890625\n",
            "2019-01-22T05:59:20.009405: step 87, loss 0.380745, acc 0.9375\n",
            "2019-01-22T05:59:20.694352: step 88, loss 0.636879, acc 0.84375\n",
            "2019-01-22T05:59:21.378674: step 89, loss 0.426988, acc 0.875\n",
            "2019-01-22T05:59:22.071718: step 90, loss 0.89246, acc 0.84375\n",
            "2019-01-22T05:59:22.754270: step 91, loss 0.208703, acc 0.921875\n",
            "2019-01-22T05:59:23.442338: step 92, loss 0.358186, acc 0.890625\n",
            "2019-01-22T05:59:24.132739: step 93, loss 0.395494, acc 0.90625\n",
            "2019-01-22T05:59:24.813922: step 94, loss 0.396672, acc 0.90625\n",
            "2019-01-22T05:59:25.493786: step 95, loss 0.114841, acc 0.953125\n",
            "2019-01-22T05:59:26.186121: step 96, loss 0.629777, acc 0.859375\n",
            "2019-01-22T05:59:26.868880: step 97, loss 0.562187, acc 0.859375\n",
            "2019-01-22T05:59:27.553938: step 98, loss 0.431184, acc 0.859375\n",
            "2019-01-22T05:59:27.973555: step 99, loss 0.309483, acc 0.857143\n",
            "2019-01-22T05:59:28.662721: step 100, loss 0.456286, acc 0.890625\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T05:59:30.329573: step 100, loss 0.118604, acc 0.939394\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-100\n",
            "\n",
            "2019-01-22T05:59:31.164113: step 101, loss 0.411981, acc 0.859375\n",
            "2019-01-22T05:59:31.847691: step 102, loss 0.226671, acc 0.890625\n",
            "2019-01-22T05:59:32.534126: step 103, loss 0.185518, acc 0.96875\n",
            "2019-01-22T05:59:33.226934: step 104, loss 0.195829, acc 0.953125\n",
            "2019-01-22T05:59:33.909956: step 105, loss 0.434811, acc 0.890625\n",
            "2019-01-22T05:59:34.595925: step 106, loss 0.559091, acc 0.859375\n",
            "2019-01-22T05:59:35.280722: step 107, loss 0.683718, acc 0.875\n",
            "2019-01-22T05:59:35.969455: step 108, loss 0.55592, acc 0.890625\n",
            "2019-01-22T05:59:36.657828: step 109, loss 0.291576, acc 0.921875\n",
            "2019-01-22T05:59:37.346838: step 110, loss 0.507686, acc 0.921875\n",
            "2019-01-22T05:59:38.039470: step 111, loss 0.232688, acc 0.9375\n",
            "2019-01-22T05:59:38.728395: step 112, loss 0.426251, acc 0.921875\n",
            "2019-01-22T05:59:39.413029: step 113, loss 0.604923, acc 0.84375\n",
            "2019-01-22T05:59:40.095903: step 114, loss 0.364955, acc 0.859375\n",
            "2019-01-22T05:59:40.781012: step 115, loss 0.465182, acc 0.890625\n",
            "2019-01-22T05:59:41.469026: step 116, loss 0.180359, acc 0.96875\n",
            "2019-01-22T05:59:42.156872: step 117, loss 0.251135, acc 0.90625\n",
            "2019-01-22T05:59:42.844522: step 118, loss 0.343274, acc 0.90625\n",
            "2019-01-22T05:59:43.526706: step 119, loss 0.401898, acc 0.90625\n",
            "2019-01-22T05:59:44.208802: step 120, loss 0.286607, acc 0.921875\n",
            "2019-01-22T05:59:44.892355: step 121, loss 0.306877, acc 0.90625\n",
            "2019-01-22T05:59:45.573356: step 122, loss 0.203984, acc 0.921875\n",
            "2019-01-22T05:59:46.255862: step 123, loss 0.357849, acc 0.890625\n",
            "2019-01-22T05:59:46.947703: step 124, loss 0.258472, acc 0.921875\n",
            "2019-01-22T05:59:47.632149: step 125, loss 0.407115, acc 0.890625\n",
            "2019-01-22T05:59:48.319679: step 126, loss 0.105273, acc 0.96875\n",
            "2019-01-22T05:59:49.000352: step 127, loss 0.588435, acc 0.875\n",
            "2019-01-22T05:59:49.685736: step 128, loss 0.16374, acc 0.96875\n",
            "2019-01-22T05:59:50.372704: step 129, loss 0.492422, acc 0.90625\n",
            "2019-01-22T05:59:51.056882: step 130, loss 0.215191, acc 0.9375\n",
            "2019-01-22T05:59:51.740599: step 131, loss 0.277191, acc 0.921875\n",
            "2019-01-22T05:59:52.155463: step 132, loss 0.206052, acc 0.885714\n",
            "2019-01-22T05:59:52.843683: step 133, loss 0.139506, acc 0.984375\n",
            "2019-01-22T05:59:53.528574: step 134, loss 0.18232, acc 0.921875\n",
            "2019-01-22T05:59:54.219307: step 135, loss 0.278009, acc 0.921875\n",
            "2019-01-22T05:59:54.909801: step 136, loss 0.223523, acc 0.9375\n",
            "2019-01-22T05:59:55.595947: step 137, loss 0.195553, acc 0.9375\n",
            "2019-01-22T05:59:56.280340: step 138, loss 0.621749, acc 0.890625\n",
            "2019-01-22T05:59:56.964677: step 139, loss 0.13992, acc 0.953125\n",
            "2019-01-22T05:59:57.647835: step 140, loss 0.199103, acc 0.953125\n",
            "2019-01-22T05:59:58.332930: step 141, loss 0.260893, acc 0.90625\n",
            "2019-01-22T05:59:59.019187: step 142, loss 0.0232421, acc 0.984375\n",
            "2019-01-22T05:59:59.705438: step 143, loss 0.657379, acc 0.859375\n",
            "2019-01-22T06:00:00.390863: step 144, loss 0.431583, acc 0.84375\n",
            "2019-01-22T06:00:01.083973: step 145, loss 0.278755, acc 0.921875\n",
            "2019-01-22T06:00:01.768448: step 146, loss 0.512687, acc 0.890625\n",
            "2019-01-22T06:00:02.452710: step 147, loss 0.161075, acc 0.9375\n",
            "2019-01-22T06:00:03.142856: step 148, loss 0.103769, acc 0.96875\n",
            "2019-01-22T06:00:03.826547: step 149, loss 0.195069, acc 0.921875\n",
            "2019-01-22T06:00:04.513942: step 150, loss 0.194497, acc 0.921875\n",
            "2019-01-22T06:00:05.202022: step 151, loss 0.277496, acc 0.953125\n",
            "2019-01-22T06:00:05.885886: step 152, loss 0.379411, acc 0.90625\n",
            "2019-01-22T06:00:06.566756: step 153, loss 0.05327, acc 0.984375\n",
            "2019-01-22T06:00:07.256928: step 154, loss 0.655932, acc 0.890625\n",
            "2019-01-22T06:00:07.942369: step 155, loss 0.221791, acc 0.90625\n",
            "2019-01-22T06:00:08.628560: step 156, loss 0.1334, acc 0.953125\n",
            "2019-01-22T06:00:09.314023: step 157, loss 0.16556, acc 0.9375\n",
            "2019-01-22T06:00:10.002851: step 158, loss 0.237692, acc 0.9375\n",
            "2019-01-22T06:00:10.686007: step 159, loss 0.596637, acc 0.859375\n",
            "2019-01-22T06:00:11.371439: step 160, loss 0.323012, acc 0.9375\n",
            "2019-01-22T06:00:12.057506: step 161, loss 0.274999, acc 0.90625\n",
            "2019-01-22T06:00:12.742815: step 162, loss 0.169546, acc 0.953125\n",
            "2019-01-22T06:00:13.431515: step 163, loss 0.301738, acc 0.90625\n",
            "2019-01-22T06:00:14.115909: step 164, loss 0.488631, acc 0.8125\n",
            "2019-01-22T06:00:14.531686: step 165, loss 0.514824, acc 0.942857\n",
            "2019-01-22T06:00:15.217378: step 166, loss 0.27838, acc 0.890625\n",
            "2019-01-22T06:00:15.900584: step 167, loss 0.231497, acc 0.921875\n",
            "2019-01-22T06:00:16.584456: step 168, loss 0.11632, acc 0.953125\n",
            "2019-01-22T06:00:17.270873: step 169, loss 0.020868, acc 1\n",
            "2019-01-22T06:00:17.957376: step 170, loss 0.168399, acc 0.953125\n",
            "2019-01-22T06:00:18.643536: step 171, loss 0.4793, acc 0.921875\n",
            "2019-01-22T06:00:19.331950: step 172, loss 0.176133, acc 0.9375\n",
            "2019-01-22T06:00:20.017700: step 173, loss 0.452271, acc 0.890625\n",
            "2019-01-22T06:00:20.702954: step 174, loss 0.24719, acc 0.9375\n",
            "2019-01-22T06:00:21.391585: step 175, loss 0.218361, acc 0.90625\n",
            "2019-01-22T06:00:22.078785: step 176, loss 0.281338, acc 0.921875\n",
            "2019-01-22T06:00:22.762731: step 177, loss 0.0789968, acc 0.953125\n",
            "2019-01-22T06:00:23.449204: step 178, loss 0.279943, acc 0.9375\n",
            "2019-01-22T06:00:24.138756: step 179, loss 0.283836, acc 0.921875\n",
            "2019-01-22T06:00:24.824297: step 180, loss 0.154795, acc 0.953125\n",
            "2019-01-22T06:00:25.509453: step 181, loss 0.130287, acc 0.96875\n",
            "2019-01-22T06:00:26.195383: step 182, loss 0.356634, acc 0.921875\n",
            "2019-01-22T06:00:26.878403: step 183, loss 0.425269, acc 0.9375\n",
            "2019-01-22T06:00:27.565543: step 184, loss 0.517018, acc 0.921875\n",
            "2019-01-22T06:00:28.249558: step 185, loss 0.448114, acc 0.921875\n",
            "2019-01-22T06:00:28.931730: step 186, loss 0.31534, acc 0.859375\n",
            "2019-01-22T06:00:29.612355: step 187, loss 0.263513, acc 0.9375\n",
            "2019-01-22T06:00:30.301272: step 188, loss 0.278155, acc 0.90625\n",
            "2019-01-22T06:00:30.986824: step 189, loss 0.266216, acc 0.921875\n",
            "2019-01-22T06:00:31.675077: step 190, loss 0.129464, acc 0.984375\n",
            "2019-01-22T06:00:32.366542: step 191, loss 0.321238, acc 0.890625\n",
            "2019-01-22T06:00:33.054942: step 192, loss 0.155332, acc 0.96875\n",
            "2019-01-22T06:00:33.743462: step 193, loss 0.214164, acc 0.9375\n",
            "2019-01-22T06:00:34.430254: step 194, loss 0.110455, acc 0.96875\n",
            "2019-01-22T06:00:35.120380: step 195, loss 0.170306, acc 0.921875\n",
            "2019-01-22T06:00:35.808651: step 196, loss 0.0137283, acc 1\n",
            "2019-01-22T06:00:36.490946: step 197, loss 0.0662293, acc 0.984375\n",
            "2019-01-22T06:00:36.910702: step 198, loss 0.175833, acc 0.942857\n",
            "2019-01-22T06:00:37.594769: step 199, loss 0.111804, acc 0.984375\n",
            "2019-01-22T06:00:38.285311: step 200, loss 0.49979, acc 0.921875\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:00:38.648052: step 200, loss 0.0486869, acc 0.978355\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-200\n",
            "\n",
            "2019-01-22T06:00:39.483732: step 201, loss 0.153391, acc 0.9375\n",
            "2019-01-22T06:00:40.172036: step 202, loss 0.262379, acc 0.921875\n",
            "2019-01-22T06:00:40.858522: step 203, loss 0.425371, acc 0.90625\n",
            "2019-01-22T06:00:41.546099: step 204, loss 0.180534, acc 0.921875\n",
            "2019-01-22T06:00:42.232011: step 205, loss 0.117812, acc 0.953125\n",
            "2019-01-22T06:00:42.921675: step 206, loss 0.122866, acc 0.96875\n",
            "2019-01-22T06:00:43.608664: step 207, loss 0.214474, acc 0.921875\n",
            "2019-01-22T06:00:44.296286: step 208, loss 0.0342611, acc 1\n",
            "2019-01-22T06:00:44.986508: step 209, loss 0.0937146, acc 0.96875\n",
            "2019-01-22T06:00:45.671813: step 210, loss 0.207398, acc 0.953125\n",
            "2019-01-22T06:00:46.356481: step 211, loss 0.132769, acc 0.9375\n",
            "2019-01-22T06:00:47.042480: step 212, loss 0.154993, acc 0.953125\n",
            "2019-01-22T06:00:47.728528: step 213, loss 0.205336, acc 0.953125\n",
            "2019-01-22T06:00:48.413606: step 214, loss 0.292507, acc 0.90625\n",
            "2019-01-22T06:00:49.099206: step 215, loss 0.19247, acc 0.953125\n",
            "2019-01-22T06:00:49.785626: step 216, loss 0.219392, acc 0.921875\n",
            "2019-01-22T06:00:50.476442: step 217, loss 0.54012, acc 0.84375\n",
            "2019-01-22T06:00:51.165730: step 218, loss 0.173591, acc 0.953125\n",
            "2019-01-22T06:00:51.857237: step 219, loss 0.291551, acc 0.921875\n",
            "2019-01-22T06:00:52.540038: step 220, loss 0.262134, acc 0.9375\n",
            "2019-01-22T06:00:53.223777: step 221, loss 0.200663, acc 0.953125\n",
            "2019-01-22T06:00:53.906050: step 222, loss 0.108785, acc 0.953125\n",
            "2019-01-22T06:00:54.589759: step 223, loss 0.342801, acc 0.921875\n",
            "2019-01-22T06:00:55.274099: step 224, loss 0.292453, acc 0.953125\n",
            "2019-01-22T06:00:55.961131: step 225, loss 0.208986, acc 0.9375\n",
            "2019-01-22T06:00:56.649694: step 226, loss 0.0233586, acc 0.984375\n",
            "2019-01-22T06:00:57.334712: step 227, loss 0.199371, acc 0.9375\n",
            "2019-01-22T06:00:58.023372: step 228, loss 0.312383, acc 0.921875\n",
            "2019-01-22T06:00:58.709510: step 229, loss 0.194415, acc 0.953125\n",
            "2019-01-22T06:00:59.394933: step 230, loss 0.133706, acc 0.96875\n",
            "2019-01-22T06:00:59.812778: step 231, loss 0.0925537, acc 0.971429\n",
            "2019-01-22T06:01:00.505090: step 232, loss 0.274141, acc 0.953125\n",
            "2019-01-22T06:01:01.196445: step 233, loss 0.0835807, acc 0.96875\n",
            "2019-01-22T06:01:01.880207: step 234, loss 0.140487, acc 0.96875\n",
            "2019-01-22T06:01:02.566625: step 235, loss 0.290436, acc 0.96875\n",
            "2019-01-22T06:01:03.256782: step 236, loss 0.0515641, acc 0.96875\n",
            "2019-01-22T06:01:03.944751: step 237, loss 0.196961, acc 0.9375\n",
            "2019-01-22T06:01:04.634116: step 238, loss 0.172112, acc 0.953125\n",
            "2019-01-22T06:01:05.320729: step 239, loss 0.122978, acc 0.953125\n",
            "2019-01-22T06:01:06.011035: step 240, loss 0.072188, acc 0.96875\n",
            "2019-01-22T06:01:06.692956: step 241, loss 0.154569, acc 0.953125\n",
            "2019-01-22T06:01:07.376531: step 242, loss 0.0550237, acc 0.984375\n",
            "2019-01-22T06:01:08.060138: step 243, loss 0.185207, acc 0.921875\n",
            "2019-01-22T06:01:08.747014: step 244, loss 0.115559, acc 0.96875\n",
            "2019-01-22T06:01:09.433255: step 245, loss 0.0942406, acc 0.953125\n",
            "2019-01-22T06:01:10.118004: step 246, loss 0.202341, acc 0.953125\n",
            "2019-01-22T06:01:10.811377: step 247, loss 0.0620485, acc 0.96875\n",
            "2019-01-22T06:01:11.500715: step 248, loss 0.325897, acc 0.90625\n",
            "2019-01-22T06:01:12.192302: step 249, loss 0.137231, acc 0.96875\n",
            "2019-01-22T06:01:12.877794: step 250, loss 0.145904, acc 0.96875\n",
            "2019-01-22T06:01:13.566098: step 251, loss 0.00342765, acc 1\n",
            "2019-01-22T06:01:14.261738: step 252, loss 0.0324846, acc 0.984375\n",
            "2019-01-22T06:01:14.950990: step 253, loss 0.0741894, acc 0.96875\n",
            "2019-01-22T06:01:15.637535: step 254, loss 0.200453, acc 0.96875\n",
            "2019-01-22T06:01:16.331029: step 255, loss 0.162052, acc 0.9375\n",
            "2019-01-22T06:01:17.014715: step 256, loss 0.0105663, acc 1\n",
            "2019-01-22T06:01:17.694308: step 257, loss 0.0497925, acc 0.96875\n",
            "2019-01-22T06:01:18.385482: step 258, loss 0.0855953, acc 0.984375\n",
            "2019-01-22T06:01:19.072886: step 259, loss 0.116849, acc 0.953125\n",
            "2019-01-22T06:01:19.761861: step 260, loss 0.0592195, acc 0.96875\n",
            "2019-01-22T06:01:20.450697: step 261, loss 0.181707, acc 0.96875\n",
            "2019-01-22T06:01:21.133380: step 262, loss 0.292664, acc 0.921875\n",
            "2019-01-22T06:01:21.815390: step 263, loss 0.0424248, acc 0.984375\n",
            "2019-01-22T06:01:22.231699: step 264, loss 0.0556727, acc 0.971429\n",
            "2019-01-22T06:01:22.919766: step 265, loss 0.227237, acc 0.9375\n",
            "2019-01-22T06:01:23.599951: step 266, loss 0.106234, acc 0.96875\n",
            "2019-01-22T06:01:24.285617: step 267, loss 0.226274, acc 0.9375\n",
            "2019-01-22T06:01:24.970362: step 268, loss 0.0324057, acc 0.984375\n",
            "2019-01-22T06:01:25.652231: step 269, loss 0.161722, acc 0.953125\n",
            "2019-01-22T06:01:26.335930: step 270, loss 0.0493987, acc 0.96875\n",
            "2019-01-22T06:01:27.022816: step 271, loss 0.0372542, acc 0.96875\n",
            "2019-01-22T06:01:27.704696: step 272, loss 0.0457825, acc 0.96875\n",
            "2019-01-22T06:01:28.395210: step 273, loss 0.202656, acc 0.921875\n",
            "2019-01-22T06:01:29.078141: step 274, loss 0.0108911, acc 1\n",
            "2019-01-22T06:01:29.765190: step 275, loss 0.0178294, acc 1\n",
            "2019-01-22T06:01:30.444229: step 276, loss 0.100151, acc 0.953125\n",
            "2019-01-22T06:01:31.122608: step 277, loss 0.0563438, acc 0.96875\n",
            "2019-01-22T06:01:31.815713: step 278, loss 0.246519, acc 0.9375\n",
            "2019-01-22T06:01:32.497074: step 279, loss 0.269363, acc 0.9375\n",
            "2019-01-22T06:01:33.182050: step 280, loss 0.251004, acc 0.953125\n",
            "2019-01-22T06:01:33.868049: step 281, loss 0.0794481, acc 0.984375\n",
            "2019-01-22T06:01:34.551336: step 282, loss 0.260209, acc 0.9375\n",
            "2019-01-22T06:01:35.236330: step 283, loss 0.140112, acc 0.921875\n",
            "2019-01-22T06:01:35.923719: step 284, loss 0.0933467, acc 0.96875\n",
            "2019-01-22T06:01:36.603425: step 285, loss 0.1096, acc 0.953125\n",
            "2019-01-22T06:01:37.285938: step 286, loss 0.0357228, acc 0.984375\n",
            "2019-01-22T06:01:37.980036: step 287, loss 0.0784227, acc 0.953125\n",
            "2019-01-22T06:01:38.666150: step 288, loss 0.011562, acc 1\n",
            "2019-01-22T06:01:39.347063: step 289, loss 0.230614, acc 0.96875\n",
            "2019-01-22T06:01:40.034893: step 290, loss 0.116588, acc 0.953125\n",
            "2019-01-22T06:01:40.719114: step 291, loss 0.127227, acc 0.96875\n",
            "2019-01-22T06:01:41.406582: step 292, loss 0.0391907, acc 0.96875\n",
            "2019-01-22T06:01:42.092964: step 293, loss 0.250608, acc 0.96875\n",
            "2019-01-22T06:01:42.779476: step 294, loss 0.177496, acc 0.921875\n",
            "2019-01-22T06:01:43.466598: step 295, loss 0.178363, acc 0.921875\n",
            "2019-01-22T06:01:44.154446: step 296, loss 0.0876758, acc 0.9375\n",
            "2019-01-22T06:01:44.568444: step 297, loss 0.177011, acc 0.914286\n",
            "2019-01-22T06:01:45.246620: step 298, loss 0.211057, acc 0.9375\n",
            "2019-01-22T06:01:45.931986: step 299, loss 0.120761, acc 0.984375\n",
            "2019-01-22T06:01:46.614804: step 300, loss 0.0842806, acc 0.953125\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:01:46.970316: step 300, loss 0.0196497, acc 0.991342\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-300\n",
            "\n",
            "2019-01-22T06:01:47.810072: step 301, loss 0.0123156, acc 1\n",
            "2019-01-22T06:01:48.492366: step 302, loss 0.128184, acc 0.96875\n",
            "2019-01-22T06:01:49.171233: step 303, loss 0.250245, acc 0.921875\n",
            "2019-01-22T06:01:49.853452: step 304, loss 0.282435, acc 0.9375\n",
            "2019-01-22T06:01:50.533984: step 305, loss 0.0222585, acc 1\n",
            "2019-01-22T06:01:51.219482: step 306, loss 0.102382, acc 0.96875\n",
            "2019-01-22T06:01:51.903277: step 307, loss 0.00361008, acc 1\n",
            "2019-01-22T06:01:52.579309: step 308, loss 0.0117381, acc 1\n",
            "2019-01-22T06:01:53.263880: step 309, loss 0.111674, acc 0.9375\n",
            "2019-01-22T06:01:53.950267: step 310, loss 0.266632, acc 0.90625\n",
            "2019-01-22T06:01:54.632273: step 311, loss 0.0434464, acc 0.984375\n",
            "2019-01-22T06:01:55.322196: step 312, loss 0.0568381, acc 0.96875\n",
            "2019-01-22T06:01:56.001287: step 313, loss 0.13971, acc 0.96875\n",
            "2019-01-22T06:01:56.685898: step 314, loss 0.133504, acc 0.96875\n",
            "2019-01-22T06:01:57.369083: step 315, loss 0.102939, acc 0.96875\n",
            "2019-01-22T06:01:58.049904: step 316, loss 0.0157602, acc 1\n",
            "2019-01-22T06:01:58.732108: step 317, loss 0.166852, acc 0.9375\n",
            "2019-01-22T06:01:59.416494: step 318, loss 0.160974, acc 0.96875\n",
            "2019-01-22T06:02:00.095883: step 319, loss 0.362444, acc 0.90625\n",
            "2019-01-22T06:02:00.775048: step 320, loss 0.245822, acc 0.9375\n",
            "2019-01-22T06:02:01.463484: step 321, loss 0.00606286, acc 1\n",
            "2019-01-22T06:02:02.151408: step 322, loss 0.123833, acc 0.96875\n",
            "2019-01-22T06:02:02.837340: step 323, loss 0.131128, acc 0.921875\n",
            "2019-01-22T06:02:03.517680: step 324, loss 0.0431658, acc 0.96875\n",
            "2019-01-22T06:02:04.195820: step 325, loss 0.100385, acc 0.953125\n",
            "2019-01-22T06:02:04.878806: step 326, loss 0.100017, acc 0.96875\n",
            "2019-01-22T06:02:05.566890: step 327, loss 0.0405849, acc 0.984375\n",
            "2019-01-22T06:02:06.254793: step 328, loss 0.0120089, acc 1\n",
            "2019-01-22T06:02:06.937841: step 329, loss 0.183949, acc 0.9375\n",
            "2019-01-22T06:02:07.354571: step 330, loss 0.0339084, acc 0.971429\n",
            "2019-01-22T06:02:08.039932: step 331, loss 0.240371, acc 0.9375\n",
            "2019-01-22T06:02:08.730249: step 332, loss 0.0383058, acc 0.984375\n",
            "2019-01-22T06:02:09.416197: step 333, loss 0.0819511, acc 0.96875\n",
            "2019-01-22T06:02:10.101396: step 334, loss 0.0236004, acc 0.984375\n",
            "2019-01-22T06:02:10.792359: step 335, loss 0.131128, acc 0.984375\n",
            "2019-01-22T06:02:11.480692: step 336, loss 0.0253552, acc 0.984375\n",
            "2019-01-22T06:02:12.165743: step 337, loss 0.0414538, acc 0.984375\n",
            "2019-01-22T06:02:12.854885: step 338, loss 0.0765536, acc 0.984375\n",
            "2019-01-22T06:02:13.543113: step 339, loss 0.0654216, acc 0.984375\n",
            "2019-01-22T06:02:14.227390: step 340, loss 0.499044, acc 0.890625\n",
            "2019-01-22T06:02:14.916698: step 341, loss 0.21715, acc 0.984375\n",
            "2019-01-22T06:02:15.600427: step 342, loss 0.100682, acc 0.96875\n",
            "2019-01-22T06:02:16.281730: step 343, loss 0.00462653, acc 1\n",
            "2019-01-22T06:02:16.975276: step 344, loss 0.209151, acc 0.953125\n",
            "2019-01-22T06:02:17.665678: step 345, loss 0.11014, acc 0.953125\n",
            "2019-01-22T06:02:18.347797: step 346, loss 0.0398726, acc 0.984375\n",
            "2019-01-22T06:02:19.038400: step 347, loss 0.162066, acc 0.96875\n",
            "2019-01-22T06:02:19.716409: step 348, loss 0.112377, acc 0.96875\n",
            "2019-01-22T06:02:20.400200: step 349, loss 0.059233, acc 0.984375\n",
            "2019-01-22T06:02:21.094401: step 350, loss 0.228622, acc 0.921875\n",
            "2019-01-22T06:02:21.777971: step 351, loss 0.0789157, acc 0.96875\n",
            "2019-01-22T06:02:22.469407: step 352, loss 0.050586, acc 0.96875\n",
            "2019-01-22T06:02:23.160359: step 353, loss 0.0269785, acc 1\n",
            "2019-01-22T06:02:23.845497: step 354, loss 0.102213, acc 0.96875\n",
            "2019-01-22T06:02:24.535891: step 355, loss 0.0376553, acc 0.984375\n",
            "2019-01-22T06:02:25.224317: step 356, loss 0.0727161, acc 0.984375\n",
            "2019-01-22T06:02:25.906084: step 357, loss 0.281081, acc 0.9375\n",
            "2019-01-22T06:02:26.584707: step 358, loss 0.0083366, acc 1\n",
            "2019-01-22T06:02:27.274390: step 359, loss 0.124633, acc 0.96875\n",
            "2019-01-22T06:02:27.963689: step 360, loss 0.0977703, acc 0.953125\n",
            "2019-01-22T06:02:28.647268: step 361, loss 0.0312952, acc 0.984375\n",
            "2019-01-22T06:02:29.334835: step 362, loss 0.00631314, acc 1\n",
            "2019-01-22T06:02:29.751355: step 363, loss 0.019705, acc 1\n",
            "2019-01-22T06:02:30.433526: step 364, loss 0.0197999, acc 0.984375\n",
            "2019-01-22T06:02:31.114554: step 365, loss 0.00808957, acc 1\n",
            "2019-01-22T06:02:31.802593: step 366, loss 0.0431369, acc 0.984375\n",
            "2019-01-22T06:02:32.483387: step 367, loss 0.378076, acc 0.9375\n",
            "2019-01-22T06:02:33.167253: step 368, loss 0.00716344, acc 1\n",
            "2019-01-22T06:02:33.848263: step 369, loss 0.0654549, acc 0.984375\n",
            "2019-01-22T06:02:34.534442: step 370, loss 0.0413494, acc 0.984375\n",
            "2019-01-22T06:02:35.213480: step 371, loss 0.153753, acc 0.9375\n",
            "2019-01-22T06:02:35.892805: step 372, loss 0.0123227, acc 1\n",
            "2019-01-22T06:02:36.573512: step 373, loss 0.0594349, acc 0.984375\n",
            "2019-01-22T06:02:37.255029: step 374, loss 0.0621881, acc 0.984375\n",
            "2019-01-22T06:02:37.941583: step 375, loss 0.200118, acc 0.953125\n",
            "2019-01-22T06:02:38.627356: step 376, loss 0.0623923, acc 0.96875\n",
            "2019-01-22T06:02:39.310708: step 377, loss 0.186, acc 0.953125\n",
            "2019-01-22T06:02:40.000270: step 378, loss 0.105939, acc 0.953125\n",
            "2019-01-22T06:02:40.689532: step 379, loss 0.0970937, acc 0.953125\n",
            "2019-01-22T06:02:41.373015: step 380, loss 0.0141026, acc 1\n",
            "2019-01-22T06:02:42.055934: step 381, loss 0.117224, acc 0.953125\n",
            "2019-01-22T06:02:42.742188: step 382, loss 0.045611, acc 0.984375\n",
            "2019-01-22T06:02:43.427439: step 383, loss 0.117739, acc 0.9375\n",
            "2019-01-22T06:02:44.117729: step 384, loss 0.104926, acc 0.953125\n",
            "2019-01-22T06:02:44.808338: step 385, loss 0.112969, acc 0.96875\n",
            "2019-01-22T06:02:45.499332: step 386, loss 0.0699141, acc 0.96875\n",
            "2019-01-22T06:02:46.184443: step 387, loss 0.0292611, acc 0.984375\n",
            "2019-01-22T06:02:46.869764: step 388, loss 0.124401, acc 0.96875\n",
            "2019-01-22T06:02:47.553341: step 389, loss 0.0989534, acc 0.953125\n",
            "2019-01-22T06:02:48.239318: step 390, loss 0.189901, acc 0.953125\n",
            "2019-01-22T06:02:48.934625: step 391, loss 0.0287674, acc 0.984375\n",
            "2019-01-22T06:02:49.615548: step 392, loss 0.254829, acc 0.953125\n",
            "2019-01-22T06:02:50.296475: step 393, loss 0.0208433, acc 0.984375\n",
            "2019-01-22T06:02:50.981532: step 394, loss 0.0466738, acc 0.96875\n",
            "2019-01-22T06:02:51.665050: step 395, loss 0.110656, acc 0.9375\n",
            "2019-01-22T06:02:52.077653: step 396, loss 0.0776907, acc 0.971429\n",
            "2019-01-22T06:02:52.767202: step 397, loss 0.0404934, acc 0.96875\n",
            "2019-01-22T06:02:53.453600: step 398, loss 0.139812, acc 0.96875\n",
            "2019-01-22T06:02:54.137697: step 399, loss 0.0581327, acc 0.984375\n",
            "2019-01-22T06:02:54.825447: step 400, loss 0.0101854, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:02:55.190712: step 400, loss 0.00315538, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-400\n",
            "\n",
            "2019-01-22T06:02:56.016517: step 401, loss 0.128739, acc 0.953125\n",
            "2019-01-22T06:02:56.713068: step 402, loss 0.0316848, acc 0.984375\n",
            "2019-01-22T06:02:57.400398: step 403, loss 0.185216, acc 0.96875\n",
            "2019-01-22T06:02:58.084860: step 404, loss 0.00892455, acc 1\n",
            "2019-01-22T06:02:58.768046: step 405, loss 0.160263, acc 0.953125\n",
            "2019-01-22T06:02:59.453616: step 406, loss 0.120485, acc 0.984375\n",
            "2019-01-22T06:03:00.136000: step 407, loss 0.18218, acc 0.9375\n",
            "2019-01-22T06:03:00.821999: step 408, loss 0.0789871, acc 0.984375\n",
            "2019-01-22T06:03:01.502256: step 409, loss 0.0560009, acc 0.96875\n",
            "2019-01-22T06:03:02.190970: step 410, loss 0.0217095, acc 0.984375\n",
            "2019-01-22T06:03:02.872906: step 411, loss 0.0790749, acc 0.953125\n",
            "2019-01-22T06:03:03.562529: step 412, loss 0.109651, acc 0.9375\n",
            "2019-01-22T06:03:04.250145: step 413, loss 0.0113496, acc 1\n",
            "2019-01-22T06:03:04.938243: step 414, loss 0.0531745, acc 0.984375\n",
            "2019-01-22T06:03:05.622048: step 415, loss 0.119474, acc 0.96875\n",
            "2019-01-22T06:03:06.311069: step 416, loss 0.0358933, acc 0.96875\n",
            "2019-01-22T06:03:06.999408: step 417, loss 0.349323, acc 0.96875\n",
            "2019-01-22T06:03:07.684125: step 418, loss 0.3031, acc 0.921875\n",
            "2019-01-22T06:03:08.373301: step 419, loss 0.0893535, acc 0.96875\n",
            "2019-01-22T06:03:09.067340: step 420, loss 0.0160708, acc 0.984375\n",
            "2019-01-22T06:03:09.755212: step 421, loss 0.0504724, acc 0.96875\n",
            "2019-01-22T06:03:10.446973: step 422, loss 0.00914196, acc 1\n",
            "2019-01-22T06:03:11.132386: step 423, loss 0.104403, acc 0.96875\n",
            "2019-01-22T06:03:11.817110: step 424, loss 0.0376624, acc 0.984375\n",
            "2019-01-22T06:03:12.505496: step 425, loss 0.0310414, acc 0.984375\n",
            "2019-01-22T06:03:13.191765: step 426, loss 0.00968318, acc 1\n",
            "2019-01-22T06:03:13.876519: step 427, loss 0.0336929, acc 0.984375\n",
            "2019-01-22T06:03:14.563929: step 428, loss 0.181927, acc 0.96875\n",
            "2019-01-22T06:03:14.984369: step 429, loss 0.0396904, acc 0.971429\n",
            "2019-01-22T06:03:15.674140: step 430, loss 0.0873252, acc 0.96875\n",
            "2019-01-22T06:03:16.353483: step 431, loss 0.197947, acc 0.953125\n",
            "2019-01-22T06:03:17.038421: step 432, loss 0.0106129, acc 1\n",
            "2019-01-22T06:03:17.731576: step 433, loss 0.00651626, acc 1\n",
            "2019-01-22T06:03:18.419261: step 434, loss 0.00600758, acc 1\n",
            "2019-01-22T06:03:19.104055: step 435, loss 0.0545879, acc 0.96875\n",
            "2019-01-22T06:03:19.793725: step 436, loss 0.110318, acc 0.96875\n",
            "2019-01-22T06:03:20.478538: step 437, loss 0.011357, acc 1\n",
            "2019-01-22T06:03:21.168373: step 438, loss 0.0197429, acc 0.984375\n",
            "2019-01-22T06:03:21.852494: step 439, loss 0.0663263, acc 0.96875\n",
            "2019-01-22T06:03:22.540802: step 440, loss 0.0890589, acc 0.96875\n",
            "2019-01-22T06:03:23.228564: step 441, loss 0.146274, acc 0.953125\n",
            "2019-01-22T06:03:23.914724: step 442, loss 0.0432294, acc 0.984375\n",
            "2019-01-22T06:03:24.601264: step 443, loss 0.102311, acc 0.984375\n",
            "2019-01-22T06:03:25.288029: step 444, loss 0.0468498, acc 0.96875\n",
            "2019-01-22T06:03:25.979569: step 445, loss 0.0423738, acc 0.984375\n",
            "2019-01-22T06:03:26.664465: step 446, loss 0.114753, acc 0.96875\n",
            "2019-01-22T06:03:27.351532: step 447, loss 0.083058, acc 0.984375\n",
            "2019-01-22T06:03:28.042401: step 448, loss 0.0653483, acc 0.984375\n",
            "2019-01-22T06:03:28.725968: step 449, loss 0.0281043, acc 0.96875\n",
            "2019-01-22T06:03:29.415098: step 450, loss 0.0107509, acc 1\n",
            "2019-01-22T06:03:30.105017: step 451, loss 0.174194, acc 0.953125\n",
            "2019-01-22T06:03:30.791971: step 452, loss 0.00495813, acc 1\n",
            "2019-01-22T06:03:31.477090: step 453, loss 0.129329, acc 0.953125\n",
            "2019-01-22T06:03:32.168542: step 454, loss 0.115963, acc 0.953125\n",
            "2019-01-22T06:03:32.852119: step 455, loss 0.0825281, acc 0.96875\n",
            "2019-01-22T06:03:33.537885: step 456, loss 0.00500127, acc 1\n",
            "2019-01-22T06:03:34.230442: step 457, loss 0.0236112, acc 1\n",
            "2019-01-22T06:03:34.913599: step 458, loss 0.0632699, acc 0.984375\n",
            "2019-01-22T06:03:35.599114: step 459, loss 0.051293, acc 0.96875\n",
            "2019-01-22T06:03:36.290220: step 460, loss 0.0543029, acc 0.96875\n",
            "2019-01-22T06:03:36.975415: step 461, loss 0.0240778, acc 0.984375\n",
            "2019-01-22T06:03:37.389762: step 462, loss 0.000496075, acc 1\n",
            "2019-01-22T06:03:38.081917: step 463, loss 0.030523, acc 0.984375\n",
            "2019-01-22T06:03:38.769338: step 464, loss 0.0117866, acc 1\n",
            "2019-01-22T06:03:39.457756: step 465, loss 0.0120993, acc 1\n",
            "2019-01-22T06:03:40.144230: step 466, loss 0.0296618, acc 0.984375\n",
            "2019-01-22T06:03:40.833141: step 467, loss 0.0273946, acc 0.984375\n",
            "2019-01-22T06:03:41.517969: step 468, loss 0.0717213, acc 0.984375\n",
            "2019-01-22T06:03:42.205244: step 469, loss 0.0157188, acc 1\n",
            "2019-01-22T06:03:42.890782: step 470, loss 0.150167, acc 0.953125\n",
            "2019-01-22T06:03:43.579870: step 471, loss 0.0152706, acc 1\n",
            "2019-01-22T06:03:44.266576: step 472, loss 0.0583262, acc 0.96875\n",
            "2019-01-22T06:03:44.957916: step 473, loss 0.0777491, acc 0.984375\n",
            "2019-01-22T06:03:45.645611: step 474, loss 0.0107641, acc 1\n",
            "2019-01-22T06:03:46.329489: step 475, loss 0.0628336, acc 0.96875\n",
            "2019-01-22T06:03:47.016397: step 476, loss 0.0191367, acc 0.984375\n",
            "2019-01-22T06:03:47.702311: step 477, loss 0.0642587, acc 0.96875\n",
            "2019-01-22T06:03:48.390538: step 478, loss 0.0105053, acc 1\n",
            "2019-01-22T06:03:49.073463: step 479, loss 0.00324118, acc 1\n",
            "2019-01-22T06:03:49.758677: step 480, loss 0.0980071, acc 0.9375\n",
            "2019-01-22T06:03:50.445992: step 481, loss 0.0337093, acc 0.984375\n",
            "2019-01-22T06:03:51.130972: step 482, loss 0.148475, acc 0.953125\n",
            "2019-01-22T06:03:51.814480: step 483, loss 0.0945932, acc 0.96875\n",
            "2019-01-22T06:03:52.499920: step 484, loss 0.0184814, acc 1\n",
            "2019-01-22T06:03:53.185358: step 485, loss 0.0743429, acc 0.96875\n",
            "2019-01-22T06:03:53.870739: step 486, loss 0.0992843, acc 0.96875\n",
            "2019-01-22T06:03:54.557914: step 487, loss 0.00650518, acc 1\n",
            "2019-01-22T06:03:55.240409: step 488, loss 0.00460613, acc 1\n",
            "2019-01-22T06:03:55.930078: step 489, loss 0.00636458, acc 1\n",
            "2019-01-22T06:03:56.611825: step 490, loss 0.00206484, acc 1\n",
            "2019-01-22T06:03:57.296588: step 491, loss 0.0103902, acc 1\n",
            "2019-01-22T06:03:57.983523: step 492, loss 0.00476459, acc 1\n",
            "2019-01-22T06:03:58.669074: step 493, loss 0.0431873, acc 0.96875\n",
            "2019-01-22T06:03:59.350515: step 494, loss 0.00633343, acc 1\n",
            "2019-01-22T06:03:59.769406: step 495, loss 0.347325, acc 0.914286\n",
            "2019-01-22T06:04:00.455751: step 496, loss 0.0121568, acc 1\n",
            "2019-01-22T06:04:01.145086: step 497, loss 0.0401903, acc 0.96875\n",
            "2019-01-22T06:04:01.830025: step 498, loss 0.170757, acc 0.953125\n",
            "2019-01-22T06:04:02.514527: step 499, loss 0.00613535, acc 1\n",
            "2019-01-22T06:04:03.198474: step 500, loss 0.00827418, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:04:03.559587: step 500, loss 0.0019105, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-500\n",
            "\n",
            "2019-01-22T06:04:04.372084: step 501, loss 0.0552827, acc 0.984375\n",
            "2019-01-22T06:04:05.069587: step 502, loss 0.043704, acc 0.96875\n",
            "2019-01-22T06:04:05.753880: step 503, loss 0.0981507, acc 0.96875\n",
            "2019-01-22T06:04:06.441186: step 504, loss 0.0248958, acc 0.984375\n",
            "2019-01-22T06:04:07.132199: step 505, loss 0.0483743, acc 0.984375\n",
            "2019-01-22T06:04:07.832346: step 506, loss 0.0172303, acc 1\n",
            "2019-01-22T06:04:08.518441: step 507, loss 0.0310073, acc 0.984375\n",
            "2019-01-22T06:04:09.211359: step 508, loss 0.0762923, acc 0.96875\n",
            "2019-01-22T06:04:09.896494: step 509, loss 0.142166, acc 0.96875\n",
            "2019-01-22T06:04:10.584493: step 510, loss 0.0284176, acc 0.984375\n",
            "2019-01-22T06:04:11.279123: step 511, loss 0.0501546, acc 0.96875\n",
            "2019-01-22T06:04:11.966258: step 512, loss 0.00793754, acc 1\n",
            "2019-01-22T06:04:12.650331: step 513, loss 0.0464948, acc 0.96875\n",
            "2019-01-22T06:04:13.337469: step 514, loss 0.0626113, acc 0.96875\n",
            "2019-01-22T06:04:14.024489: step 515, loss 0.0593918, acc 0.984375\n",
            "2019-01-22T06:04:14.711772: step 516, loss 0.126508, acc 0.953125\n",
            "2019-01-22T06:04:15.398571: step 517, loss 0.0107842, acc 1\n",
            "2019-01-22T06:04:16.085661: step 518, loss 0.00384336, acc 1\n",
            "2019-01-22T06:04:16.770641: step 519, loss 0.0202897, acc 1\n",
            "2019-01-22T06:04:17.456571: step 520, loss 0.00653223, acc 1\n",
            "2019-01-22T06:04:18.137792: step 521, loss 0.0227043, acc 0.984375\n",
            "2019-01-22T06:04:18.821976: step 522, loss 0.0362566, acc 0.984375\n",
            "2019-01-22T06:04:19.505043: step 523, loss 0.0260872, acc 0.984375\n",
            "2019-01-22T06:04:20.190000: step 524, loss 0.0295335, acc 0.984375\n",
            "2019-01-22T06:04:20.873075: step 525, loss 0.0949755, acc 0.96875\n",
            "2019-01-22T06:04:21.567131: step 526, loss 0.0513148, acc 0.96875\n",
            "2019-01-22T06:04:22.250543: step 527, loss 0.0827464, acc 0.96875\n",
            "2019-01-22T06:04:22.669474: step 528, loss 0.00287772, acc 1\n",
            "2019-01-22T06:04:23.354721: step 529, loss 0.00751911, acc 1\n",
            "2019-01-22T06:04:24.039992: step 530, loss 0.0985097, acc 0.96875\n",
            "2019-01-22T06:04:24.725325: step 531, loss 0.0805069, acc 0.96875\n",
            "2019-01-22T06:04:25.408349: step 532, loss 0.0875542, acc 0.96875\n",
            "2019-01-22T06:04:26.093275: step 533, loss 0.0872402, acc 0.953125\n",
            "2019-01-22T06:04:26.776920: step 534, loss 0.0276386, acc 0.984375\n",
            "2019-01-22T06:04:27.463123: step 535, loss 0.00417008, acc 1\n",
            "2019-01-22T06:04:28.150980: step 536, loss 0.031136, acc 0.984375\n",
            "2019-01-22T06:04:28.832175: step 537, loss 0.00173328, acc 1\n",
            "2019-01-22T06:04:29.519334: step 538, loss 0.147384, acc 0.984375\n",
            "2019-01-22T06:04:30.205930: step 539, loss 0.00244992, acc 1\n",
            "2019-01-22T06:04:30.889821: step 540, loss 0.0084988, acc 1\n",
            "2019-01-22T06:04:31.576064: step 541, loss 0.139434, acc 0.953125\n",
            "2019-01-22T06:04:32.260343: step 542, loss 0.00989219, acc 1\n",
            "2019-01-22T06:04:32.944394: step 543, loss 0.0049324, acc 1\n",
            "2019-01-22T06:04:33.629383: step 544, loss 0.0404807, acc 0.984375\n",
            "2019-01-22T06:04:34.311889: step 545, loss 0.0504567, acc 0.984375\n",
            "2019-01-22T06:04:34.994014: step 546, loss 0.0288229, acc 0.984375\n",
            "2019-01-22T06:04:35.679582: step 547, loss 0.0502284, acc 0.984375\n",
            "2019-01-22T06:04:36.365234: step 548, loss 0.0489977, acc 0.96875\n",
            "2019-01-22T06:04:37.049623: step 549, loss 0.00222779, acc 1\n",
            "2019-01-22T06:04:37.739287: step 550, loss 0.145612, acc 0.953125\n",
            "2019-01-22T06:04:38.421604: step 551, loss 0.00744472, acc 1\n",
            "2019-01-22T06:04:39.110343: step 552, loss 0.0100799, acc 1\n",
            "2019-01-22T06:04:39.794605: step 553, loss 0.00449694, acc 1\n",
            "2019-01-22T06:04:40.480964: step 554, loss 0.106839, acc 0.96875\n",
            "2019-01-22T06:04:41.163550: step 555, loss 0.0591108, acc 0.984375\n",
            "2019-01-22T06:04:41.848299: step 556, loss 0.0295491, acc 0.984375\n",
            "2019-01-22T06:04:42.542501: step 557, loss 0.00530046, acc 1\n",
            "2019-01-22T06:04:43.229152: step 558, loss 0.021617, acc 0.984375\n",
            "2019-01-22T06:04:43.918535: step 559, loss 0.0442214, acc 0.984375\n",
            "2019-01-22T06:04:44.601950: step 560, loss 0.0708336, acc 0.96875\n",
            "2019-01-22T06:04:45.021197: step 561, loss 0.00473192, acc 1\n",
            "2019-01-22T06:04:45.702864: step 562, loss 0.106846, acc 0.984375\n",
            "2019-01-22T06:04:46.387054: step 563, loss 0.0141908, acc 1\n",
            "2019-01-22T06:04:47.071818: step 564, loss 0.0144811, acc 0.984375\n",
            "2019-01-22T06:04:47.753381: step 565, loss 0.004916, acc 1\n",
            "2019-01-22T06:04:48.443987: step 566, loss 0.0145937, acc 1\n",
            "2019-01-22T06:04:49.127214: step 567, loss 0.0169615, acc 1\n",
            "2019-01-22T06:04:49.808724: step 568, loss 0.0988897, acc 0.953125\n",
            "2019-01-22T06:04:50.491863: step 569, loss 0.0145572, acc 1\n",
            "2019-01-22T06:04:51.176067: step 570, loss 0.0572902, acc 0.984375\n",
            "2019-01-22T06:04:51.864103: step 571, loss 0.01317, acc 1\n",
            "2019-01-22T06:04:52.547700: step 572, loss 0.00557417, acc 1\n",
            "2019-01-22T06:04:53.230667: step 573, loss 0.0131132, acc 1\n",
            "2019-01-22T06:04:53.916487: step 574, loss 0.102933, acc 0.96875\n",
            "2019-01-22T06:04:54.602348: step 575, loss 0.0344761, acc 0.984375\n",
            "2019-01-22T06:04:55.286861: step 576, loss 0.0130178, acc 1\n",
            "2019-01-22T06:04:55.975922: step 577, loss 0.00706213, acc 1\n",
            "2019-01-22T06:04:56.658941: step 578, loss 0.0456407, acc 0.984375\n",
            "2019-01-22T06:04:57.345468: step 579, loss 0.0163575, acc 0.984375\n",
            "2019-01-22T06:04:58.027550: step 580, loss 0.111237, acc 0.96875\n",
            "2019-01-22T06:04:58.710728: step 581, loss 0.0659057, acc 0.953125\n",
            "2019-01-22T06:04:59.398146: step 582, loss 0.00752692, acc 1\n",
            "2019-01-22T06:05:00.084863: step 583, loss 0.0403842, acc 0.96875\n",
            "2019-01-22T06:05:00.769583: step 584, loss 0.0808454, acc 0.96875\n",
            "2019-01-22T06:05:01.456550: step 585, loss 0.103459, acc 0.953125\n",
            "2019-01-22T06:05:02.140976: step 586, loss 0.0106412, acc 1\n",
            "2019-01-22T06:05:02.825587: step 587, loss 0.302031, acc 0.9375\n",
            "2019-01-22T06:05:03.512466: step 588, loss 0.023232, acc 0.984375\n",
            "2019-01-22T06:05:04.197611: step 589, loss 0.0429647, acc 0.96875\n",
            "2019-01-22T06:05:04.880124: step 590, loss 0.0334958, acc 0.96875\n",
            "2019-01-22T06:05:05.561300: step 591, loss 0.00643288, acc 1\n",
            "2019-01-22T06:05:06.244608: step 592, loss 0.0232529, acc 0.984375\n",
            "2019-01-22T06:05:06.927868: step 593, loss 0.00747093, acc 1\n",
            "2019-01-22T06:05:07.343930: step 594, loss 0.0302847, acc 0.971429\n",
            "2019-01-22T06:05:08.026694: step 595, loss 0.118695, acc 0.96875\n",
            "2019-01-22T06:05:08.709396: step 596, loss 0.022634, acc 0.984375\n",
            "2019-01-22T06:05:09.392726: step 597, loss 0.13561, acc 0.96875\n",
            "2019-01-22T06:05:10.080715: step 598, loss 0.0211082, acc 0.984375\n",
            "2019-01-22T06:05:10.761296: step 599, loss 0.0261719, acc 0.984375\n",
            "2019-01-22T06:05:11.449990: step 600, loss 0.00645281, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:05:11.810692: step 600, loss 0.000927288, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-600\n",
            "\n",
            "2019-01-22T06:05:12.642189: step 601, loss 0.0277003, acc 0.984375\n",
            "2019-01-22T06:05:13.325525: step 602, loss 0.092178, acc 0.96875\n",
            "2019-01-22T06:05:14.013738: step 603, loss 0.0154229, acc 1\n",
            "2019-01-22T06:05:14.704940: step 604, loss 0.00519654, acc 1\n",
            "2019-01-22T06:05:15.388694: step 605, loss 0.00871123, acc 1\n",
            "2019-01-22T06:05:16.080935: step 606, loss 0.0177935, acc 0.984375\n",
            "2019-01-22T06:05:16.767617: step 607, loss 0.0298478, acc 0.984375\n",
            "2019-01-22T06:05:17.456535: step 608, loss 0.0684218, acc 0.96875\n",
            "2019-01-22T06:05:18.149337: step 609, loss 0.0203062, acc 0.984375\n",
            "2019-01-22T06:05:18.832214: step 610, loss 0.0362393, acc 0.984375\n",
            "2019-01-22T06:05:19.513331: step 611, loss 0.0204448, acc 1\n",
            "2019-01-22T06:05:20.204372: step 612, loss 0.0704938, acc 0.984375\n",
            "2019-01-22T06:05:20.886326: step 613, loss 0.0449698, acc 0.96875\n",
            "2019-01-22T06:05:21.567530: step 614, loss 0.0710459, acc 0.984375\n",
            "2019-01-22T06:05:22.253836: step 615, loss 0.0772267, acc 0.984375\n",
            "2019-01-22T06:05:22.935818: step 616, loss 0.0108027, acc 1\n",
            "2019-01-22T06:05:23.622585: step 617, loss 0.0163439, acc 1\n",
            "2019-01-22T06:05:24.311634: step 618, loss 0.0223756, acc 0.984375\n",
            "2019-01-22T06:05:24.993746: step 619, loss 0.0173364, acc 0.984375\n",
            "2019-01-22T06:05:25.675769: step 620, loss 0.0341496, acc 0.984375\n",
            "2019-01-22T06:05:26.369848: step 621, loss 0.0127367, acc 1\n",
            "2019-01-22T06:05:27.053864: step 622, loss 0.110974, acc 0.953125\n",
            "2019-01-22T06:05:27.737404: step 623, loss 0.00831564, acc 1\n",
            "2019-01-22T06:05:28.430926: step 624, loss 0.0211305, acc 0.984375\n",
            "2019-01-22T06:05:29.119363: step 625, loss 0.0528435, acc 0.984375\n",
            "2019-01-22T06:05:29.802650: step 626, loss 0.0254372, acc 0.984375\n",
            "2019-01-22T06:05:30.217271: step 627, loss 0.0996498, acc 0.971429\n",
            "2019-01-22T06:05:30.900044: step 628, loss 0.00512809, acc 1\n",
            "2019-01-22T06:05:31.584886: step 629, loss 0.0806685, acc 0.96875\n",
            "2019-01-22T06:05:32.269004: step 630, loss 0.0253005, acc 0.984375\n",
            "2019-01-22T06:05:32.954407: step 631, loss 0.18201, acc 0.96875\n",
            "2019-01-22T06:05:33.639820: step 632, loss 0.00671687, acc 1\n",
            "2019-01-22T06:05:34.323978: step 633, loss 0.000441309, acc 1\n",
            "2019-01-22T06:05:35.007391: step 634, loss 0.0216294, acc 0.984375\n",
            "2019-01-22T06:05:35.690259: step 635, loss 0.108889, acc 0.96875\n",
            "2019-01-22T06:05:36.376502: step 636, loss 0.021928, acc 0.984375\n",
            "2019-01-22T06:05:37.062934: step 637, loss 0.0570402, acc 0.984375\n",
            "2019-01-22T06:05:37.750119: step 638, loss 0.00187794, acc 1\n",
            "2019-01-22T06:05:38.432453: step 639, loss 0.106847, acc 0.96875\n",
            "2019-01-22T06:05:39.116278: step 640, loss 0.00786175, acc 1\n",
            "2019-01-22T06:05:39.803906: step 641, loss 0.00483236, acc 1\n",
            "2019-01-22T06:05:40.494760: step 642, loss 0.0220864, acc 0.984375\n",
            "2019-01-22T06:05:41.182743: step 643, loss 0.0153724, acc 1\n",
            "2019-01-22T06:05:41.871912: step 644, loss 0.0791462, acc 0.96875\n",
            "2019-01-22T06:05:42.559701: step 645, loss 0.0319501, acc 0.984375\n",
            "2019-01-22T06:05:43.245800: step 646, loss 0.00515265, acc 1\n",
            "2019-01-22T06:05:43.928724: step 647, loss 0.00134381, acc 1\n",
            "2019-01-22T06:05:44.611363: step 648, loss 0.00888316, acc 1\n",
            "2019-01-22T06:05:45.294277: step 649, loss 0.0657563, acc 0.984375\n",
            "2019-01-22T06:05:45.981371: step 650, loss 0.000922577, acc 1\n",
            "2019-01-22T06:05:46.665528: step 651, loss 0.00602602, acc 1\n",
            "2019-01-22T06:05:47.347859: step 652, loss 0.0580673, acc 0.984375\n",
            "2019-01-22T06:05:48.034218: step 653, loss 0.0187621, acc 0.984375\n",
            "2019-01-22T06:05:48.713930: step 654, loss 0.00134863, acc 1\n",
            "2019-01-22T06:05:49.397868: step 655, loss 0.0209144, acc 0.984375\n",
            "2019-01-22T06:05:50.086841: step 656, loss 0.0417338, acc 0.984375\n",
            "2019-01-22T06:05:50.774080: step 657, loss 0.0419419, acc 0.984375\n",
            "2019-01-22T06:05:51.461371: step 658, loss 0.224647, acc 0.96875\n",
            "2019-01-22T06:05:52.143820: step 659, loss 0.0437968, acc 0.984375\n",
            "2019-01-22T06:05:52.554762: step 660, loss 0.0126022, acc 1\n",
            "2019-01-22T06:05:53.236753: step 661, loss 0.0366647, acc 0.984375\n",
            "2019-01-22T06:05:53.923889: step 662, loss 0.00155593, acc 1\n",
            "2019-01-22T06:05:54.607014: step 663, loss 0.0140513, acc 1\n",
            "2019-01-22T06:05:55.292549: step 664, loss 0.00314385, acc 1\n",
            "2019-01-22T06:05:55.975108: step 665, loss 0.102803, acc 0.96875\n",
            "2019-01-22T06:05:56.659258: step 666, loss 0.00124274, acc 1\n",
            "2019-01-22T06:05:57.349291: step 667, loss 0.00662769, acc 1\n",
            "2019-01-22T06:05:58.030293: step 668, loss 0.0417151, acc 0.984375\n",
            "2019-01-22T06:05:58.714819: step 669, loss 0.00996777, acc 1\n",
            "2019-01-22T06:05:59.396334: step 670, loss 0.0262315, acc 0.984375\n",
            "2019-01-22T06:06:00.077946: step 671, loss 0.103484, acc 0.984375\n",
            "2019-01-22T06:06:00.761600: step 672, loss 0.0169301, acc 1\n",
            "2019-01-22T06:06:01.447260: step 673, loss 0.0115481, acc 1\n",
            "2019-01-22T06:06:02.129412: step 674, loss 0.0318002, acc 0.984375\n",
            "2019-01-22T06:06:02.814769: step 675, loss 0.0236212, acc 0.984375\n",
            "2019-01-22T06:06:03.504038: step 676, loss 0.0559009, acc 0.984375\n",
            "2019-01-22T06:06:04.186018: step 677, loss 0.00636006, acc 1\n",
            "2019-01-22T06:06:04.871084: step 678, loss 0.0638365, acc 0.953125\n",
            "2019-01-22T06:06:05.555390: step 679, loss 0.0479683, acc 0.96875\n",
            "2019-01-22T06:06:06.243323: step 680, loss 0.00231364, acc 1\n",
            "2019-01-22T06:06:06.929144: step 681, loss 0.0474865, acc 0.96875\n",
            "2019-01-22T06:06:07.616576: step 682, loss 0.0624604, acc 0.984375\n",
            "2019-01-22T06:06:08.297193: step 683, loss 0.051291, acc 0.96875\n",
            "2019-01-22T06:06:08.987203: step 684, loss 0.0289944, acc 0.984375\n",
            "2019-01-22T06:06:09.670947: step 685, loss 0.0179445, acc 0.984375\n",
            "2019-01-22T06:06:10.356110: step 686, loss 0.0104856, acc 1\n",
            "2019-01-22T06:06:11.039114: step 687, loss 0.00897845, acc 1\n",
            "2019-01-22T06:06:11.719396: step 688, loss 0.0183176, acc 0.984375\n",
            "2019-01-22T06:06:12.405217: step 689, loss 0.0113451, acc 0.984375\n",
            "2019-01-22T06:06:13.085757: step 690, loss 0.0190686, acc 1\n",
            "2019-01-22T06:06:13.771229: step 691, loss 0.00364949, acc 1\n",
            "2019-01-22T06:06:14.457214: step 692, loss 0.0734457, acc 0.984375\n",
            "2019-01-22T06:06:14.873438: step 693, loss 0.0111358, acc 1\n",
            "2019-01-22T06:06:15.557995: step 694, loss 0.0121722, acc 1\n",
            "2019-01-22T06:06:16.241367: step 695, loss 0.0833092, acc 0.984375\n",
            "2019-01-22T06:06:16.926583: step 696, loss 0.0119752, acc 1\n",
            "2019-01-22T06:06:17.611378: step 697, loss 0.0147858, acc 1\n",
            "2019-01-22T06:06:18.292371: step 698, loss 0.00452859, acc 1\n",
            "2019-01-22T06:06:18.973311: step 699, loss 0.0845967, acc 0.984375\n",
            "2019-01-22T06:06:19.657471: step 700, loss 0.00485788, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:06:20.012210: step 700, loss 0.000756435, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-700\n",
            "\n",
            "2019-01-22T06:06:20.860578: step 701, loss 0.000753592, acc 1\n",
            "2019-01-22T06:06:21.545131: step 702, loss 0.165387, acc 0.96875\n",
            "2019-01-22T06:06:22.226142: step 703, loss 0.022634, acc 0.984375\n",
            "2019-01-22T06:06:22.911494: step 704, loss 0.0113623, acc 1\n",
            "2019-01-22T06:06:23.595395: step 705, loss 0.0138943, acc 1\n",
            "2019-01-22T06:06:24.280876: step 706, loss 0.0150375, acc 0.984375\n",
            "2019-01-22T06:06:24.966674: step 707, loss 0.0294757, acc 0.984375\n",
            "2019-01-22T06:06:25.656976: step 708, loss 0.00727231, acc 1\n",
            "2019-01-22T06:06:26.337884: step 709, loss 0.00323491, acc 1\n",
            "2019-01-22T06:06:27.022887: step 710, loss 0.0452269, acc 0.96875\n",
            "2019-01-22T06:06:27.707398: step 711, loss 0.00236604, acc 1\n",
            "2019-01-22T06:06:28.388288: step 712, loss 0.127838, acc 0.984375\n",
            "2019-01-22T06:06:29.077594: step 713, loss 0.0118182, acc 1\n",
            "2019-01-22T06:06:29.763872: step 714, loss 0.0148757, acc 1\n",
            "2019-01-22T06:06:30.450348: step 715, loss 0.0294385, acc 0.984375\n",
            "2019-01-22T06:06:31.135402: step 716, loss 0.0381466, acc 0.96875\n",
            "2019-01-22T06:06:31.825195: step 717, loss 0.0224467, acc 1\n",
            "2019-01-22T06:06:32.508238: step 718, loss 0.0309488, acc 0.984375\n",
            "2019-01-22T06:06:33.197391: step 719, loss 0.00688258, acc 1\n",
            "2019-01-22T06:06:33.879675: step 720, loss 0.0288036, acc 0.984375\n",
            "2019-01-22T06:06:34.563641: step 721, loss 0.044919, acc 0.984375\n",
            "2019-01-22T06:06:35.246621: step 722, loss 0.0301592, acc 0.984375\n",
            "2019-01-22T06:06:35.930420: step 723, loss 0.0761849, acc 0.96875\n",
            "2019-01-22T06:06:36.614143: step 724, loss 0.00418163, acc 1\n",
            "2019-01-22T06:06:37.297381: step 725, loss 0.00263443, acc 1\n",
            "2019-01-22T06:06:37.715724: step 726, loss 0.0341023, acc 0.971429\n",
            "2019-01-22T06:06:38.406650: step 727, loss 0.0243403, acc 0.984375\n",
            "2019-01-22T06:06:39.090410: step 728, loss 0.00176747, acc 1\n",
            "2019-01-22T06:06:39.773377: step 729, loss 0.028484, acc 0.984375\n",
            "2019-01-22T06:06:40.469900: step 730, loss 0.0108122, acc 1\n",
            "2019-01-22T06:06:41.151088: step 731, loss 0.0137692, acc 1\n",
            "2019-01-22T06:06:41.830423: step 732, loss 0.00068546, acc 1\n",
            "2019-01-22T06:06:42.513792: step 733, loss 0.0739767, acc 0.953125\n",
            "2019-01-22T06:06:43.196634: step 734, loss 0.000338321, acc 1\n",
            "2019-01-22T06:06:43.878242: step 735, loss 0.0286126, acc 0.984375\n",
            "2019-01-22T06:06:44.569331: step 736, loss 0.000978792, acc 1\n",
            "2019-01-22T06:06:45.256076: step 737, loss 0.0597827, acc 0.984375\n",
            "2019-01-22T06:06:45.942152: step 738, loss 0.00273474, acc 1\n",
            "2019-01-22T06:06:46.626124: step 739, loss 0.00202797, acc 1\n",
            "2019-01-22T06:06:47.309771: step 740, loss 0.0687818, acc 0.96875\n",
            "2019-01-22T06:06:47.993855: step 741, loss 0.000556461, acc 1\n",
            "2019-01-22T06:06:48.687994: step 742, loss 0.00470546, acc 1\n",
            "2019-01-22T06:06:49.369551: step 743, loss 0.00139858, acc 1\n",
            "2019-01-22T06:06:50.055519: step 744, loss 0.0049196, acc 1\n",
            "2019-01-22T06:06:50.743869: step 745, loss 0.0334245, acc 0.96875\n",
            "2019-01-22T06:06:51.424894: step 746, loss 0.00482822, acc 1\n",
            "2019-01-22T06:06:52.111243: step 747, loss 0.052291, acc 0.984375\n",
            "2019-01-22T06:06:52.801935: step 748, loss 0.010125, acc 1\n",
            "2019-01-22T06:06:53.486869: step 749, loss 0.00141403, acc 1\n",
            "2019-01-22T06:06:54.177629: step 750, loss 0.00375821, acc 1\n",
            "2019-01-22T06:06:54.864348: step 751, loss 0.00361062, acc 1\n",
            "2019-01-22T06:06:55.549631: step 752, loss 0.0222871, acc 0.984375\n",
            "2019-01-22T06:06:56.236093: step 753, loss 0.00652466, acc 1\n",
            "2019-01-22T06:06:56.926550: step 754, loss 0.0581823, acc 0.984375\n",
            "2019-01-22T06:06:57.609403: step 755, loss 0.0793454, acc 0.953125\n",
            "2019-01-22T06:06:58.295278: step 756, loss 0.00303429, acc 1\n",
            "2019-01-22T06:06:58.982859: step 757, loss 0.0444296, acc 0.984375\n",
            "2019-01-22T06:06:59.664377: step 758, loss 0.00294077, acc 1\n",
            "2019-01-22T06:07:00.081279: step 759, loss 0.00685169, acc 1\n",
            "2019-01-22T06:07:00.762070: step 760, loss 0.00497491, acc 1\n",
            "2019-01-22T06:07:01.445248: step 761, loss 0.0268791, acc 0.984375\n",
            "2019-01-22T06:07:02.129667: step 762, loss 0.0777625, acc 0.984375\n",
            "2019-01-22T06:07:02.816363: step 763, loss 0.00240679, acc 1\n",
            "2019-01-22T06:07:03.496921: step 764, loss 0.0287303, acc 1\n",
            "2019-01-22T06:07:04.177893: step 765, loss 0.0702645, acc 0.984375\n",
            "2019-01-22T06:07:04.861214: step 766, loss 0.00742278, acc 1\n",
            "2019-01-22T06:07:05.547715: step 767, loss 0.0360372, acc 0.984375\n",
            "2019-01-22T06:07:06.234304: step 768, loss 0.14599, acc 0.96875\n",
            "2019-01-22T06:07:06.919379: step 769, loss 0.00197604, acc 1\n",
            "2019-01-22T06:07:07.602848: step 770, loss 0.0117298, acc 1\n",
            "2019-01-22T06:07:08.285148: step 771, loss 0.00282677, acc 1\n",
            "2019-01-22T06:07:08.969930: step 772, loss 0.000353279, acc 1\n",
            "2019-01-22T06:07:09.654410: step 773, loss 0.045492, acc 0.96875\n",
            "2019-01-22T06:07:10.341288: step 774, loss 0.00964119, acc 1\n",
            "2019-01-22T06:07:11.027882: step 775, loss 0.00921229, acc 1\n",
            "2019-01-22T06:07:11.708794: step 776, loss 0.0142663, acc 1\n",
            "2019-01-22T06:07:12.395367: step 777, loss 0.0255587, acc 0.96875\n",
            "2019-01-22T06:07:13.078035: step 778, loss 0.00134359, acc 1\n",
            "2019-01-22T06:07:13.765259: step 779, loss 0.0435178, acc 0.984375\n",
            "2019-01-22T06:07:14.450734: step 780, loss 0.0884511, acc 0.953125\n",
            "2019-01-22T06:07:15.131294: step 781, loss 0.044309, acc 0.984375\n",
            "2019-01-22T06:07:15.815838: step 782, loss 0.00395878, acc 1\n",
            "2019-01-22T06:07:16.500895: step 783, loss 0.00294222, acc 1\n",
            "2019-01-22T06:07:17.187393: step 784, loss 0.0145201, acc 0.984375\n",
            "2019-01-22T06:07:17.870858: step 785, loss 0.142844, acc 0.953125\n",
            "2019-01-22T06:07:18.555942: step 786, loss 0.00166966, acc 1\n",
            "2019-01-22T06:07:19.235379: step 787, loss 0.000715078, acc 1\n",
            "2019-01-22T06:07:19.916572: step 788, loss 0.00482724, acc 1\n",
            "2019-01-22T06:07:20.606798: step 789, loss 0.0609483, acc 0.984375\n",
            "2019-01-22T06:07:21.291074: step 790, loss 0.000405774, acc 1\n",
            "2019-01-22T06:07:21.972508: step 791, loss 0.000786419, acc 1\n",
            "2019-01-22T06:07:22.385737: step 792, loss 0.0124671, acc 1\n",
            "2019-01-22T06:07:23.069864: step 793, loss 0.0213068, acc 0.984375\n",
            "2019-01-22T06:07:23.751409: step 794, loss 0.0191412, acc 0.984375\n",
            "2019-01-22T06:07:24.432754: step 795, loss 0.00131329, acc 1\n",
            "2019-01-22T06:07:25.116847: step 796, loss 0.0425863, acc 0.984375\n",
            "2019-01-22T06:07:25.800634: step 797, loss 0.00449684, acc 1\n",
            "2019-01-22T06:07:26.486452: step 798, loss 0.0730063, acc 0.984375\n",
            "2019-01-22T06:07:27.171444: step 799, loss 0.0201229, acc 0.984375\n",
            "2019-01-22T06:07:27.856325: step 800, loss 0.00275546, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:07:28.212524: step 800, loss 0.000501825, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-800\n",
            "\n",
            "2019-01-22T06:07:29.056688: step 801, loss 0.0477857, acc 0.96875\n",
            "2019-01-22T06:07:29.738654: step 802, loss 0.00302421, acc 1\n",
            "2019-01-22T06:07:30.422442: step 803, loss 0.051912, acc 0.96875\n",
            "2019-01-22T06:07:31.110365: step 804, loss 0.00690366, acc 1\n",
            "2019-01-22T06:07:31.791945: step 805, loss 0.00379745, acc 1\n",
            "2019-01-22T06:07:32.475207: step 806, loss 0.00179548, acc 1\n",
            "2019-01-22T06:07:33.158446: step 807, loss 0.0769533, acc 0.984375\n",
            "2019-01-22T06:07:33.846887: step 808, loss 0.0172794, acc 0.984375\n",
            "2019-01-22T06:07:34.529039: step 809, loss 0.0269438, acc 0.984375\n",
            "2019-01-22T06:07:35.212788: step 810, loss 0.0399431, acc 0.984375\n",
            "2019-01-22T06:07:35.901892: step 811, loss 0.0125399, acc 1\n",
            "2019-01-22T06:07:36.587485: step 812, loss 0.0672769, acc 0.984375\n",
            "2019-01-22T06:07:37.271090: step 813, loss 0.00937461, acc 1\n",
            "2019-01-22T06:07:37.954616: step 814, loss 0.00840249, acc 1\n",
            "2019-01-22T06:07:38.635626: step 815, loss 0.014489, acc 1\n",
            "2019-01-22T06:07:39.319387: step 816, loss 0.000834604, acc 1\n",
            "2019-01-22T06:07:40.002576: step 817, loss 0.0409803, acc 0.984375\n",
            "2019-01-22T06:07:40.687963: step 818, loss 0.0155981, acc 0.984375\n",
            "2019-01-22T06:07:41.376534: step 819, loss 0.0356422, acc 0.984375\n",
            "2019-01-22T06:07:42.058445: step 820, loss 0.00408988, acc 1\n",
            "2019-01-22T06:07:42.741353: step 821, loss 0.0308292, acc 0.984375\n",
            "2019-01-22T06:07:43.421337: step 822, loss 0.0208582, acc 0.984375\n",
            "2019-01-22T06:07:44.101729: step 823, loss 0.00165937, acc 1\n",
            "2019-01-22T06:07:44.786556: step 824, loss 0.0338875, acc 0.984375\n",
            "2019-01-22T06:07:45.213426: step 825, loss 0.0624602, acc 0.971429\n",
            "2019-01-22T06:07:45.897438: step 826, loss 0.0247101, acc 0.984375\n",
            "2019-01-22T06:07:46.581558: step 827, loss 0.0292963, acc 0.984375\n",
            "2019-01-22T06:07:47.275073: step 828, loss 0.0794363, acc 0.96875\n",
            "2019-01-22T06:07:47.960025: step 829, loss 0.0159598, acc 0.984375\n",
            "2019-01-22T06:07:48.646547: step 830, loss 0.0788348, acc 0.96875\n",
            "2019-01-22T06:07:49.336320: step 831, loss 0.00233149, acc 1\n",
            "2019-01-22T06:07:50.021227: step 832, loss 0.00152775, acc 1\n",
            "2019-01-22T06:07:50.702533: step 833, loss 0.110327, acc 0.96875\n",
            "2019-01-22T06:07:51.392659: step 834, loss 0.00603715, acc 1\n",
            "2019-01-22T06:07:52.077318: step 835, loss 0.0373191, acc 0.96875\n",
            "2019-01-22T06:07:52.763404: step 836, loss 0.0935365, acc 0.96875\n",
            "2019-01-22T06:07:53.454561: step 837, loss 0.0181834, acc 1\n",
            "2019-01-22T06:07:54.139421: step 838, loss 0.0175937, acc 1\n",
            "2019-01-22T06:07:54.824243: step 839, loss 0.00663752, acc 1\n",
            "2019-01-22T06:07:55.514680: step 840, loss 0.0539028, acc 0.96875\n",
            "2019-01-22T06:07:56.202299: step 841, loss 0.0128153, acc 1\n",
            "2019-01-22T06:07:56.885813: step 842, loss 0.00823639, acc 1\n",
            "2019-01-22T06:07:57.573699: step 843, loss 0.0175676, acc 1\n",
            "2019-01-22T06:07:58.260070: step 844, loss 0.00787068, acc 1\n",
            "2019-01-22T06:07:58.942455: step 845, loss 0.0364214, acc 0.96875\n",
            "2019-01-22T06:07:59.626239: step 846, loss 0.072815, acc 0.96875\n",
            "2019-01-22T06:08:00.309282: step 847, loss 0.0579045, acc 0.96875\n",
            "2019-01-22T06:08:00.996853: step 848, loss 0.00610862, acc 1\n",
            "2019-01-22T06:08:01.686879: step 849, loss 0.0133773, acc 0.984375\n",
            "2019-01-22T06:08:02.373220: step 850, loss 0.00283431, acc 1\n",
            "2019-01-22T06:08:03.055449: step 851, loss 0.0342764, acc 0.984375\n",
            "2019-01-22T06:08:03.742023: step 852, loss 0.000857202, acc 1\n",
            "2019-01-22T06:08:04.425038: step 853, loss 0.00716832, acc 1\n",
            "2019-01-22T06:08:05.110822: step 854, loss 0.0161174, acc 1\n",
            "2019-01-22T06:08:05.799990: step 855, loss 0.0166963, acc 0.984375\n",
            "2019-01-22T06:08:06.487404: step 856, loss 0.0480449, acc 0.984375\n",
            "2019-01-22T06:08:07.174347: step 857, loss 0.00217265, acc 1\n",
            "2019-01-22T06:08:07.592664: step 858, loss 0.0296099, acc 0.971429\n",
            "2019-01-22T06:08:08.275057: step 859, loss 0.0102212, acc 1\n",
            "2019-01-22T06:08:08.958875: step 860, loss 0.0058288, acc 1\n",
            "2019-01-22T06:08:09.644578: step 861, loss 0.00180992, acc 1\n",
            "2019-01-22T06:08:10.328493: step 862, loss 0.111856, acc 0.984375\n",
            "2019-01-22T06:08:11.014382: step 863, loss 0.00610372, acc 1\n",
            "2019-01-22T06:08:11.699470: step 864, loss 0.00101003, acc 1\n",
            "2019-01-22T06:08:12.384874: step 865, loss 0.00040766, acc 1\n",
            "2019-01-22T06:08:13.069550: step 866, loss 0.00112143, acc 1\n",
            "2019-01-22T06:08:13.751445: step 867, loss 0.00872365, acc 1\n",
            "2019-01-22T06:08:14.438098: step 868, loss 0.0225711, acc 0.984375\n",
            "2019-01-22T06:08:15.126280: step 869, loss 0.00385379, acc 1\n",
            "2019-01-22T06:08:15.809076: step 870, loss 0.0290361, acc 0.984375\n",
            "2019-01-22T06:08:16.492470: step 871, loss 0.00478235, acc 1\n",
            "2019-01-22T06:08:17.174026: step 872, loss 0.00154199, acc 1\n",
            "2019-01-22T06:08:17.854075: step 873, loss 0.0522835, acc 0.984375\n",
            "2019-01-22T06:08:18.539517: step 874, loss 0.0985119, acc 0.984375\n",
            "2019-01-22T06:08:19.225012: step 875, loss 0.151231, acc 0.984375\n",
            "2019-01-22T06:08:19.905963: step 876, loss 0.0201833, acc 0.984375\n",
            "2019-01-22T06:08:20.584904: step 877, loss 0.0384621, acc 0.984375\n",
            "2019-01-22T06:08:21.271615: step 878, loss 0.138343, acc 0.96875\n",
            "2019-01-22T06:08:21.955132: step 879, loss 0.0998933, acc 0.96875\n",
            "2019-01-22T06:08:22.637713: step 880, loss 0.00202139, acc 1\n",
            "2019-01-22T06:08:23.325406: step 881, loss 0.000690945, acc 1\n",
            "2019-01-22T06:08:24.010484: step 882, loss 0.00221328, acc 1\n",
            "2019-01-22T06:08:24.695230: step 883, loss 0.00648555, acc 1\n",
            "2019-01-22T06:08:25.381084: step 884, loss 0.00206546, acc 1\n",
            "2019-01-22T06:08:26.067061: step 885, loss 0.000256451, acc 1\n",
            "2019-01-22T06:08:26.749183: step 886, loss 0.00892431, acc 1\n",
            "2019-01-22T06:08:27.442247: step 887, loss 0.00867121, acc 1\n",
            "2019-01-22T06:08:28.129399: step 888, loss 0.00196254, acc 1\n",
            "2019-01-22T06:08:28.811833: step 889, loss 0.0173388, acc 0.984375\n",
            "2019-01-22T06:08:29.501932: step 890, loss 0.0151638, acc 1\n",
            "2019-01-22T06:08:29.918954: step 891, loss 0.0014137, acc 1\n",
            "2019-01-22T06:08:30.603903: step 892, loss 0.00427152, acc 1\n",
            "2019-01-22T06:08:31.295669: step 893, loss 0.0261376, acc 0.984375\n",
            "2019-01-22T06:08:31.980048: step 894, loss 0.00403834, acc 1\n",
            "2019-01-22T06:08:32.665323: step 895, loss 0.0112839, acc 1\n",
            "2019-01-22T06:08:33.346761: step 896, loss 0.0258031, acc 0.984375\n",
            "2019-01-22T06:08:34.030313: step 897, loss 0.0128228, acc 1\n",
            "2019-01-22T06:08:34.717811: step 898, loss 0.0167891, acc 0.984375\n",
            "2019-01-22T06:08:35.402955: step 899, loss 0.00569883, acc 1\n",
            "2019-01-22T06:08:36.084465: step 900, loss 0.0317784, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:08:36.447033: step 900, loss 0.000405992, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-900\n",
            "\n",
            "2019-01-22T06:08:37.278501: step 901, loss 0.00732466, acc 1\n",
            "2019-01-22T06:08:37.961653: step 902, loss 0.0051462, acc 1\n",
            "2019-01-22T06:08:38.639490: step 903, loss 0.0426877, acc 0.984375\n",
            "2019-01-22T06:08:39.326966: step 904, loss 0.032215, acc 0.984375\n",
            "2019-01-22T06:08:40.009683: step 905, loss 0.000484521, acc 1\n",
            "2019-01-22T06:08:40.692899: step 906, loss 0.000445727, acc 1\n",
            "2019-01-22T06:08:41.375977: step 907, loss 0.0165799, acc 1\n",
            "2019-01-22T06:08:42.059451: step 908, loss 0.12034, acc 0.96875\n",
            "2019-01-22T06:08:42.747229: step 909, loss 0.0364611, acc 0.984375\n",
            "2019-01-22T06:08:43.431117: step 910, loss 0.00403222, acc 1\n",
            "2019-01-22T06:08:44.114457: step 911, loss 0.00106854, acc 1\n",
            "2019-01-22T06:08:44.813548: step 912, loss 0.0279906, acc 0.984375\n",
            "2019-01-22T06:08:45.495503: step 913, loss 0.0446063, acc 0.984375\n",
            "2019-01-22T06:08:46.176021: step 914, loss 0.0322109, acc 0.984375\n",
            "2019-01-22T06:08:46.860902: step 915, loss 0.0658504, acc 0.953125\n",
            "2019-01-22T06:08:47.544344: step 916, loss 0.00197732, acc 1\n",
            "2019-01-22T06:08:48.226952: step 917, loss 0.0220293, acc 0.984375\n",
            "2019-01-22T06:08:48.909774: step 918, loss 0.0077304, acc 1\n",
            "2019-01-22T06:08:49.589041: step 919, loss 0.0891619, acc 0.984375\n",
            "2019-01-22T06:08:50.277923: step 920, loss 0.0107245, acc 1\n",
            "2019-01-22T06:08:50.960689: step 921, loss 0.0256833, acc 0.984375\n",
            "2019-01-22T06:08:51.639888: step 922, loss 0.0362437, acc 0.96875\n",
            "2019-01-22T06:08:52.322103: step 923, loss 0.0308414, acc 0.984375\n",
            "2019-01-22T06:08:52.739534: step 924, loss 0.0520467, acc 0.971429\n",
            "2019-01-22T06:08:53.422334: step 925, loss 0.00133211, acc 1\n",
            "2019-01-22T06:08:54.116099: step 926, loss 0.0523096, acc 0.96875\n",
            "2019-01-22T06:08:54.805550: step 927, loss 0.00266508, acc 1\n",
            "2019-01-22T06:08:55.489366: step 928, loss 0.0172801, acc 0.984375\n",
            "2019-01-22T06:08:56.179417: step 929, loss 0.0324524, acc 0.984375\n",
            "2019-01-22T06:08:56.864824: step 930, loss 0.0108002, acc 1\n",
            "2019-01-22T06:08:57.546386: step 931, loss 0.000326138, acc 1\n",
            "2019-01-22T06:08:58.239632: step 932, loss 0.0159831, acc 1\n",
            "2019-01-22T06:08:58.926068: step 933, loss 0.0847437, acc 0.953125\n",
            "2019-01-22T06:08:59.609584: step 934, loss 0.00143847, acc 1\n",
            "2019-01-22T06:09:00.298715: step 935, loss 0.0208259, acc 0.984375\n",
            "2019-01-22T06:09:00.980833: step 936, loss 0.00351416, acc 1\n",
            "2019-01-22T06:09:01.662103: step 937, loss 0.02254, acc 0.984375\n",
            "2019-01-22T06:09:02.353952: step 938, loss 0.0203137, acc 0.984375\n",
            "2019-01-22T06:09:03.036075: step 939, loss 0.0356115, acc 0.984375\n",
            "2019-01-22T06:09:03.718336: step 940, loss 0.0306924, acc 0.984375\n",
            "2019-01-22T06:09:04.409348: step 941, loss 0.00260524, acc 1\n",
            "2019-01-22T06:09:05.091943: step 942, loss 0.0882937, acc 0.984375\n",
            "2019-01-22T06:09:05.776100: step 943, loss 0.000948143, acc 1\n",
            "2019-01-22T06:09:06.464081: step 944, loss 0.0698578, acc 0.984375\n",
            "2019-01-22T06:09:07.152450: step 945, loss 0.00135778, acc 1\n",
            "2019-01-22T06:09:07.833692: step 946, loss 0.0901787, acc 0.984375\n",
            "2019-01-22T06:09:08.524350: step 947, loss 0.0354647, acc 0.984375\n",
            "2019-01-22T06:09:09.206579: step 948, loss 0.0434359, acc 0.984375\n",
            "2019-01-22T06:09:09.886716: step 949, loss 0.00504204, acc 1\n",
            "2019-01-22T06:09:10.575547: step 950, loss 0.0194871, acc 0.984375\n",
            "2019-01-22T06:09:11.263672: step 951, loss 0.00240966, acc 1\n",
            "2019-01-22T06:09:11.943615: step 952, loss 0.00911834, acc 1\n",
            "2019-01-22T06:09:12.628707: step 953, loss 0.00132953, acc 1\n",
            "2019-01-22T06:09:13.310834: step 954, loss 0.0423715, acc 0.96875\n",
            "2019-01-22T06:09:13.993940: step 955, loss 0.0126242, acc 0.984375\n",
            "2019-01-22T06:09:14.682371: step 956, loss 0.00675077, acc 1\n",
            "2019-01-22T06:09:15.097571: step 957, loss 0.0131105, acc 1\n",
            "2019-01-22T06:09:15.777139: step 958, loss 0.00914317, acc 1\n",
            "2019-01-22T06:09:16.462347: step 959, loss 0.000225909, acc 1\n",
            "2019-01-22T06:09:17.144366: step 960, loss 0.00180284, acc 1\n",
            "2019-01-22T06:09:17.824457: step 961, loss 0.0296879, acc 0.96875\n",
            "2019-01-22T06:09:18.510639: step 962, loss 0.00508352, acc 1\n",
            "2019-01-22T06:09:19.195913: step 963, loss 0.0178002, acc 0.984375\n",
            "2019-01-22T06:09:19.875330: step 964, loss 0.0500516, acc 0.984375\n",
            "2019-01-22T06:09:20.556449: step 965, loss 0.0109105, acc 1\n",
            "2019-01-22T06:09:21.237085: step 966, loss 0.00289376, acc 1\n",
            "2019-01-22T06:09:21.920473: step 967, loss 0.00726797, acc 1\n",
            "2019-01-22T06:09:22.605843: step 968, loss 0.000378009, acc 1\n",
            "2019-01-22T06:09:23.287846: step 969, loss 0.000925425, acc 1\n",
            "2019-01-22T06:09:23.978027: step 970, loss 0.0118989, acc 1\n",
            "2019-01-22T06:09:24.662591: step 971, loss 0.00378139, acc 1\n",
            "2019-01-22T06:09:25.347564: step 972, loss 0.00145516, acc 1\n",
            "2019-01-22T06:09:26.039914: step 973, loss 0.0109266, acc 1\n",
            "2019-01-22T06:09:26.724058: step 974, loss 0.00818753, acc 1\n",
            "2019-01-22T06:09:27.405651: step 975, loss 0.00460601, acc 1\n",
            "2019-01-22T06:09:28.091921: step 976, loss 0.00088619, acc 1\n",
            "2019-01-22T06:09:28.777083: step 977, loss 0.00299782, acc 1\n",
            "2019-01-22T06:09:29.459251: step 978, loss 0.124236, acc 0.984375\n",
            "2019-01-22T06:09:30.152030: step 979, loss 0.0198915, acc 0.984375\n",
            "2019-01-22T06:09:30.833650: step 980, loss 0.0811818, acc 0.96875\n",
            "2019-01-22T06:09:31.517200: step 981, loss 0.00754959, acc 1\n",
            "2019-01-22T06:09:32.205792: step 982, loss 0.00688675, acc 1\n",
            "2019-01-22T06:09:32.888972: step 983, loss 0.0189116, acc 0.984375\n",
            "2019-01-22T06:09:33.570324: step 984, loss 0.0189587, acc 0.984375\n",
            "2019-01-22T06:09:34.258620: step 985, loss 0.0310942, acc 0.984375\n",
            "2019-01-22T06:09:34.940392: step 986, loss 0.0181589, acc 0.984375\n",
            "2019-01-22T06:09:35.622093: step 987, loss 0.0222313, acc 0.984375\n",
            "2019-01-22T06:09:36.311590: step 988, loss 0.00399506, acc 1\n",
            "2019-01-22T06:09:37.000320: step 989, loss 0.00432962, acc 1\n",
            "2019-01-22T06:09:37.413910: step 990, loss 0.000473128, acc 1\n",
            "2019-01-22T06:09:38.098641: step 991, loss 0.0268853, acc 0.984375\n",
            "2019-01-22T06:09:38.783923: step 992, loss 0.00144156, acc 1\n",
            "2019-01-22T06:09:39.471028: step 993, loss 0.0325839, acc 0.96875\n",
            "2019-01-22T06:09:40.156926: step 994, loss 0.00307706, acc 1\n",
            "2019-01-22T06:09:40.842293: step 995, loss 0.0371988, acc 0.96875\n",
            "2019-01-22T06:09:41.523748: step 996, loss 0.00242138, acc 1\n",
            "2019-01-22T06:09:42.207356: step 997, loss 0.206232, acc 0.96875\n",
            "2019-01-22T06:09:42.890472: step 998, loss 0.00850028, acc 1\n",
            "2019-01-22T06:09:43.568799: step 999, loss 0.0887762, acc 0.984375\n",
            "2019-01-22T06:09:44.247125: step 1000, loss 0.0402121, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:09:44.605752: step 1000, loss 0.000186655, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1000\n",
            "\n",
            "2019-01-22T06:09:45.455012: step 1001, loss 0.00304037, acc 1\n",
            "2019-01-22T06:09:46.138398: step 1002, loss 0.000393935, acc 1\n",
            "2019-01-22T06:09:46.822931: step 1003, loss 0.000637763, acc 1\n",
            "2019-01-22T06:09:47.517400: step 1004, loss 0.0419658, acc 0.984375\n",
            "2019-01-22T06:09:48.201522: step 1005, loss 0.000845566, acc 1\n",
            "2019-01-22T06:09:48.885447: step 1006, loss 0.00175138, acc 1\n",
            "2019-01-22T06:09:49.567520: step 1007, loss 0.0262635, acc 0.984375\n",
            "2019-01-22T06:09:50.251935: step 1008, loss 0.000525613, acc 1\n",
            "2019-01-22T06:09:50.934778: step 1009, loss 0.00421514, acc 1\n",
            "2019-01-22T06:09:51.612284: step 1010, loss 0.0142793, acc 1\n",
            "2019-01-22T06:09:52.291723: step 1011, loss 0.0316326, acc 0.984375\n",
            "2019-01-22T06:09:52.971709: step 1012, loss 0.020386, acc 0.984375\n",
            "2019-01-22T06:09:53.654412: step 1013, loss 0.000249402, acc 1\n",
            "2019-01-22T06:09:54.340096: step 1014, loss 0.000923906, acc 1\n",
            "2019-01-22T06:09:55.020764: step 1015, loss 0.00846543, acc 1\n",
            "2019-01-22T06:09:55.701032: step 1016, loss 0.0152714, acc 1\n",
            "2019-01-22T06:09:56.382340: step 1017, loss 0.0103343, acc 1\n",
            "2019-01-22T06:09:57.066111: step 1018, loss 0.0645839, acc 0.984375\n",
            "2019-01-22T06:09:57.747467: step 1019, loss 0.013109, acc 0.984375\n",
            "2019-01-22T06:09:58.430303: step 1020, loss 0.000736748, acc 1\n",
            "2019-01-22T06:09:59.113796: step 1021, loss 0.00131892, acc 1\n",
            "2019-01-22T06:09:59.800616: step 1022, loss 0.045296, acc 0.984375\n",
            "2019-01-22T06:10:00.214017: step 1023, loss 0.00021115, acc 1\n",
            "2019-01-22T06:10:00.897933: step 1024, loss 0.00034636, acc 1\n",
            "2019-01-22T06:10:01.586326: step 1025, loss 0.0856782, acc 0.984375\n",
            "2019-01-22T06:10:02.267017: step 1026, loss 0.00158038, acc 1\n",
            "2019-01-22T06:10:02.950204: step 1027, loss 0.00189448, acc 1\n",
            "2019-01-22T06:10:03.638664: step 1028, loss 0.00151661, acc 1\n",
            "2019-01-22T06:10:04.320115: step 1029, loss 0.0295054, acc 0.984375\n",
            "2019-01-22T06:10:05.004071: step 1030, loss 0.00872076, acc 1\n",
            "2019-01-22T06:10:05.688846: step 1031, loss 0.00653889, acc 1\n",
            "2019-01-22T06:10:06.373075: step 1032, loss 0.0281255, acc 0.984375\n",
            "2019-01-22T06:10:07.057466: step 1033, loss 0.0035079, acc 1\n",
            "2019-01-22T06:10:07.739396: step 1034, loss 0.00114999, acc 1\n",
            "2019-01-22T06:10:08.421514: step 1035, loss 0.00372202, acc 1\n",
            "2019-01-22T06:10:09.106463: step 1036, loss 0.000717979, acc 1\n",
            "2019-01-22T06:10:09.791646: step 1037, loss 0.0222079, acc 0.984375\n",
            "2019-01-22T06:10:10.479328: step 1038, loss 0.0453826, acc 0.984375\n",
            "2019-01-22T06:10:11.163879: step 1039, loss 0.0278368, acc 0.984375\n",
            "2019-01-22T06:10:11.849022: step 1040, loss 0.000389019, acc 1\n",
            "2019-01-22T06:10:12.531762: step 1041, loss 0.000451637, acc 1\n",
            "2019-01-22T06:10:13.215818: step 1042, loss 0.00233377, acc 1\n",
            "2019-01-22T06:10:13.900345: step 1043, loss 0.000494028, acc 1\n",
            "2019-01-22T06:10:14.577969: step 1044, loss 0.000331401, acc 1\n",
            "2019-01-22T06:10:15.258775: step 1045, loss 0.12236, acc 0.953125\n",
            "2019-01-22T06:10:15.945750: step 1046, loss 0.0198309, acc 0.984375\n",
            "2019-01-22T06:10:16.625768: step 1047, loss 0.00460521, acc 1\n",
            "2019-01-22T06:10:17.307696: step 1048, loss 0.00460723, acc 1\n",
            "2019-01-22T06:10:17.993240: step 1049, loss 0.0706664, acc 0.984375\n",
            "2019-01-22T06:10:18.672934: step 1050, loss 0.0981514, acc 0.96875\n",
            "2019-01-22T06:10:19.358883: step 1051, loss 0.071921, acc 0.96875\n",
            "2019-01-22T06:10:20.041557: step 1052, loss 0.0485312, acc 0.984375\n",
            "2019-01-22T06:10:20.725296: step 1053, loss 0.030381, acc 0.984375\n",
            "2019-01-22T06:10:21.406961: step 1054, loss 0.00847984, acc 1\n",
            "2019-01-22T06:10:22.090877: step 1055, loss 0.013091, acc 0.984375\n",
            "2019-01-22T06:10:22.515592: step 1056, loss 8.35098e-06, acc 1\n",
            "2019-01-22T06:10:23.196863: step 1057, loss 0.000833855, acc 1\n",
            "2019-01-22T06:10:23.880785: step 1058, loss 0.00358757, acc 1\n",
            "2019-01-22T06:10:24.572083: step 1059, loss 0.00307727, acc 1\n",
            "2019-01-22T06:10:25.255911: step 1060, loss 0.0366601, acc 0.984375\n",
            "2019-01-22T06:10:25.939593: step 1061, loss 0.00111754, acc 1\n",
            "2019-01-22T06:10:26.630518: step 1062, loss 0.000511201, acc 1\n",
            "2019-01-22T06:10:27.313406: step 1063, loss 0.00117257, acc 1\n",
            "2019-01-22T06:10:27.997761: step 1064, loss 0.000574625, acc 1\n",
            "2019-01-22T06:10:28.686413: step 1065, loss 0.00960349, acc 1\n",
            "2019-01-22T06:10:29.366575: step 1066, loss 0.00613507, acc 1\n",
            "2019-01-22T06:10:30.051341: step 1067, loss 0.000386393, acc 1\n",
            "2019-01-22T06:10:30.739066: step 1068, loss 0.00412391, acc 1\n",
            "2019-01-22T06:10:31.422915: step 1069, loss 0.00827707, acc 1\n",
            "2019-01-22T06:10:32.108998: step 1070, loss 0.00420894, acc 1\n",
            "2019-01-22T06:10:32.798415: step 1071, loss 0.0133275, acc 1\n",
            "2019-01-22T06:10:33.482442: step 1072, loss 0.00439564, acc 1\n",
            "2019-01-22T06:10:34.163223: step 1073, loss 0.00746365, acc 1\n",
            "2019-01-22T06:10:34.852640: step 1074, loss 0.00959746, acc 1\n",
            "2019-01-22T06:10:35.536402: step 1075, loss 0.0202253, acc 0.984375\n",
            "2019-01-22T06:10:36.219587: step 1076, loss 0.0230289, acc 0.984375\n",
            "2019-01-22T06:10:36.906826: step 1077, loss 0.00592582, acc 1\n",
            "2019-01-22T06:10:37.593368: step 1078, loss 0.0179894, acc 0.984375\n",
            "2019-01-22T06:10:38.274902: step 1079, loss 0.0382056, acc 0.984375\n",
            "2019-01-22T06:10:38.966819: step 1080, loss 0.0413782, acc 0.984375\n",
            "2019-01-22T06:10:39.650927: step 1081, loss 0.0358477, acc 0.984375\n",
            "2019-01-22T06:10:40.337871: step 1082, loss 0.000505928, acc 1\n",
            "2019-01-22T06:10:41.027417: step 1083, loss 0.0380446, acc 0.984375\n",
            "2019-01-22T06:10:41.708333: step 1084, loss 0.00106833, acc 1\n",
            "2019-01-22T06:10:42.390817: step 1085, loss 0.00425155, acc 1\n",
            "2019-01-22T06:10:43.082406: step 1086, loss 0.0504938, acc 0.96875\n",
            "2019-01-22T06:10:43.762776: step 1087, loss 0.00355426, acc 1\n",
            "2019-01-22T06:10:44.443767: step 1088, loss 0.0027644, acc 1\n",
            "2019-01-22T06:10:44.857632: step 1089, loss 3.46671e-05, acc 1\n",
            "2019-01-22T06:10:45.540788: step 1090, loss 0.00657706, acc 1\n",
            "2019-01-22T06:10:46.221215: step 1091, loss 0.000913281, acc 1\n",
            "2019-01-22T06:10:46.902317: step 1092, loss 0.0191091, acc 0.984375\n",
            "2019-01-22T06:10:47.582575: step 1093, loss 0.0256555, acc 0.984375\n",
            "2019-01-22T06:10:48.260575: step 1094, loss 0.000154358, acc 1\n",
            "2019-01-22T06:10:48.943977: step 1095, loss 0.000562312, acc 1\n",
            "2019-01-22T06:10:49.628904: step 1096, loss 0.0452576, acc 0.984375\n",
            "2019-01-22T06:10:50.309949: step 1097, loss 0.0020585, acc 1\n",
            "2019-01-22T06:10:50.999214: step 1098, loss 0.0646837, acc 0.984375\n",
            "2019-01-22T06:10:51.680713: step 1099, loss 0.0325437, acc 0.984375\n",
            "2019-01-22T06:10:52.363100: step 1100, loss 0.0203759, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:10:52.727095: step 1100, loss 0.000223912, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1100\n",
            "\n",
            "2019-01-22T06:10:53.538243: step 1101, loss 0.00534466, acc 1\n",
            "2019-01-22T06:10:54.215982: step 1102, loss 0.000971193, acc 1\n",
            "2019-01-22T06:10:54.901459: step 1103, loss 0.000876303, acc 1\n",
            "2019-01-22T06:10:55.582353: step 1104, loss 0.00228087, acc 1\n",
            "2019-01-22T06:10:56.266185: step 1105, loss 0.00779973, acc 1\n",
            "2019-01-22T06:10:56.952133: step 1106, loss 0.0103843, acc 1\n",
            "2019-01-22T06:10:57.634702: step 1107, loss 0.0392093, acc 0.984375\n",
            "2019-01-22T06:10:58.323777: step 1108, loss 0.0124385, acc 0.984375\n",
            "2019-01-22T06:10:59.006581: step 1109, loss 0.00236742, acc 1\n",
            "2019-01-22T06:10:59.689536: step 1110, loss 0.000798836, acc 1\n",
            "2019-01-22T06:11:00.374613: step 1111, loss 0.0427066, acc 0.96875\n",
            "2019-01-22T06:11:01.061447: step 1112, loss 0.0545159, acc 0.96875\n",
            "2019-01-22T06:11:01.742031: step 1113, loss 0.000611233, acc 1\n",
            "2019-01-22T06:11:02.426498: step 1114, loss 0.019007, acc 1\n",
            "2019-01-22T06:11:03.108691: step 1115, loss 0.160787, acc 0.96875\n",
            "2019-01-22T06:11:03.788986: step 1116, loss 0.0078877, acc 1\n",
            "2019-01-22T06:11:04.472381: step 1117, loss 0.00942321, acc 1\n",
            "2019-01-22T06:11:05.156707: step 1118, loss 0.0184216, acc 0.984375\n",
            "2019-01-22T06:11:05.834091: step 1119, loss 0.00250169, acc 1\n",
            "2019-01-22T06:11:06.512303: step 1120, loss 0.00086563, acc 1\n",
            "2019-01-22T06:11:07.192369: step 1121, loss 0.0789763, acc 0.984375\n",
            "2019-01-22T06:11:07.608609: step 1122, loss 0.109073, acc 0.971429\n",
            "2019-01-22T06:11:08.287307: step 1123, loss 0.000122899, acc 1\n",
            "2019-01-22T06:11:08.971845: step 1124, loss 0.000861997, acc 1\n",
            "2019-01-22T06:11:09.653253: step 1125, loss 0.000947293, acc 1\n",
            "2019-01-22T06:11:10.334668: step 1126, loss 0.00667267, acc 1\n",
            "2019-01-22T06:11:11.018407: step 1127, loss 0.00194115, acc 1\n",
            "2019-01-22T06:11:11.699573: step 1128, loss 0.00134045, acc 1\n",
            "2019-01-22T06:11:12.389137: step 1129, loss 0.0332933, acc 0.984375\n",
            "2019-01-22T06:11:13.070985: step 1130, loss 0.00730155, acc 1\n",
            "2019-01-22T06:11:13.761452: step 1131, loss 0.00411073, acc 1\n",
            "2019-01-22T06:11:14.443598: step 1132, loss 0.06826, acc 0.984375\n",
            "2019-01-22T06:11:15.124297: step 1133, loss 0.000998091, acc 1\n",
            "2019-01-22T06:11:15.808145: step 1134, loss 0.000185955, acc 1\n",
            "2019-01-22T06:11:16.496840: step 1135, loss 0.000603432, acc 1\n",
            "2019-01-22T06:11:17.177656: step 1136, loss 0.0266738, acc 0.984375\n",
            "2019-01-22T06:11:17.852915: step 1137, loss 0.000936563, acc 1\n",
            "2019-01-22T06:11:18.532906: step 1138, loss 0.00136689, acc 1\n",
            "2019-01-22T06:11:19.215832: step 1139, loss 0.024597, acc 0.984375\n",
            "2019-01-22T06:11:19.894350: step 1140, loss 0.00198084, acc 1\n",
            "2019-01-22T06:11:20.576842: step 1141, loss 0.00318408, acc 1\n",
            "2019-01-22T06:11:21.258283: step 1142, loss 0.00258602, acc 1\n",
            "2019-01-22T06:11:21.941461: step 1143, loss 0.00554239, acc 1\n",
            "2019-01-22T06:11:22.623438: step 1144, loss 0.000492837, acc 1\n",
            "2019-01-22T06:11:23.304692: step 1145, loss 0.00447122, acc 1\n",
            "2019-01-22T06:11:23.989947: step 1146, loss 0.0154106, acc 0.984375\n",
            "2019-01-22T06:11:24.675844: step 1147, loss 0.000919108, acc 1\n",
            "2019-01-22T06:11:25.357621: step 1148, loss 0.0354572, acc 0.96875\n",
            "2019-01-22T06:11:26.041363: step 1149, loss 0.20452, acc 0.984375\n",
            "2019-01-22T06:11:26.724300: step 1150, loss 0.0253384, acc 0.984375\n",
            "2019-01-22T06:11:27.407599: step 1151, loss 0.00341334, acc 1\n",
            "2019-01-22T06:11:28.088934: step 1152, loss 0.0172662, acc 0.984375\n",
            "2019-01-22T06:11:28.768868: step 1153, loss 0.0274265, acc 0.984375\n",
            "2019-01-22T06:11:29.455114: step 1154, loss 0.0143104, acc 0.984375\n",
            "2019-01-22T06:11:29.867105: step 1155, loss 0.000575948, acc 1\n",
            "2019-01-22T06:11:30.547951: step 1156, loss 0.000616686, acc 1\n",
            "2019-01-22T06:11:31.226106: step 1157, loss 0.00718855, acc 1\n",
            "2019-01-22T06:11:31.905139: step 1158, loss 0.00959903, acc 1\n",
            "2019-01-22T06:11:32.588577: step 1159, loss 0.000581212, acc 1\n",
            "2019-01-22T06:11:33.273681: step 1160, loss 0.000917783, acc 1\n",
            "2019-01-22T06:11:33.955826: step 1161, loss 0.00658681, acc 1\n",
            "2019-01-22T06:11:34.636322: step 1162, loss 0.00176699, acc 1\n",
            "2019-01-22T06:11:35.322307: step 1163, loss 0.0087099, acc 1\n",
            "2019-01-22T06:11:36.006441: step 1164, loss 0.00670803, acc 1\n",
            "2019-01-22T06:11:36.689219: step 1165, loss 0.00856378, acc 1\n",
            "2019-01-22T06:11:37.371399: step 1166, loss 0.00029499, acc 1\n",
            "2019-01-22T06:11:38.055897: step 1167, loss 0.00669643, acc 1\n",
            "2019-01-22T06:11:38.739277: step 1168, loss 0.0125047, acc 1\n",
            "2019-01-22T06:11:39.423509: step 1169, loss 0.000473344, acc 1\n",
            "2019-01-22T06:11:40.107029: step 1170, loss 0.00400465, acc 1\n",
            "2019-01-22T06:11:40.788660: step 1171, loss 0.00102097, acc 1\n",
            "2019-01-22T06:11:41.473817: step 1172, loss 0.00138589, acc 1\n",
            "2019-01-22T06:11:42.154732: step 1173, loss 0.00161273, acc 1\n",
            "2019-01-22T06:11:42.838002: step 1174, loss 0.00456735, acc 1\n",
            "2019-01-22T06:11:43.522973: step 1175, loss 0.00135667, acc 1\n",
            "2019-01-22T06:11:44.206478: step 1176, loss 0.00219004, acc 1\n",
            "2019-01-22T06:11:44.886571: step 1177, loss 0.000525831, acc 1\n",
            "2019-01-22T06:11:45.567908: step 1178, loss 0.000840625, acc 1\n",
            "2019-01-22T06:11:46.248327: step 1179, loss 0.0044247, acc 1\n",
            "2019-01-22T06:11:46.938416: step 1180, loss 0.000349473, acc 1\n",
            "2019-01-22T06:11:47.620657: step 1181, loss 0.000114815, acc 1\n",
            "2019-01-22T06:11:48.302987: step 1182, loss 0.00691499, acc 1\n",
            "2019-01-22T06:11:48.989019: step 1183, loss 0.149617, acc 0.953125\n",
            "2019-01-22T06:11:49.673362: step 1184, loss 0.00440862, acc 1\n",
            "2019-01-22T06:11:50.358681: step 1185, loss 0.00499365, acc 1\n",
            "2019-01-22T06:11:51.049586: step 1186, loss 0.00545205, acc 1\n",
            "2019-01-22T06:11:51.732317: step 1187, loss 0.0133392, acc 1\n",
            "2019-01-22T06:11:52.146143: step 1188, loss 0.000135786, acc 1\n",
            "2019-01-22T06:11:52.828384: step 1189, loss 0.0451092, acc 0.96875\n",
            "2019-01-22T06:11:53.509585: step 1190, loss 0.0214449, acc 0.984375\n",
            "2019-01-22T06:11:54.194083: step 1191, loss 0.00131367, acc 1\n",
            "2019-01-22T06:11:54.877953: step 1192, loss 0.00112259, acc 1\n",
            "2019-01-22T06:11:55.563335: step 1193, loss 0.0477261, acc 0.984375\n",
            "2019-01-22T06:11:56.245830: step 1194, loss 0.0688301, acc 0.984375\n",
            "2019-01-22T06:11:56.925147: step 1195, loss 0.00130827, acc 1\n",
            "2019-01-22T06:11:57.607407: step 1196, loss 0.0128579, acc 0.984375\n",
            "2019-01-22T06:11:58.286789: step 1197, loss 0.0049211, acc 1\n",
            "2019-01-22T06:11:58.972868: step 1198, loss 0.0623042, acc 0.96875\n",
            "2019-01-22T06:11:59.650726: step 1199, loss 0.00526759, acc 1\n",
            "2019-01-22T06:12:00.331258: step 1200, loss 0.00569846, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:12:00.691103: step 1200, loss 0.000173804, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1200\n",
            "\n",
            "2019-01-22T06:12:01.527114: step 1201, loss 0.00216068, acc 1\n",
            "2019-01-22T06:12:02.210721: step 1202, loss 0.0521044, acc 0.984375\n",
            "2019-01-22T06:12:02.893108: step 1203, loss 0.0579585, acc 0.984375\n",
            "2019-01-22T06:12:03.574850: step 1204, loss 0.00123117, acc 1\n",
            "2019-01-22T06:12:04.255596: step 1205, loss 0.0626554, acc 0.96875\n",
            "2019-01-22T06:12:04.934718: step 1206, loss 0.000412782, acc 1\n",
            "2019-01-22T06:12:05.619653: step 1207, loss 0.167561, acc 0.984375\n",
            "2019-01-22T06:12:06.303561: step 1208, loss 0.000426575, acc 1\n",
            "2019-01-22T06:12:06.987727: step 1209, loss 0.000844799, acc 1\n",
            "2019-01-22T06:12:07.671344: step 1210, loss 0.00180949, acc 1\n",
            "2019-01-22T06:12:08.354135: step 1211, loss 0.00374742, acc 1\n",
            "2019-01-22T06:12:09.036570: step 1212, loss 0.00425947, acc 1\n",
            "2019-01-22T06:12:09.719266: step 1213, loss 0.00147369, acc 1\n",
            "2019-01-22T06:12:10.401649: step 1214, loss 0.00620485, acc 1\n",
            "2019-01-22T06:12:11.085560: step 1215, loss 0.00066886, acc 1\n",
            "2019-01-22T06:12:11.767977: step 1216, loss 0.012362, acc 1\n",
            "2019-01-22T06:12:12.452671: step 1217, loss 0.0229171, acc 1\n",
            "2019-01-22T06:12:13.138172: step 1218, loss 0.192287, acc 0.984375\n",
            "2019-01-22T06:12:13.816883: step 1219, loss 0.000287565, acc 1\n",
            "2019-01-22T06:12:14.499762: step 1220, loss 0.00988477, acc 1\n",
            "2019-01-22T06:12:14.913305: step 1221, loss 0.199629, acc 0.971429\n",
            "2019-01-22T06:12:15.598778: step 1222, loss 0.00129521, acc 1\n",
            "2019-01-22T06:12:16.282056: step 1223, loss 0.0501857, acc 0.984375\n",
            "2019-01-22T06:12:16.969481: step 1224, loss 0.00581727, acc 1\n",
            "2019-01-22T06:12:17.654621: step 1225, loss 0.0493662, acc 0.984375\n",
            "2019-01-22T06:12:18.336968: step 1226, loss 0.00178693, acc 1\n",
            "2019-01-22T06:12:19.021878: step 1227, loss 0.0222706, acc 0.984375\n",
            "2019-01-22T06:12:19.702874: step 1228, loss 0.000373683, acc 1\n",
            "2019-01-22T06:12:20.387223: step 1229, loss 0.0864779, acc 0.96875\n",
            "2019-01-22T06:12:21.072695: step 1230, loss 0.00366797, acc 1\n",
            "2019-01-22T06:12:21.753294: step 1231, loss 0.00124605, acc 1\n",
            "2019-01-22T06:12:22.435610: step 1232, loss 0.000324883, acc 1\n",
            "2019-01-22T06:12:23.116361: step 1233, loss 0.00176902, acc 1\n",
            "2019-01-22T06:12:23.803596: step 1234, loss 0.00173283, acc 1\n",
            "2019-01-22T06:12:24.486790: step 1235, loss 0.00725405, acc 1\n",
            "2019-01-22T06:12:25.170602: step 1236, loss 0.0235139, acc 0.984375\n",
            "2019-01-22T06:12:25.853940: step 1237, loss 0.000705487, acc 1\n",
            "2019-01-22T06:12:26.538271: step 1238, loss 0.0135174, acc 0.984375\n",
            "2019-01-22T06:12:27.220909: step 1239, loss 0.0234636, acc 0.984375\n",
            "2019-01-22T06:12:27.908695: step 1240, loss 0.0124153, acc 1\n",
            "2019-01-22T06:12:28.590580: step 1241, loss 0.00985876, acc 1\n",
            "2019-01-22T06:12:29.271915: step 1242, loss 0.000708162, acc 1\n",
            "2019-01-22T06:12:29.953641: step 1243, loss 0.0159746, acc 0.984375\n",
            "2019-01-22T06:12:30.634049: step 1244, loss 0.000648723, acc 1\n",
            "2019-01-22T06:12:31.320392: step 1245, loss 0.0452249, acc 0.984375\n",
            "2019-01-22T06:12:32.002796: step 1246, loss 0.00167716, acc 1\n",
            "2019-01-22T06:12:32.685399: step 1247, loss 0.0680669, acc 0.984375\n",
            "2019-01-22T06:12:33.366809: step 1248, loss 0.00449574, acc 1\n",
            "2019-01-22T06:12:34.046297: step 1249, loss 0.00518792, acc 1\n",
            "2019-01-22T06:12:34.731304: step 1250, loss 0.000396096, acc 1\n",
            "2019-01-22T06:12:35.414964: step 1251, loss 0.0215839, acc 1\n",
            "2019-01-22T06:12:36.096685: step 1252, loss 0.0499727, acc 0.984375\n",
            "2019-01-22T06:12:36.779856: step 1253, loss 0.0858169, acc 0.984375\n",
            "2019-01-22T06:12:37.199357: step 1254, loss 0.00693226, acc 1\n",
            "2019-01-22T06:12:37.880433: step 1255, loss 0.00979761, acc 1\n",
            "2019-01-22T06:12:38.562464: step 1256, loss 0.00360686, acc 1\n",
            "2019-01-22T06:12:39.246263: step 1257, loss 0.030587, acc 0.96875\n",
            "2019-01-22T06:12:39.929835: step 1258, loss 0.00200121, acc 1\n",
            "2019-01-22T06:12:40.612320: step 1259, loss 0.0425343, acc 0.984375\n",
            "2019-01-22T06:12:41.297578: step 1260, loss 0.00894559, acc 1\n",
            "2019-01-22T06:12:41.984526: step 1261, loss 0.00344503, acc 1\n",
            "2019-01-22T06:12:42.670570: step 1262, loss 0.000375612, acc 1\n",
            "2019-01-22T06:12:43.352474: step 1263, loss 0.00580098, acc 1\n",
            "2019-01-22T06:12:44.034057: step 1264, loss 0.00996029, acc 1\n",
            "2019-01-22T06:12:44.717679: step 1265, loss 0.0417596, acc 0.984375\n",
            "2019-01-22T06:12:45.400067: step 1266, loss 0.000213406, acc 1\n",
            "2019-01-22T06:12:46.088348: step 1267, loss 0.0779031, acc 0.984375\n",
            "2019-01-22T06:12:46.769256: step 1268, loss 0.000526634, acc 1\n",
            "2019-01-22T06:12:47.452033: step 1269, loss 0.00189019, acc 1\n",
            "2019-01-22T06:12:48.134758: step 1270, loss 0.00455991, acc 1\n",
            "2019-01-22T06:12:48.818297: step 1271, loss 0.0076816, acc 1\n",
            "2019-01-22T06:12:49.501947: step 1272, loss 0.0213818, acc 1\n",
            "2019-01-22T06:12:50.185936: step 1273, loss 0.00226881, acc 1\n",
            "2019-01-22T06:12:50.865742: step 1274, loss 0.0286931, acc 0.984375\n",
            "2019-01-22T06:12:51.547546: step 1275, loss 0.00506139, acc 1\n",
            "2019-01-22T06:12:52.230219: step 1276, loss 0.0217862, acc 0.984375\n",
            "2019-01-22T06:12:52.915404: step 1277, loss 0.000233419, acc 1\n",
            "2019-01-22T06:12:53.597184: step 1278, loss 0.000484262, acc 1\n",
            "2019-01-22T06:12:54.277433: step 1279, loss 0.000199775, acc 1\n",
            "2019-01-22T06:12:54.966843: step 1280, loss 0.0601138, acc 0.953125\n",
            "2019-01-22T06:12:55.650341: step 1281, loss 0.0149046, acc 0.984375\n",
            "2019-01-22T06:12:56.334971: step 1282, loss 0.00108704, acc 1\n",
            "2019-01-22T06:12:57.016867: step 1283, loss 0.0313059, acc 0.984375\n",
            "2019-01-22T06:12:57.700023: step 1284, loss 0.0012979, acc 1\n",
            "2019-01-22T06:12:58.381735: step 1285, loss 0.00216465, acc 1\n",
            "2019-01-22T06:12:59.064446: step 1286, loss 0.066809, acc 0.984375\n",
            "2019-01-22T06:12:59.481377: step 1287, loss 0.000283348, acc 1\n",
            "2019-01-22T06:13:00.158964: step 1288, loss 0.00931555, acc 1\n",
            "2019-01-22T06:13:00.843130: step 1289, loss 0.00141592, acc 1\n",
            "2019-01-22T06:13:01.527269: step 1290, loss 0.000145245, acc 1\n",
            "2019-01-22T06:13:02.208348: step 1291, loss 0.00135877, acc 1\n",
            "2019-01-22T06:13:02.889021: step 1292, loss 0.0006914, acc 1\n",
            "2019-01-22T06:13:03.570148: step 1293, loss 0.00553558, acc 1\n",
            "2019-01-22T06:13:04.250106: step 1294, loss 0.0149431, acc 1\n",
            "2019-01-22T06:13:04.930376: step 1295, loss 0.0420053, acc 0.984375\n",
            "2019-01-22T06:13:05.610595: step 1296, loss 0.00750785, acc 1\n",
            "2019-01-22T06:13:06.294770: step 1297, loss 5.81841e-05, acc 1\n",
            "2019-01-22T06:13:06.981003: step 1298, loss 5.64923e-05, acc 1\n",
            "2019-01-22T06:13:07.662779: step 1299, loss 0.00466016, acc 1\n",
            "2019-01-22T06:13:08.341832: step 1300, loss 0.000224789, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:13:08.703255: step 1300, loss 0.000148794, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1300\n",
            "\n",
            "2019-01-22T06:13:09.555120: step 1301, loss 0.00194921, acc 1\n",
            "2019-01-22T06:13:10.234401: step 1302, loss 0.0368717, acc 0.984375\n",
            "2019-01-22T06:13:10.917601: step 1303, loss 0.00145765, acc 1\n",
            "2019-01-22T06:13:11.603462: step 1304, loss 0.00486739, acc 1\n",
            "2019-01-22T06:13:12.288029: step 1305, loss 0.00239685, acc 1\n",
            "2019-01-22T06:13:12.970969: step 1306, loss 0.00476586, acc 1\n",
            "2019-01-22T06:13:13.651872: step 1307, loss 0.00204961, acc 1\n",
            "2019-01-22T06:13:14.335512: step 1308, loss 0.0365109, acc 0.984375\n",
            "2019-01-22T06:13:15.017978: step 1309, loss 0.00846345, acc 1\n",
            "2019-01-22T06:13:15.701364: step 1310, loss 0.0162812, acc 0.984375\n",
            "2019-01-22T06:13:16.385096: step 1311, loss 0.00459449, acc 1\n",
            "2019-01-22T06:13:17.066600: step 1312, loss 0.000856774, acc 1\n",
            "2019-01-22T06:13:17.748035: step 1313, loss 0.124158, acc 0.984375\n",
            "2019-01-22T06:13:18.431484: step 1314, loss 0.0259964, acc 0.984375\n",
            "2019-01-22T06:13:19.118480: step 1315, loss 0.0429214, acc 0.984375\n",
            "2019-01-22T06:13:19.803833: step 1316, loss 0.00343849, acc 1\n",
            "2019-01-22T06:13:20.482960: step 1317, loss 0.00136797, acc 1\n",
            "2019-01-22T06:13:21.166700: step 1318, loss 0.00291817, acc 1\n",
            "2019-01-22T06:13:21.845117: step 1319, loss 0.000309787, acc 1\n",
            "2019-01-22T06:13:22.259577: step 1320, loss 0.00234153, acc 1\n",
            "2019-01-22T06:13:22.942140: step 1321, loss 0.0014753, acc 1\n",
            "2019-01-22T06:13:23.625764: step 1322, loss 0.00030044, acc 1\n",
            "2019-01-22T06:13:24.309989: step 1323, loss 0.0156188, acc 0.984375\n",
            "2019-01-22T06:13:24.994531: step 1324, loss 0.0183819, acc 0.984375\n",
            "2019-01-22T06:13:25.678332: step 1325, loss 2.28979e-05, acc 1\n",
            "2019-01-22T06:13:26.364376: step 1326, loss 5.56864e-05, acc 1\n",
            "2019-01-22T06:13:27.044900: step 1327, loss 0.00248118, acc 1\n",
            "2019-01-22T06:13:27.731488: step 1328, loss 0.00262111, acc 1\n",
            "2019-01-22T06:13:28.415347: step 1329, loss 0.00165568, acc 1\n",
            "2019-01-22T06:13:29.100939: step 1330, loss 0.00459794, acc 1\n",
            "2019-01-22T06:13:29.789320: step 1331, loss 0.0028157, acc 1\n",
            "2019-01-22T06:13:30.478628: step 1332, loss 0.0606035, acc 0.984375\n",
            "2019-01-22T06:13:31.160021: step 1333, loss 0.000112237, acc 1\n",
            "2019-01-22T06:13:31.849576: step 1334, loss 0.000808366, acc 1\n",
            "2019-01-22T06:13:32.530060: step 1335, loss 0.0209735, acc 0.984375\n",
            "2019-01-22T06:13:33.210271: step 1336, loss 0.000804151, acc 1\n",
            "2019-01-22T06:13:33.900683: step 1337, loss 0.000813118, acc 1\n",
            "2019-01-22T06:13:34.586006: step 1338, loss 0.000647437, acc 1\n",
            "2019-01-22T06:13:35.267855: step 1339, loss 0.014893, acc 0.984375\n",
            "2019-01-22T06:13:35.956111: step 1340, loss 4.71694e-05, acc 1\n",
            "2019-01-22T06:13:36.639274: step 1341, loss 0.00296468, acc 1\n",
            "2019-01-22T06:13:37.318599: step 1342, loss 0.00560087, acc 1\n",
            "2019-01-22T06:13:38.011644: step 1343, loss 0.0162717, acc 0.984375\n",
            "2019-01-22T06:13:38.693847: step 1344, loss 0.0054795, acc 1\n",
            "2019-01-22T06:13:39.372590: step 1345, loss 0.000204679, acc 1\n",
            "2019-01-22T06:13:40.060540: step 1346, loss 0.00667374, acc 1\n",
            "2019-01-22T06:13:40.740241: step 1347, loss 0.000532371, acc 1\n",
            "2019-01-22T06:13:41.425647: step 1348, loss 0.00309231, acc 1\n",
            "2019-01-22T06:13:42.115658: step 1349, loss 0.000887783, acc 1\n",
            "2019-01-22T06:13:42.802585: step 1350, loss 0.00137842, acc 1\n",
            "2019-01-22T06:13:43.482657: step 1351, loss 0.00868281, acc 1\n",
            "2019-01-22T06:13:44.170419: step 1352, loss 0.00138929, acc 1\n",
            "2019-01-22T06:13:44.586063: step 1353, loss 0.00123934, acc 1\n",
            "2019-01-22T06:13:45.268820: step 1354, loss 0.00238808, acc 1\n",
            "2019-01-22T06:13:45.953452: step 1355, loss 0.0625417, acc 0.984375\n",
            "2019-01-22T06:13:46.638012: step 1356, loss 0.0722451, acc 0.984375\n",
            "2019-01-22T06:13:47.319808: step 1357, loss 0.022113, acc 0.984375\n",
            "2019-01-22T06:13:48.003594: step 1358, loss 0.00391814, acc 1\n",
            "2019-01-22T06:13:48.684720: step 1359, loss 0.000241346, acc 1\n",
            "2019-01-22T06:13:49.365715: step 1360, loss 0.000296046, acc 1\n",
            "2019-01-22T06:13:50.050095: step 1361, loss 0.0163161, acc 0.984375\n",
            "2019-01-22T06:13:50.731348: step 1362, loss 0.0660418, acc 0.96875\n",
            "2019-01-22T06:13:51.412365: step 1363, loss 0.00240672, acc 1\n",
            "2019-01-22T06:13:52.093442: step 1364, loss 0.0193916, acc 0.984375\n",
            "2019-01-22T06:13:52.777802: step 1365, loss 0.0576606, acc 0.96875\n",
            "2019-01-22T06:13:53.465576: step 1366, loss 0.00335569, acc 1\n",
            "2019-01-22T06:13:54.148989: step 1367, loss 0.00924251, acc 1\n",
            "2019-01-22T06:13:54.831378: step 1368, loss 0.0011882, acc 1\n",
            "2019-01-22T06:13:55.521261: step 1369, loss 0.00355862, acc 1\n",
            "2019-01-22T06:13:56.202833: step 1370, loss 0.00737023, acc 1\n",
            "2019-01-22T06:13:56.888325: step 1371, loss 0.00607915, acc 1\n",
            "2019-01-22T06:13:57.577101: step 1372, loss 0.0100535, acc 1\n",
            "2019-01-22T06:13:58.255498: step 1373, loss 0.00670098, acc 1\n",
            "2019-01-22T06:13:58.937701: step 1374, loss 0.00210069, acc 1\n",
            "2019-01-22T06:13:59.622273: step 1375, loss 0.000344529, acc 1\n",
            "2019-01-22T06:14:00.305500: step 1376, loss 0.00230691, acc 1\n",
            "2019-01-22T06:14:00.987627: step 1377, loss 0.0546203, acc 0.984375\n",
            "2019-01-22T06:14:01.677122: step 1378, loss 0.000423989, acc 1\n",
            "2019-01-22T06:14:02.356961: step 1379, loss 0.0124963, acc 1\n",
            "2019-01-22T06:14:03.039449: step 1380, loss 0.00298314, acc 1\n",
            "2019-01-22T06:14:03.729822: step 1381, loss 0.000476574, acc 1\n",
            "2019-01-22T06:14:04.410869: step 1382, loss 0.00323214, acc 1\n",
            "2019-01-22T06:14:05.090132: step 1383, loss 0.00102931, acc 1\n",
            "2019-01-22T06:14:05.778838: step 1384, loss 0.00428331, acc 1\n",
            "2019-01-22T06:14:06.458606: step 1385, loss 0.000624082, acc 1\n",
            "2019-01-22T06:14:06.869860: step 1386, loss 0.000194047, acc 1\n",
            "2019-01-22T06:14:07.552409: step 1387, loss 0.020209, acc 0.984375\n",
            "2019-01-22T06:14:08.236981: step 1388, loss 0.00190251, acc 1\n",
            "2019-01-22T06:14:08.919370: step 1389, loss 0.0252987, acc 0.984375\n",
            "2019-01-22T06:14:09.600636: step 1390, loss 0.0170716, acc 0.984375\n",
            "2019-01-22T06:14:10.284540: step 1391, loss 0.00201405, acc 1\n",
            "2019-01-22T06:14:10.965485: step 1392, loss 0.0129106, acc 1\n",
            "2019-01-22T06:14:11.647430: step 1393, loss 9.98904e-05, acc 1\n",
            "2019-01-22T06:14:12.328317: step 1394, loss 0.000350329, acc 1\n",
            "2019-01-22T06:14:13.012217: step 1395, loss 0.000114939, acc 1\n",
            "2019-01-22T06:14:13.691640: step 1396, loss 0.0133153, acc 0.984375\n",
            "2019-01-22T06:14:14.371788: step 1397, loss 0.00274209, acc 1\n",
            "2019-01-22T06:14:15.059604: step 1398, loss 0.000409457, acc 1\n",
            "2019-01-22T06:14:15.741133: step 1399, loss 0.000157061, acc 1\n",
            "2019-01-22T06:14:16.421239: step 1400, loss 0.000627188, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:14:16.783187: step 1400, loss 0.00013937, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1400\n",
            "\n",
            "2019-01-22T06:14:17.633378: step 1401, loss 8.46996e-05, acc 1\n",
            "2019-01-22T06:14:18.311952: step 1402, loss 0.0150366, acc 0.984375\n",
            "2019-01-22T06:14:18.994959: step 1403, loss 0.00342177, acc 1\n",
            "2019-01-22T06:14:19.676300: step 1404, loss 0.00181713, acc 1\n",
            "2019-01-22T06:14:20.361738: step 1405, loss 0.00204641, acc 1\n",
            "2019-01-22T06:14:21.047542: step 1406, loss 0.000119618, acc 1\n",
            "2019-01-22T06:14:21.730105: step 1407, loss 0.00100421, acc 1\n",
            "2019-01-22T06:14:22.411481: step 1408, loss 0.00153069, acc 1\n",
            "2019-01-22T06:14:23.095775: step 1409, loss 0.000120258, acc 1\n",
            "2019-01-22T06:14:23.779145: step 1410, loss 0.00985803, acc 1\n",
            "2019-01-22T06:14:24.465749: step 1411, loss 0.000690799, acc 1\n",
            "2019-01-22T06:14:25.144715: step 1412, loss 0.000455023, acc 1\n",
            "2019-01-22T06:14:25.827306: step 1413, loss 0.0103584, acc 1\n",
            "2019-01-22T06:14:26.509431: step 1414, loss 0.000328047, acc 1\n",
            "2019-01-22T06:14:27.185613: step 1415, loss 0.000279465, acc 1\n",
            "2019-01-22T06:14:27.870474: step 1416, loss 0.0256413, acc 0.984375\n",
            "2019-01-22T06:14:28.553365: step 1417, loss 0.00325368, acc 1\n",
            "2019-01-22T06:14:29.230528: step 1418, loss 0.00230559, acc 1\n",
            "2019-01-22T06:14:29.646576: step 1419, loss 7.4156e-05, acc 1\n",
            "2019-01-22T06:14:30.328482: step 1420, loss 0.00409547, acc 1\n",
            "2019-01-22T06:14:31.012978: step 1421, loss 0.002197, acc 1\n",
            "2019-01-22T06:14:31.692509: step 1422, loss 0.000766566, acc 1\n",
            "2019-01-22T06:14:32.374240: step 1423, loss 0.000333699, acc 1\n",
            "2019-01-22T06:14:33.053285: step 1424, loss 0.000168152, acc 1\n",
            "2019-01-22T06:14:33.737924: step 1425, loss 0.0124451, acc 0.984375\n",
            "2019-01-22T06:14:34.418681: step 1426, loss 0.00107367, acc 1\n",
            "2019-01-22T06:14:35.097987: step 1427, loss 0.000684109, acc 1\n",
            "2019-01-22T06:14:35.779568: step 1428, loss 0.000572585, acc 1\n",
            "2019-01-22T06:14:36.465590: step 1429, loss 0.000484805, acc 1\n",
            "2019-01-22T06:14:37.146761: step 1430, loss 0.00263936, acc 1\n",
            "2019-01-22T06:14:37.827732: step 1431, loss 0.00299505, acc 1\n",
            "2019-01-22T06:14:38.511740: step 1432, loss 0.000488821, acc 1\n",
            "2019-01-22T06:14:39.192714: step 1433, loss 0.000357085, acc 1\n",
            "2019-01-22T06:14:39.875668: step 1434, loss 0.0138984, acc 0.984375\n",
            "2019-01-22T06:14:40.557265: step 1435, loss 0.067393, acc 0.984375\n",
            "2019-01-22T06:14:41.237428: step 1436, loss 0.00224763, acc 1\n",
            "2019-01-22T06:14:41.916628: step 1437, loss 0.000111931, acc 1\n",
            "2019-01-22T06:14:42.600340: step 1438, loss 0.0997165, acc 0.96875\n",
            "2019-01-22T06:14:43.286523: step 1439, loss 5.67168e-05, acc 1\n",
            "2019-01-22T06:14:43.969643: step 1440, loss 4.30136e-05, acc 1\n",
            "2019-01-22T06:14:44.649864: step 1441, loss 0.00587924, acc 1\n",
            "2019-01-22T06:14:45.327974: step 1442, loss 0.0032171, acc 1\n",
            "2019-01-22T06:14:46.014080: step 1443, loss 7.03558e-05, acc 1\n",
            "2019-01-22T06:14:46.698696: step 1444, loss 0.0677129, acc 0.984375\n",
            "2019-01-22T06:14:47.381982: step 1445, loss 0.00579636, acc 1\n",
            "2019-01-22T06:14:48.061235: step 1446, loss 0.00100067, acc 1\n",
            "2019-01-22T06:14:48.739902: step 1447, loss 0.00028571, acc 1\n",
            "2019-01-22T06:14:49.422145: step 1448, loss 0.0351993, acc 0.984375\n",
            "2019-01-22T06:14:50.103547: step 1449, loss 0.0181402, acc 0.984375\n",
            "2019-01-22T06:14:50.784939: step 1450, loss 7.97483e-05, acc 1\n",
            "2019-01-22T06:14:51.466965: step 1451, loss 0.0026002, acc 1\n",
            "2019-01-22T06:14:51.882346: step 1452, loss 0.0123111, acc 1\n",
            "2019-01-22T06:14:52.568386: step 1453, loss 0.000642319, acc 1\n",
            "2019-01-22T06:14:53.249869: step 1454, loss 0.000175007, acc 1\n",
            "2019-01-22T06:14:53.934008: step 1455, loss 0.0141584, acc 0.984375\n",
            "2019-01-22T06:14:54.616213: step 1456, loss 0.00715955, acc 1\n",
            "2019-01-22T06:14:55.294426: step 1457, loss 0.04647, acc 0.984375\n",
            "2019-01-22T06:14:55.976642: step 1458, loss 0.0483597, acc 0.984375\n",
            "2019-01-22T06:14:56.659429: step 1459, loss 0.0214431, acc 0.984375\n",
            "2019-01-22T06:14:57.342885: step 1460, loss 0.00195552, acc 1\n",
            "2019-01-22T06:14:58.024665: step 1461, loss 0.000148326, acc 1\n",
            "2019-01-22T06:14:58.705045: step 1462, loss 0.00387406, acc 1\n",
            "2019-01-22T06:14:59.392603: step 1463, loss 0.0217107, acc 0.984375\n",
            "2019-01-22T06:15:00.074028: step 1464, loss 0.00138317, acc 1\n",
            "2019-01-22T06:15:00.761885: step 1465, loss 0.0130047, acc 1\n",
            "2019-01-22T06:15:01.446477: step 1466, loss 0.00650171, acc 1\n",
            "2019-01-22T06:15:02.128830: step 1467, loss 0.00520754, acc 1\n",
            "2019-01-22T06:15:02.814679: step 1468, loss 0.000903896, acc 1\n",
            "2019-01-22T06:15:03.506852: step 1469, loss 0.0448298, acc 0.984375\n",
            "2019-01-22T06:15:04.190238: step 1470, loss 0.00109187, acc 1\n",
            "2019-01-22T06:15:04.880882: step 1471, loss 0.00226536, acc 1\n",
            "2019-01-22T06:15:05.573325: step 1472, loss 0.00106253, acc 1\n",
            "2019-01-22T06:15:06.254447: step 1473, loss 0.000146633, acc 1\n",
            "2019-01-22T06:15:06.937046: step 1474, loss 0.000254296, acc 1\n",
            "2019-01-22T06:15:07.626307: step 1475, loss 0.0240722, acc 0.984375\n",
            "2019-01-22T06:15:08.308388: step 1476, loss 0.0370647, acc 0.984375\n",
            "2019-01-22T06:15:08.996118: step 1477, loss 0.11141, acc 0.984375\n",
            "2019-01-22T06:15:09.682751: step 1478, loss 0.00942926, acc 1\n",
            "2019-01-22T06:15:10.365664: step 1479, loss 6.33138e-05, acc 1\n",
            "2019-01-22T06:15:11.049439: step 1480, loss 0.00130925, acc 1\n",
            "2019-01-22T06:15:11.737568: step 1481, loss 0.0292026, acc 0.984375\n",
            "2019-01-22T06:15:12.421433: step 1482, loss 0.000504261, acc 1\n",
            "2019-01-22T06:15:13.112036: step 1483, loss 0.0686592, acc 0.984375\n",
            "2019-01-22T06:15:13.801514: step 1484, loss 0.00189979, acc 1\n",
            "2019-01-22T06:15:14.215139: step 1485, loss 0.00240767, acc 1\n",
            "2019-01-22T06:15:14.896952: step 1486, loss 0.0934297, acc 0.984375\n",
            "2019-01-22T06:15:15.578896: step 1487, loss 0.00346061, acc 1\n",
            "2019-01-22T06:15:16.269180: step 1488, loss 0.00309049, acc 1\n",
            "2019-01-22T06:15:16.950606: step 1489, loss 0.002224, acc 1\n",
            "2019-01-22T06:15:17.635880: step 1490, loss 0.0048188, acc 1\n",
            "2019-01-22T06:15:18.316476: step 1491, loss 0.0179293, acc 0.984375\n",
            "2019-01-22T06:15:18.999530: step 1492, loss 0.000470109, acc 1\n",
            "2019-01-22T06:15:19.681546: step 1493, loss 0.00626861, acc 1\n",
            "2019-01-22T06:15:20.367611: step 1494, loss 0.000175856, acc 1\n",
            "2019-01-22T06:15:21.054700: step 1495, loss 0.000725562, acc 1\n",
            "2019-01-22T06:15:21.737779: step 1496, loss 0.016943, acc 0.984375\n",
            "2019-01-22T06:15:22.422301: step 1497, loss 0.0395248, acc 0.984375\n",
            "2019-01-22T06:15:23.107103: step 1498, loss 0.0024204, acc 1\n",
            "2019-01-22T06:15:23.790622: step 1499, loss 0.00648226, acc 1\n",
            "2019-01-22T06:15:24.469446: step 1500, loss 0.000209155, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:15:24.829026: step 1500, loss 0.000150123, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1500\n",
            "\n",
            "2019-01-22T06:15:25.664548: step 1501, loss 0.000505738, acc 1\n",
            "2019-01-22T06:15:26.345729: step 1502, loss 0.00136947, acc 1\n",
            "2019-01-22T06:15:27.028841: step 1503, loss 0.000740662, acc 1\n",
            "2019-01-22T06:15:27.716779: step 1504, loss 0.0012248, acc 1\n",
            "2019-01-22T06:15:28.399314: step 1505, loss 7.00188e-05, acc 1\n",
            "2019-01-22T06:15:29.086405: step 1506, loss 0.00111871, acc 1\n",
            "2019-01-22T06:15:29.770857: step 1507, loss 0.0211421, acc 0.984375\n",
            "2019-01-22T06:15:30.453832: step 1508, loss 0.00095814, acc 1\n",
            "2019-01-22T06:15:31.136416: step 1509, loss 0.00260953, acc 1\n",
            "2019-01-22T06:15:31.819288: step 1510, loss 0.000742632, acc 1\n",
            "2019-01-22T06:15:32.500968: step 1511, loss 0.0415441, acc 0.984375\n",
            "2019-01-22T06:15:33.182584: step 1512, loss 0.0183446, acc 0.984375\n",
            "2019-01-22T06:15:33.862752: step 1513, loss 0.000706768, acc 1\n",
            "2019-01-22T06:15:34.548441: step 1514, loss 0.0223674, acc 0.984375\n",
            "2019-01-22T06:15:35.230694: step 1515, loss 0.00853881, acc 1\n",
            "2019-01-22T06:15:35.919659: step 1516, loss 0.00074559, acc 1\n",
            "2019-01-22T06:15:36.604691: step 1517, loss 0.00728448, acc 1\n",
            "2019-01-22T06:15:37.020947: step 1518, loss 0.000396562, acc 1\n",
            "2019-01-22T06:15:37.702752: step 1519, loss 0.0794024, acc 0.984375\n",
            "2019-01-22T06:15:38.387926: step 1520, loss 0.000652808, acc 1\n",
            "2019-01-22T06:15:39.068262: step 1521, loss 0.000460659, acc 1\n",
            "2019-01-22T06:15:39.751104: step 1522, loss 0.000157571, acc 1\n",
            "2019-01-22T06:15:40.437508: step 1523, loss 0.00160675, acc 1\n",
            "2019-01-22T06:15:41.121316: step 1524, loss 0.00191432, acc 1\n",
            "2019-01-22T06:15:41.806462: step 1525, loss 0.0102853, acc 1\n",
            "2019-01-22T06:15:42.488879: step 1526, loss 0.000132367, acc 1\n",
            "2019-01-22T06:15:43.174549: step 1527, loss 0.0104754, acc 1\n",
            "2019-01-22T06:15:43.855515: step 1528, loss 0.00239533, acc 1\n",
            "2019-01-22T06:15:44.536705: step 1529, loss 0.00177017, acc 1\n",
            "2019-01-22T06:15:45.215660: step 1530, loss 0.0164753, acc 1\n",
            "2019-01-22T06:15:45.897058: step 1531, loss 0.00135584, acc 1\n",
            "2019-01-22T06:15:46.578665: step 1532, loss 0.00102125, acc 1\n",
            "2019-01-22T06:15:47.263203: step 1533, loss 0.000167035, acc 1\n",
            "2019-01-22T06:15:47.940899: step 1534, loss 0.00368118, acc 1\n",
            "2019-01-22T06:15:48.619592: step 1535, loss 0.00129191, acc 1\n",
            "2019-01-22T06:15:49.299759: step 1536, loss 0.0134572, acc 1\n",
            "2019-01-22T06:15:49.982964: step 1537, loss 0.00126111, acc 1\n",
            "2019-01-22T06:15:50.668448: step 1538, loss 0.000167408, acc 1\n",
            "2019-01-22T06:15:51.346039: step 1539, loss 0.000481678, acc 1\n",
            "2019-01-22T06:15:52.026674: step 1540, loss 0.0187488, acc 0.984375\n",
            "2019-01-22T06:15:52.711112: step 1541, loss 0.00355687, acc 1\n",
            "2019-01-22T06:15:53.394434: step 1542, loss 0.00102564, acc 1\n",
            "2019-01-22T06:15:54.078917: step 1543, loss 0.00357067, acc 1\n",
            "2019-01-22T06:15:54.759517: step 1544, loss 0.00391919, acc 1\n",
            "2019-01-22T06:15:55.444654: step 1545, loss 0.0273741, acc 0.984375\n",
            "2019-01-22T06:15:56.124134: step 1546, loss 0.0196648, acc 0.984375\n",
            "2019-01-22T06:15:56.807728: step 1547, loss 0.00117945, acc 1\n",
            "2019-01-22T06:15:57.488506: step 1548, loss 0.000708009, acc 1\n",
            "2019-01-22T06:15:58.166891: step 1549, loss 0.000154599, acc 1\n",
            "2019-01-22T06:15:58.850079: step 1550, loss 0.00062796, acc 1\n",
            "2019-01-22T06:15:59.266962: step 1551, loss 0.00204199, acc 1\n",
            "2019-01-22T06:15:59.950496: step 1552, loss 0.0266033, acc 0.984375\n",
            "2019-01-22T06:16:00.629548: step 1553, loss 0.0121961, acc 0.984375\n",
            "2019-01-22T06:16:01.315919: step 1554, loss 0.00786414, acc 1\n",
            "2019-01-22T06:16:01.997500: step 1555, loss 0.000775419, acc 1\n",
            "2019-01-22T06:16:02.680100: step 1556, loss 0.000845295, acc 1\n",
            "2019-01-22T06:16:03.366206: step 1557, loss 9.058e-05, acc 1\n",
            "2019-01-22T06:16:04.047311: step 1558, loss 0.000274772, acc 1\n",
            "2019-01-22T06:16:04.731833: step 1559, loss 0.000815691, acc 1\n",
            "2019-01-22T06:16:05.412749: step 1560, loss 0.0290383, acc 0.984375\n",
            "2019-01-22T06:16:06.093330: step 1561, loss 0.0012689, acc 1\n",
            "2019-01-22T06:16:06.777223: step 1562, loss 0.00622912, acc 1\n",
            "2019-01-22T06:16:07.462642: step 1563, loss 0.00134372, acc 1\n",
            "2019-01-22T06:16:08.150044: step 1564, loss 0.00202933, acc 1\n",
            "2019-01-22T06:16:08.828923: step 1565, loss 0.00175065, acc 1\n",
            "2019-01-22T06:16:09.512239: step 1566, loss 0.0300117, acc 0.984375\n",
            "2019-01-22T06:16:10.192558: step 1567, loss 0.00372126, acc 1\n",
            "2019-01-22T06:16:10.874902: step 1568, loss 0.00397714, acc 1\n",
            "2019-01-22T06:16:11.556994: step 1569, loss 0.00094567, acc 1\n",
            "2019-01-22T06:16:12.240268: step 1570, loss 0.0122724, acc 0.984375\n",
            "2019-01-22T06:16:12.922395: step 1571, loss 4.69046e-05, acc 1\n",
            "2019-01-22T06:16:13.609182: step 1572, loss 0.0112351, acc 1\n",
            "2019-01-22T06:16:14.290948: step 1573, loss 0.0834262, acc 0.96875\n",
            "2019-01-22T06:16:14.974464: step 1574, loss 0.000212642, acc 1\n",
            "2019-01-22T06:16:15.659707: step 1575, loss 0.000345302, acc 1\n",
            "2019-01-22T06:16:16.339323: step 1576, loss 0.000470232, acc 1\n",
            "2019-01-22T06:16:17.021547: step 1577, loss 0.00957409, acc 1\n",
            "2019-01-22T06:16:17.702995: step 1578, loss 0.00123657, acc 1\n",
            "2019-01-22T06:16:18.386411: step 1579, loss 0.000663815, acc 1\n",
            "2019-01-22T06:16:19.074823: step 1580, loss 0.000245342, acc 1\n",
            "2019-01-22T06:16:19.754680: step 1581, loss 0.00132108, acc 1\n",
            "2019-01-22T06:16:20.438989: step 1582, loss 0.000269986, acc 1\n",
            "2019-01-22T06:16:21.123113: step 1583, loss 0.000996524, acc 1\n",
            "2019-01-22T06:16:21.539087: step 1584, loss 0.00496341, acc 1\n",
            "2019-01-22T06:16:22.218480: step 1585, loss 5.27952e-05, acc 1\n",
            "2019-01-22T06:16:22.897380: step 1586, loss 0.000717925, acc 1\n",
            "2019-01-22T06:16:23.580884: step 1587, loss 0.0496977, acc 0.984375\n",
            "2019-01-22T06:16:24.258326: step 1588, loss 0.0554465, acc 0.984375\n",
            "2019-01-22T06:16:24.941644: step 1589, loss 4.46102e-05, acc 1\n",
            "2019-01-22T06:16:25.627952: step 1590, loss 0.00701205, acc 1\n",
            "2019-01-22T06:16:26.309862: step 1591, loss 0.0201693, acc 0.984375\n",
            "2019-01-22T06:16:26.987870: step 1592, loss 0.0138034, acc 0.984375\n",
            "2019-01-22T06:16:27.668998: step 1593, loss 0.000605913, acc 1\n",
            "2019-01-22T06:16:28.348353: step 1594, loss 0.0104359, acc 1\n",
            "2019-01-22T06:16:29.032230: step 1595, loss 0.000309846, acc 1\n",
            "2019-01-22T06:16:29.714338: step 1596, loss 0.00101143, acc 1\n",
            "2019-01-22T06:16:30.396823: step 1597, loss 0.00145177, acc 1\n",
            "2019-01-22T06:16:31.077205: step 1598, loss 0.00200621, acc 1\n",
            "2019-01-22T06:16:31.757916: step 1599, loss 0.00902877, acc 1\n",
            "2019-01-22T06:16:32.441920: step 1600, loss 0.000133038, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:16:32.806402: step 1600, loss 0.000243744, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1600\n",
            "\n",
            "2019-01-22T06:16:33.652261: step 1601, loss 0.00671634, acc 1\n",
            "2019-01-22T06:16:34.334603: step 1602, loss 0.0226245, acc 0.984375\n",
            "2019-01-22T06:16:35.019986: step 1603, loss 0.0264662, acc 0.984375\n",
            "2019-01-22T06:16:35.702177: step 1604, loss 0.00366642, acc 1\n",
            "2019-01-22T06:16:36.383989: step 1605, loss 0.0269585, acc 0.984375\n",
            "2019-01-22T06:16:37.077062: step 1606, loss 0.0201171, acc 0.984375\n",
            "2019-01-22T06:16:37.756465: step 1607, loss 0.000488055, acc 1\n",
            "2019-01-22T06:16:38.443641: step 1608, loss 0.00523296, acc 1\n",
            "2019-01-22T06:16:39.133711: step 1609, loss 0.0520789, acc 0.984375\n",
            "2019-01-22T06:16:39.816318: step 1610, loss 0.0026279, acc 1\n",
            "2019-01-22T06:16:40.500140: step 1611, loss 0.00114541, acc 1\n",
            "2019-01-22T06:16:41.193232: step 1612, loss 0.0273843, acc 0.984375\n",
            "2019-01-22T06:16:41.878151: step 1613, loss 0.00164763, acc 1\n",
            "2019-01-22T06:16:42.562769: step 1614, loss 0.00148434, acc 1\n",
            "2019-01-22T06:16:43.250311: step 1615, loss 0.00737615, acc 1\n",
            "2019-01-22T06:16:43.937574: step 1616, loss 0.0875999, acc 0.96875\n",
            "2019-01-22T06:16:44.356401: step 1617, loss 0.156948, acc 0.971429\n",
            "2019-01-22T06:16:45.040013: step 1618, loss 0.000459625, acc 1\n",
            "2019-01-22T06:16:45.726205: step 1619, loss 0.0229154, acc 0.984375\n",
            "2019-01-22T06:16:46.413246: step 1620, loss 0.000638262, acc 1\n",
            "2019-01-22T06:16:47.094184: step 1621, loss 0.000670873, acc 1\n",
            "2019-01-22T06:16:47.774362: step 1622, loss 0.000502954, acc 1\n",
            "2019-01-22T06:16:48.457234: step 1623, loss 0.0477075, acc 0.984375\n",
            "2019-01-22T06:16:49.136143: step 1624, loss 0.0718298, acc 0.984375\n",
            "2019-01-22T06:16:49.820270: step 1625, loss 0.0596902, acc 0.984375\n",
            "2019-01-22T06:16:50.502716: step 1626, loss 0.01859, acc 0.984375\n",
            "2019-01-22T06:16:51.182550: step 1627, loss 0.000320332, acc 1\n",
            "2019-01-22T06:16:51.862520: step 1628, loss 0.000268708, acc 1\n",
            "2019-01-22T06:16:52.545632: step 1629, loss 0.000187129, acc 1\n",
            "2019-01-22T06:16:53.226652: step 1630, loss 0.000356677, acc 1\n",
            "2019-01-22T06:16:53.905831: step 1631, loss 0.0154207, acc 0.984375\n",
            "2019-01-22T06:16:54.590801: step 1632, loss 0.0632354, acc 0.984375\n",
            "2019-01-22T06:16:55.273468: step 1633, loss 0.00639852, acc 1\n",
            "2019-01-22T06:16:55.956991: step 1634, loss 0.000715078, acc 1\n",
            "2019-01-22T06:16:56.642535: step 1635, loss 0.002262, acc 1\n",
            "2019-01-22T06:16:57.328612: step 1636, loss 0.023086, acc 0.984375\n",
            "2019-01-22T06:16:58.013971: step 1637, loss 0.0168488, acc 0.984375\n",
            "2019-01-22T06:16:58.698469: step 1638, loss 0.000266387, acc 1\n",
            "2019-01-22T06:16:59.375515: step 1639, loss 0.00244249, acc 1\n",
            "2019-01-22T06:17:00.057183: step 1640, loss 0.0964675, acc 0.96875\n",
            "2019-01-22T06:17:00.743821: step 1641, loss 0.0186149, acc 0.984375\n",
            "2019-01-22T06:17:01.425348: step 1642, loss 0.000150331, acc 1\n",
            "2019-01-22T06:17:02.106014: step 1643, loss 0.00290347, acc 1\n",
            "2019-01-22T06:17:02.801851: step 1644, loss 0.080651, acc 0.984375\n",
            "2019-01-22T06:17:03.484624: step 1645, loss 0.000387215, acc 1\n",
            "2019-01-22T06:17:04.163996: step 1646, loss 0.00375054, acc 1\n",
            "2019-01-22T06:17:04.851062: step 1647, loss 0.00156311, acc 1\n",
            "2019-01-22T06:17:05.534311: step 1648, loss 0.0350922, acc 0.984375\n",
            "2019-01-22T06:17:06.211706: step 1649, loss 0.00284457, acc 1\n",
            "2019-01-22T06:17:06.628417: step 1650, loss 0.00626803, acc 1\n",
            "2019-01-22T06:17:07.309535: step 1651, loss 0.00512182, acc 1\n",
            "2019-01-22T06:17:07.988661: step 1652, loss 4.70009e-05, acc 1\n",
            "2019-01-22T06:17:08.672036: step 1653, loss 9.86654e-05, acc 1\n",
            "2019-01-22T06:17:09.354349: step 1654, loss 0.00231859, acc 1\n",
            "2019-01-22T06:17:10.040966: step 1655, loss 0.00224603, acc 1\n",
            "2019-01-22T06:17:10.722207: step 1656, loss 0.000726615, acc 1\n",
            "2019-01-22T06:17:11.404901: step 1657, loss 0.000476608, acc 1\n",
            "2019-01-22T06:17:12.092070: step 1658, loss 0.0729881, acc 0.96875\n",
            "2019-01-22T06:17:12.776825: step 1659, loss 0.0432818, acc 0.984375\n",
            "2019-01-22T06:17:13.461974: step 1660, loss 0.00658596, acc 1\n",
            "2019-01-22T06:17:14.147051: step 1661, loss 0.00162246, acc 1\n",
            "2019-01-22T06:17:14.834463: step 1662, loss 0.0324295, acc 0.96875\n",
            "2019-01-22T06:17:15.519368: step 1663, loss 0.0441355, acc 0.984375\n",
            "2019-01-22T06:17:16.207448: step 1664, loss 0.00584168, acc 1\n",
            "2019-01-22T06:17:16.887718: step 1665, loss 0.00958648, acc 1\n",
            "2019-01-22T06:17:17.569269: step 1666, loss 0.0536797, acc 0.984375\n",
            "2019-01-22T06:17:18.263339: step 1667, loss 0.00206436, acc 1\n",
            "2019-01-22T06:17:18.944073: step 1668, loss 0.00503195, acc 1\n",
            "2019-01-22T06:17:19.622629: step 1669, loss 0.00880202, acc 1\n",
            "2019-01-22T06:17:20.309702: step 1670, loss 0.02216, acc 0.984375\n",
            "2019-01-22T06:17:20.990502: step 1671, loss 0.000300093, acc 1\n",
            "2019-01-22T06:17:21.672292: step 1672, loss 0.000478629, acc 1\n",
            "2019-01-22T06:17:22.357646: step 1673, loss 0.0288809, acc 0.984375\n",
            "2019-01-22T06:17:23.040143: step 1674, loss 0.0184689, acc 0.984375\n",
            "2019-01-22T06:17:23.719206: step 1675, loss 0.0538286, acc 0.984375\n",
            "2019-01-22T06:17:24.407437: step 1676, loss 0.0158853, acc 0.984375\n",
            "2019-01-22T06:17:25.087918: step 1677, loss 0.00492907, acc 1\n",
            "2019-01-22T06:17:25.773025: step 1678, loss 0.00662924, acc 1\n",
            "2019-01-22T06:17:26.462337: step 1679, loss 0.000392882, acc 1\n",
            "2019-01-22T06:17:27.145429: step 1680, loss 0.0122519, acc 0.984375\n",
            "2019-01-22T06:17:27.824537: step 1681, loss 0.00735276, acc 1\n",
            "2019-01-22T06:17:28.508910: step 1682, loss 0.0003262, acc 1\n",
            "2019-01-22T06:17:28.918198: step 1683, loss 5.60797e-05, acc 1\n",
            "2019-01-22T06:17:29.596947: step 1684, loss 0.0100882, acc 1\n",
            "2019-01-22T06:17:30.276276: step 1685, loss 0.00122673, acc 1\n",
            "2019-01-22T06:17:30.959484: step 1686, loss 0.000743326, acc 1\n",
            "2019-01-22T06:17:31.637941: step 1687, loss 0.0044283, acc 1\n",
            "2019-01-22T06:17:32.322174: step 1688, loss 0.021095, acc 0.984375\n",
            "2019-01-22T06:17:33.009281: step 1689, loss 0.00126625, acc 1\n",
            "2019-01-22T06:17:33.689598: step 1690, loss 0.0178638, acc 0.984375\n",
            "2019-01-22T06:17:34.373780: step 1691, loss 0.000835159, acc 1\n",
            "2019-01-22T06:17:35.056714: step 1692, loss 0.000763566, acc 1\n",
            "2019-01-22T06:17:35.743479: step 1693, loss 0.000624192, acc 1\n",
            "2019-01-22T06:17:36.429138: step 1694, loss 0.00898028, acc 1\n",
            "2019-01-22T06:17:37.109976: step 1695, loss 0.00206981, acc 1\n",
            "2019-01-22T06:17:37.797822: step 1696, loss 0.00257939, acc 1\n",
            "2019-01-22T06:17:38.478681: step 1697, loss 0.012173, acc 0.984375\n",
            "2019-01-22T06:17:39.157496: step 1698, loss 0.00623105, acc 1\n",
            "2019-01-22T06:17:39.849819: step 1699, loss 0.000796991, acc 1\n",
            "2019-01-22T06:17:40.532211: step 1700, loss 9.35373e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:17:40.893142: step 1700, loss 5.63806e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1700\n",
            "\n",
            "2019-01-22T06:17:41.736270: step 1701, loss 0.00407649, acc 1\n",
            "2019-01-22T06:17:42.417830: step 1702, loss 0.00060923, acc 1\n",
            "2019-01-22T06:17:43.099699: step 1703, loss 0.00947643, acc 1\n",
            "2019-01-22T06:17:43.782213: step 1704, loss 0.000362039, acc 1\n",
            "2019-01-22T06:17:44.467016: step 1705, loss 0.00233712, acc 1\n",
            "2019-01-22T06:17:45.147556: step 1706, loss 0.0564303, acc 0.984375\n",
            "2019-01-22T06:17:45.829272: step 1707, loss 0.0119428, acc 0.984375\n",
            "2019-01-22T06:17:46.512922: step 1708, loss 0.00400269, acc 1\n",
            "2019-01-22T06:17:47.194834: step 1709, loss 0.00186845, acc 1\n",
            "2019-01-22T06:17:47.878891: step 1710, loss 0.000591424, acc 1\n",
            "2019-01-22T06:17:48.559407: step 1711, loss 0.000322288, acc 1\n",
            "2019-01-22T06:17:49.237386: step 1712, loss 0.00245589, acc 1\n",
            "2019-01-22T06:17:49.922213: step 1713, loss 0.000221765, acc 1\n",
            "2019-01-22T06:17:50.601576: step 1714, loss 0.00100072, acc 1\n",
            "2019-01-22T06:17:51.282768: step 1715, loss 6.36679e-05, acc 1\n",
            "2019-01-22T06:17:51.701495: step 1716, loss 0.0131976, acc 1\n",
            "2019-01-22T06:17:52.386557: step 1717, loss 0.0186965, acc 0.984375\n",
            "2019-01-22T06:17:53.069681: step 1718, loss 0.000809342, acc 1\n",
            "2019-01-22T06:17:53.749944: step 1719, loss 0.0198806, acc 0.984375\n",
            "2019-01-22T06:17:54.430098: step 1720, loss 0.0010059, acc 1\n",
            "2019-01-22T06:17:55.112063: step 1721, loss 4.56855e-05, acc 1\n",
            "2019-01-22T06:17:55.789401: step 1722, loss 0.000827454, acc 1\n",
            "2019-01-22T06:17:56.471719: step 1723, loss 0.0273813, acc 0.984375\n",
            "2019-01-22T06:17:57.154266: step 1724, loss 0.132042, acc 0.984375\n",
            "2019-01-22T06:17:57.832542: step 1725, loss 0.000548027, acc 1\n",
            "2019-01-22T06:17:58.521758: step 1726, loss 0.00388892, acc 1\n",
            "2019-01-22T06:17:59.201637: step 1727, loss 0.00184657, acc 1\n",
            "2019-01-22T06:17:59.878825: step 1728, loss 0.000439571, acc 1\n",
            "2019-01-22T06:18:00.563726: step 1729, loss 0.000259342, acc 1\n",
            "2019-01-22T06:18:01.243206: step 1730, loss 0.000258539, acc 1\n",
            "2019-01-22T06:18:01.929941: step 1731, loss 6.29047e-05, acc 1\n",
            "2019-01-22T06:18:02.612070: step 1732, loss 0.00967761, acc 1\n",
            "2019-01-22T06:18:03.288285: step 1733, loss 0.0206332, acc 0.984375\n",
            "2019-01-22T06:18:03.972215: step 1734, loss 0.00122715, acc 1\n",
            "2019-01-22T06:18:04.656848: step 1735, loss 2.11699e-05, acc 1\n",
            "2019-01-22T06:18:05.341115: step 1736, loss 0.000171904, acc 1\n",
            "2019-01-22T06:18:06.020267: step 1737, loss 0.000283361, acc 1\n",
            "2019-01-22T06:18:06.699696: step 1738, loss 0.000332727, acc 1\n",
            "2019-01-22T06:18:07.384471: step 1739, loss 0.000142765, acc 1\n",
            "2019-01-22T06:18:08.070866: step 1740, loss 0.0331228, acc 0.984375\n",
            "2019-01-22T06:18:08.749515: step 1741, loss 0.00211809, acc 1\n",
            "2019-01-22T06:18:09.432952: step 1742, loss 0.000802546, acc 1\n",
            "2019-01-22T06:18:10.115568: step 1743, loss 0.00316438, acc 1\n",
            "2019-01-22T06:18:10.799776: step 1744, loss 0.00264584, acc 1\n",
            "2019-01-22T06:18:11.477689: step 1745, loss 4.28359e-05, acc 1\n",
            "2019-01-22T06:18:12.164052: step 1746, loss 0.00134505, acc 1\n",
            "2019-01-22T06:18:12.845747: step 1747, loss 0.00188402, acc 1\n",
            "2019-01-22T06:18:13.527016: step 1748, loss 0.00282129, acc 1\n",
            "2019-01-22T06:18:13.943847: step 1749, loss 0.00183269, acc 1\n",
            "2019-01-22T06:18:14.628332: step 1750, loss 0.000710079, acc 1\n",
            "2019-01-22T06:18:15.302268: step 1751, loss 2.14196e-05, acc 1\n",
            "2019-01-22T06:18:15.993312: step 1752, loss 0.00135701, acc 1\n",
            "2019-01-22T06:18:16.670210: step 1753, loss 0.0023807, acc 1\n",
            "2019-01-22T06:18:17.353416: step 1754, loss 0.000708975, acc 1\n",
            "2019-01-22T06:18:18.033768: step 1755, loss 0.0123737, acc 1\n",
            "2019-01-22T06:18:18.715990: step 1756, loss 0.0485182, acc 0.96875\n",
            "2019-01-22T06:18:19.399179: step 1757, loss 0.000412029, acc 1\n",
            "2019-01-22T06:18:20.081046: step 1758, loss 8.57217e-05, acc 1\n",
            "2019-01-22T06:18:20.761109: step 1759, loss 0.00450229, acc 1\n",
            "2019-01-22T06:18:21.443574: step 1760, loss 0.0117303, acc 1\n",
            "2019-01-22T06:18:22.127568: step 1761, loss 1.83515e-05, acc 1\n",
            "2019-01-22T06:18:22.808832: step 1762, loss 0.00371646, acc 1\n",
            "2019-01-22T06:18:23.487516: step 1763, loss 0.00203797, acc 1\n",
            "2019-01-22T06:18:24.168913: step 1764, loss 0.0439118, acc 0.984375\n",
            "2019-01-22T06:18:24.850719: step 1765, loss 0.0937939, acc 0.96875\n",
            "2019-01-22T06:18:25.534696: step 1766, loss 0.0198761, acc 0.984375\n",
            "2019-01-22T06:18:26.224990: step 1767, loss 0.000712818, acc 1\n",
            "2019-01-22T06:18:26.906791: step 1768, loss 0.00163246, acc 1\n",
            "2019-01-22T06:18:27.588538: step 1769, loss 8.31551e-05, acc 1\n",
            "2019-01-22T06:18:28.274431: step 1770, loss 0.000653277, acc 1\n",
            "2019-01-22T06:18:28.956710: step 1771, loss 0.00115397, acc 1\n",
            "2019-01-22T06:18:29.632095: step 1772, loss 0.00183479, acc 1\n",
            "2019-01-22T06:18:30.320040: step 1773, loss 0.000177694, acc 1\n",
            "2019-01-22T06:18:31.004522: step 1774, loss 0.00653682, acc 1\n",
            "2019-01-22T06:18:31.687707: step 1775, loss 0.0282645, acc 0.96875\n",
            "2019-01-22T06:18:32.378273: step 1776, loss 0.0170369, acc 0.984375\n",
            "2019-01-22T06:18:33.062690: step 1777, loss 0.000516262, acc 1\n",
            "2019-01-22T06:18:33.742488: step 1778, loss 2.06673e-05, acc 1\n",
            "2019-01-22T06:18:34.429492: step 1779, loss 0.0175443, acc 0.984375\n",
            "2019-01-22T06:18:35.111615: step 1780, loss 0.0581317, acc 0.984375\n",
            "2019-01-22T06:18:35.795735: step 1781, loss 0.00660572, acc 1\n",
            "2019-01-22T06:18:36.206585: step 1782, loss 0.00168945, acc 1\n",
            "2019-01-22T06:18:36.884042: step 1783, loss 0.00891344, acc 1\n",
            "2019-01-22T06:18:37.567957: step 1784, loss 0.00141306, acc 1\n",
            "2019-01-22T06:18:38.246928: step 1785, loss 0.0138777, acc 1\n",
            "2019-01-22T06:18:38.934560: step 1786, loss 0.000117087, acc 1\n",
            "2019-01-22T06:18:39.615052: step 1787, loss 0.00119931, acc 1\n",
            "2019-01-22T06:18:40.301440: step 1788, loss 0.000143169, acc 1\n",
            "2019-01-22T06:18:40.987436: step 1789, loss 0.00234325, acc 1\n",
            "2019-01-22T06:18:41.669531: step 1790, loss 0.0013337, acc 1\n",
            "2019-01-22T06:18:42.352152: step 1791, loss 0.00305785, acc 1\n",
            "2019-01-22T06:18:43.033957: step 1792, loss 0.000588573, acc 1\n",
            "2019-01-22T06:18:43.725734: step 1793, loss 0.0152852, acc 0.984375\n",
            "2019-01-22T06:18:44.409096: step 1794, loss 0.00313954, acc 1\n",
            "2019-01-22T06:18:45.094100: step 1795, loss 0.000955636, acc 1\n",
            "2019-01-22T06:18:45.784733: step 1796, loss 0.000957085, acc 1\n",
            "2019-01-22T06:18:46.472012: step 1797, loss 0.000188191, acc 1\n",
            "2019-01-22T06:18:47.155573: step 1798, loss 0.000225867, acc 1\n",
            "2019-01-22T06:18:47.848551: step 1799, loss 0.00102606, acc 1\n",
            "2019-01-22T06:18:48.534787: step 1800, loss 4.37327e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:18:48.898971: step 1800, loss 4.42912e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1800\n",
            "\n",
            "2019-01-22T06:18:49.755952: step 1801, loss 0.000213712, acc 1\n",
            "2019-01-22T06:18:50.444514: step 1802, loss 0.00100119, acc 1\n",
            "2019-01-22T06:18:51.126205: step 1803, loss 0.00594879, acc 1\n",
            "2019-01-22T06:18:51.809681: step 1804, loss 0.0385755, acc 0.984375\n",
            "2019-01-22T06:18:52.490929: step 1805, loss 0.00103205, acc 1\n",
            "2019-01-22T06:18:53.179081: step 1806, loss 0.000381974, acc 1\n",
            "2019-01-22T06:18:53.862496: step 1807, loss 0.0011851, acc 1\n",
            "2019-01-22T06:18:54.549232: step 1808, loss 0.000798326, acc 1\n",
            "2019-01-22T06:18:55.228647: step 1809, loss 0.000381535, acc 1\n",
            "2019-01-22T06:18:55.911964: step 1810, loss 0.0214345, acc 0.984375\n",
            "2019-01-22T06:18:56.600139: step 1811, loss 0.00339802, acc 1\n",
            "2019-01-22T06:18:57.280932: step 1812, loss 5.31702e-05, acc 1\n",
            "2019-01-22T06:18:57.965324: step 1813, loss 0.000574206, acc 1\n",
            "2019-01-22T06:18:58.649367: step 1814, loss 0.00191603, acc 1\n",
            "2019-01-22T06:18:59.061308: step 1815, loss 0.00469897, acc 1\n",
            "2019-01-22T06:18:59.742022: step 1816, loss 0.0135075, acc 0.984375\n",
            "2019-01-22T06:19:00.422120: step 1817, loss 5.23807e-05, acc 1\n",
            "2019-01-22T06:19:01.104817: step 1818, loss 0.00158196, acc 1\n",
            "2019-01-22T06:19:01.790347: step 1819, loss 0.000248032, acc 1\n",
            "2019-01-22T06:19:02.472711: step 1820, loss 8.63203e-05, acc 1\n",
            "2019-01-22T06:19:03.152476: step 1821, loss 0.00210443, acc 1\n",
            "2019-01-22T06:19:03.833779: step 1822, loss 0.00269491, acc 1\n",
            "2019-01-22T06:19:04.515783: step 1823, loss 0.0102154, acc 1\n",
            "2019-01-22T06:19:05.199694: step 1824, loss 0.000744192, acc 1\n",
            "2019-01-22T06:19:05.883123: step 1825, loss 0.0166855, acc 1\n",
            "2019-01-22T06:19:06.563719: step 1826, loss 0.00698159, acc 1\n",
            "2019-01-22T06:19:07.245768: step 1827, loss 4.33089e-05, acc 1\n",
            "2019-01-22T06:19:07.924094: step 1828, loss 0.0110854, acc 1\n",
            "2019-01-22T06:19:08.606657: step 1829, loss 0.000585017, acc 1\n",
            "2019-01-22T06:19:09.286212: step 1830, loss 0.0252891, acc 0.984375\n",
            "2019-01-22T06:19:09.968624: step 1831, loss 0.001992, acc 1\n",
            "2019-01-22T06:19:10.649378: step 1832, loss 0.0966017, acc 0.96875\n",
            "2019-01-22T06:19:11.336831: step 1833, loss 0.0227443, acc 0.984375\n",
            "2019-01-22T06:19:12.019628: step 1834, loss 0.000211675, acc 1\n",
            "2019-01-22T06:19:12.703310: step 1835, loss 0.0015367, acc 1\n",
            "2019-01-22T06:19:13.382710: step 1836, loss 0.00397373, acc 1\n",
            "2019-01-22T06:19:14.061922: step 1837, loss 0.0183237, acc 0.984375\n",
            "2019-01-22T06:19:14.747538: step 1838, loss 0.00326452, acc 1\n",
            "2019-01-22T06:19:15.429419: step 1839, loss 0.000843093, acc 1\n",
            "2019-01-22T06:19:16.113119: step 1840, loss 0.0156268, acc 0.984375\n",
            "2019-01-22T06:19:16.795968: step 1841, loss 9.15055e-05, acc 1\n",
            "2019-01-22T06:19:17.479548: step 1842, loss 0.013547, acc 0.984375\n",
            "2019-01-22T06:19:18.164476: step 1843, loss 0.00231042, acc 1\n",
            "2019-01-22T06:19:18.846564: step 1844, loss 0.00604388, acc 1\n",
            "2019-01-22T06:19:19.529570: step 1845, loss 0.0426081, acc 0.984375\n",
            "2019-01-22T06:19:20.210993: step 1846, loss 0.0212887, acc 0.984375\n",
            "2019-01-22T06:19:20.895416: step 1847, loss 0.0537981, acc 0.984375\n",
            "2019-01-22T06:19:21.311415: step 1848, loss 0.000701925, acc 1\n",
            "2019-01-22T06:19:21.991500: step 1849, loss 0.00123913, acc 1\n",
            "2019-01-22T06:19:22.677790: step 1850, loss 0.00380475, acc 1\n",
            "2019-01-22T06:19:23.360373: step 1851, loss 0.000204992, acc 1\n",
            "2019-01-22T06:19:24.042831: step 1852, loss 0.00218357, acc 1\n",
            "2019-01-22T06:19:24.726175: step 1853, loss 0.00362009, acc 1\n",
            "2019-01-22T06:19:25.406233: step 1854, loss 0.00278807, acc 1\n",
            "2019-01-22T06:19:26.089440: step 1855, loss 0.00012273, acc 1\n",
            "2019-01-22T06:19:26.771248: step 1856, loss 0.00427379, acc 1\n",
            "2019-01-22T06:19:27.451521: step 1857, loss 0.0103715, acc 1\n",
            "2019-01-22T06:19:28.133980: step 1858, loss 0.0519122, acc 0.96875\n",
            "2019-01-22T06:19:28.811564: step 1859, loss 0.00215768, acc 1\n",
            "2019-01-22T06:19:29.496459: step 1860, loss 0.00306859, acc 1\n",
            "2019-01-22T06:19:30.178389: step 1861, loss 0.0001274, acc 1\n",
            "2019-01-22T06:19:30.858495: step 1862, loss 0.00236427, acc 1\n",
            "2019-01-22T06:19:31.546007: step 1863, loss 0.00343144, acc 1\n",
            "2019-01-22T06:19:32.227262: step 1864, loss 0.000387214, acc 1\n",
            "2019-01-22T06:19:32.914990: step 1865, loss 3.93732e-05, acc 1\n",
            "2019-01-22T06:19:33.593536: step 1866, loss 0.00011127, acc 1\n",
            "2019-01-22T06:19:34.278491: step 1867, loss 6.09548e-05, acc 1\n",
            "2019-01-22T06:19:34.961531: step 1868, loss 0.000181816, acc 1\n",
            "2019-01-22T06:19:35.643697: step 1869, loss 0.00637469, acc 1\n",
            "2019-01-22T06:19:36.329850: step 1870, loss 0.000630902, acc 1\n",
            "2019-01-22T06:19:37.008600: step 1871, loss 0.000170746, acc 1\n",
            "2019-01-22T06:19:37.688659: step 1872, loss 0.00283774, acc 1\n",
            "2019-01-22T06:19:38.376499: step 1873, loss 0.00408676, acc 1\n",
            "2019-01-22T06:19:39.056306: step 1874, loss 0.000107025, acc 1\n",
            "2019-01-22T06:19:39.740552: step 1875, loss 0.00212692, acc 1\n",
            "2019-01-22T06:19:40.422255: step 1876, loss 0.000718238, acc 1\n",
            "2019-01-22T06:19:41.101017: step 1877, loss 0.00317626, acc 1\n",
            "2019-01-22T06:19:41.781460: step 1878, loss 0.000234775, acc 1\n",
            "2019-01-22T06:19:42.468338: step 1879, loss 0.0219709, acc 0.984375\n",
            "2019-01-22T06:19:43.149457: step 1880, loss 0.000917985, acc 1\n",
            "2019-01-22T06:19:43.563931: step 1881, loss 0.000923658, acc 1\n",
            "2019-01-22T06:19:44.250005: step 1882, loss 6.13036e-05, acc 1\n",
            "2019-01-22T06:19:44.932223: step 1883, loss 0.000528454, acc 1\n",
            "2019-01-22T06:19:45.612624: step 1884, loss 0.000734215, acc 1\n",
            "2019-01-22T06:19:46.293312: step 1885, loss 0.00293244, acc 1\n",
            "2019-01-22T06:19:46.977115: step 1886, loss 0.000481362, acc 1\n",
            "2019-01-22T06:19:47.653406: step 1887, loss 5.67157e-05, acc 1\n",
            "2019-01-22T06:19:48.333741: step 1888, loss 0.00217359, acc 1\n",
            "2019-01-22T06:19:49.016074: step 1889, loss 0.000310528, acc 1\n",
            "2019-01-22T06:19:49.696281: step 1890, loss 7.03984e-05, acc 1\n",
            "2019-01-22T06:19:50.379689: step 1891, loss 0.0121073, acc 1\n",
            "2019-01-22T06:19:51.060553: step 1892, loss 0.0245904, acc 0.984375\n",
            "2019-01-22T06:19:51.740082: step 1893, loss 0.0272152, acc 0.984375\n",
            "2019-01-22T06:19:52.424561: step 1894, loss 0.00424915, acc 1\n",
            "2019-01-22T06:19:53.102854: step 1895, loss 0.0755495, acc 0.96875\n",
            "2019-01-22T06:19:53.781642: step 1896, loss 0.0301929, acc 0.984375\n",
            "2019-01-22T06:19:54.463939: step 1897, loss 0.00545101, acc 1\n",
            "2019-01-22T06:19:55.146645: step 1898, loss 0.0015425, acc 1\n",
            "2019-01-22T06:19:55.830992: step 1899, loss 0.00102597, acc 1\n",
            "2019-01-22T06:19:56.515688: step 1900, loss 8.3439e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:19:56.878353: step 1900, loss 8.1493e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-1900\n",
            "\n",
            "2019-01-22T06:19:57.700623: step 1901, loss 0.000341231, acc 1\n",
            "2019-01-22T06:19:58.378790: step 1902, loss 0.0005398, acc 1\n",
            "2019-01-22T06:19:59.059723: step 1903, loss 6.25492e-05, acc 1\n",
            "2019-01-22T06:19:59.743104: step 1904, loss 0.00443419, acc 1\n",
            "2019-01-22T06:20:00.425323: step 1905, loss 0.000155604, acc 1\n",
            "2019-01-22T06:20:01.108784: step 1906, loss 0.0014842, acc 1\n",
            "2019-01-22T06:20:01.790464: step 1907, loss 0.000167947, acc 1\n",
            "2019-01-22T06:20:02.468894: step 1908, loss 0.0797224, acc 0.984375\n",
            "2019-01-22T06:20:03.148120: step 1909, loss 0.00151701, acc 1\n",
            "2019-01-22T06:20:03.828073: step 1910, loss 0.000380687, acc 1\n",
            "2019-01-22T06:20:04.507958: step 1911, loss 0.00675515, acc 1\n",
            "2019-01-22T06:20:05.189232: step 1912, loss 0.0255866, acc 0.984375\n",
            "2019-01-22T06:20:05.875721: step 1913, loss 0.0115015, acc 0.984375\n",
            "2019-01-22T06:20:06.293611: step 1914, loss 0.0357012, acc 0.971429\n",
            "2019-01-22T06:20:06.978592: step 1915, loss 0.000267978, acc 1\n",
            "2019-01-22T06:20:07.669432: step 1916, loss 0.000431116, acc 1\n",
            "2019-01-22T06:20:08.347042: step 1917, loss 0.00543367, acc 1\n",
            "2019-01-22T06:20:09.035090: step 1918, loss 0.000370373, acc 1\n",
            "2019-01-22T06:20:09.714303: step 1919, loss 0.00100184, acc 1\n",
            "2019-01-22T06:20:10.394940: step 1920, loss 0.114135, acc 0.984375\n",
            "2019-01-22T06:20:11.081186: step 1921, loss 0.000241906, acc 1\n",
            "2019-01-22T06:20:11.765345: step 1922, loss 0.00977996, acc 1\n",
            "2019-01-22T06:20:12.445341: step 1923, loss 0.000358467, acc 1\n",
            "2019-01-22T06:20:13.126530: step 1924, loss 0.0216228, acc 0.984375\n",
            "2019-01-22T06:20:13.808302: step 1925, loss 7.56353e-05, acc 1\n",
            "2019-01-22T06:20:14.490712: step 1926, loss 0.00100282, acc 1\n",
            "2019-01-22T06:20:15.173801: step 1927, loss 0.0366024, acc 0.984375\n",
            "2019-01-22T06:20:15.856925: step 1928, loss 6.52531e-05, acc 1\n",
            "2019-01-22T06:20:16.540621: step 1929, loss 0.0214706, acc 0.984375\n",
            "2019-01-22T06:20:17.221513: step 1930, loss 5.63765e-05, acc 1\n",
            "2019-01-22T06:20:17.905118: step 1931, loss 0.000113308, acc 1\n",
            "2019-01-22T06:20:18.588461: step 1932, loss 0.000699518, acc 1\n",
            "2019-01-22T06:20:19.271939: step 1933, loss 0.00178496, acc 1\n",
            "2019-01-22T06:20:19.953088: step 1934, loss 0.0221042, acc 0.984375\n",
            "2019-01-22T06:20:20.635091: step 1935, loss 9.74308e-06, acc 1\n",
            "2019-01-22T06:20:21.321863: step 1936, loss 0.00659401, acc 1\n",
            "2019-01-22T06:20:22.005961: step 1937, loss 0.00366387, acc 1\n",
            "2019-01-22T06:20:22.687603: step 1938, loss 0.000152839, acc 1\n",
            "2019-01-22T06:20:23.369798: step 1939, loss 0.00876838, acc 1\n",
            "2019-01-22T06:20:24.051136: step 1940, loss 0.00072673, acc 1\n",
            "2019-01-22T06:20:24.733280: step 1941, loss 0.0480264, acc 0.96875\n",
            "2019-01-22T06:20:25.414274: step 1942, loss 0.0290185, acc 0.984375\n",
            "2019-01-22T06:20:26.095138: step 1943, loss 0.00214413, acc 1\n",
            "2019-01-22T06:20:26.780120: step 1944, loss 0.0119888, acc 0.984375\n",
            "2019-01-22T06:20:27.463238: step 1945, loss 0.000154841, acc 1\n",
            "2019-01-22T06:20:28.140788: step 1946, loss 0.00775574, acc 1\n",
            "2019-01-22T06:20:28.561379: step 1947, loss 0.00707482, acc 1\n",
            "2019-01-22T06:20:29.245703: step 1948, loss 5.77142e-05, acc 1\n",
            "2019-01-22T06:20:29.928500: step 1949, loss 0.00053317, acc 1\n",
            "2019-01-22T06:20:30.617111: step 1950, loss 0.000181533, acc 1\n",
            "2019-01-22T06:20:31.299802: step 1951, loss 0.00023001, acc 1\n",
            "2019-01-22T06:20:31.981931: step 1952, loss 0.000752495, acc 1\n",
            "2019-01-22T06:20:32.671736: step 1953, loss 1.67798e-05, acc 1\n",
            "2019-01-22T06:20:33.351346: step 1954, loss 0.00317458, acc 1\n",
            "2019-01-22T06:20:34.030884: step 1955, loss 0.00100101, acc 1\n",
            "2019-01-22T06:20:34.718680: step 1956, loss 0.000383051, acc 1\n",
            "2019-01-22T06:20:35.399761: step 1957, loss 4.95633e-05, acc 1\n",
            "2019-01-22T06:20:36.081533: step 1958, loss 0.00108012, acc 1\n",
            "2019-01-22T06:20:36.771086: step 1959, loss 0.0120421, acc 0.984375\n",
            "2019-01-22T06:20:37.451300: step 1960, loss 0.000156466, acc 1\n",
            "2019-01-22T06:20:38.132463: step 1961, loss 0.00448236, acc 1\n",
            "2019-01-22T06:20:38.822882: step 1962, loss 0.000575689, acc 1\n",
            "2019-01-22T06:20:39.503634: step 1963, loss 0.054655, acc 0.984375\n",
            "2019-01-22T06:20:40.184823: step 1964, loss 0.00226465, acc 1\n",
            "2019-01-22T06:20:40.870671: step 1965, loss 0.00301922, acc 1\n",
            "2019-01-22T06:20:41.553264: step 1966, loss 0.000139645, acc 1\n",
            "2019-01-22T06:20:42.232769: step 1967, loss 9.61347e-05, acc 1\n",
            "2019-01-22T06:20:42.917258: step 1968, loss 0.000864098, acc 1\n",
            "2019-01-22T06:20:43.603463: step 1969, loss 0.0263331, acc 0.984375\n",
            "2019-01-22T06:20:44.288952: step 1970, loss 0.0293756, acc 0.984375\n",
            "2019-01-22T06:20:44.979969: step 1971, loss 0.00579122, acc 1\n",
            "2019-01-22T06:20:45.664458: step 1972, loss 0.00474669, acc 1\n",
            "2019-01-22T06:20:46.344821: step 1973, loss 0.0104161, acc 1\n",
            "2019-01-22T06:20:47.030701: step 1974, loss 0.000138947, acc 1\n",
            "2019-01-22T06:20:47.715824: step 1975, loss 0.0115095, acc 1\n",
            "2019-01-22T06:20:48.398624: step 1976, loss 0.000392528, acc 1\n",
            "2019-01-22T06:20:49.079382: step 1977, loss 0.000410685, acc 1\n",
            "2019-01-22T06:20:49.760635: step 1978, loss 0.0130977, acc 1\n",
            "2019-01-22T06:20:50.441584: step 1979, loss 0.0010333, acc 1\n",
            "2019-01-22T06:20:50.859566: step 1980, loss 0.00154634, acc 1\n",
            "2019-01-22T06:20:51.541961: step 1981, loss 0.000877448, acc 1\n",
            "2019-01-22T06:20:52.229316: step 1982, loss 0.000139116, acc 1\n",
            "2019-01-22T06:20:52.915251: step 1983, loss 0.0103443, acc 1\n",
            "2019-01-22T06:20:53.594067: step 1984, loss 0.000130022, acc 1\n",
            "2019-01-22T06:20:54.281134: step 1985, loss 0.108791, acc 0.984375\n",
            "2019-01-22T06:20:54.962217: step 1986, loss 0.00324564, acc 1\n",
            "2019-01-22T06:20:55.643848: step 1987, loss 0.000663645, acc 1\n",
            "2019-01-22T06:20:56.330579: step 1988, loss 0.000294988, acc 1\n",
            "2019-01-22T06:20:57.013822: step 1989, loss 2.86535e-05, acc 1\n",
            "2019-01-22T06:20:57.693587: step 1990, loss 0.000157152, acc 1\n",
            "2019-01-22T06:20:58.382868: step 1991, loss 0.000223723, acc 1\n",
            "2019-01-22T06:20:59.062268: step 1992, loss 0.00325851, acc 1\n",
            "2019-01-22T06:20:59.745677: step 1993, loss 0.00393034, acc 1\n",
            "2019-01-22T06:21:00.437575: step 1994, loss 0.000621398, acc 1\n",
            "2019-01-22T06:21:01.122842: step 1995, loss 0.00375823, acc 1\n",
            "2019-01-22T06:21:01.809075: step 1996, loss 0.0117686, acc 0.984375\n",
            "2019-01-22T06:21:02.499108: step 1997, loss 0.0590581, acc 0.96875\n",
            "2019-01-22T06:21:03.179776: step 1998, loss 0.00059335, acc 1\n",
            "2019-01-22T06:21:03.858015: step 1999, loss 0.00216235, acc 1\n",
            "2019-01-22T06:21:04.545025: step 2000, loss 0.00138023, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:21:04.902563: step 2000, loss 0.000116316, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2000\n",
            "\n",
            "2019-01-22T06:21:05.723197: step 2001, loss 0.000847539, acc 1\n",
            "2019-01-22T06:21:06.400980: step 2002, loss 0.00496135, acc 1\n",
            "2019-01-22T06:21:07.079146: step 2003, loss 2.23941e-05, acc 1\n",
            "2019-01-22T06:21:07.759742: step 2004, loss 0.00139955, acc 1\n",
            "2019-01-22T06:21:08.442369: step 2005, loss 0.0148884, acc 0.984375\n",
            "2019-01-22T06:21:09.124368: step 2006, loss 0.000105815, acc 1\n",
            "2019-01-22T06:21:09.803349: step 2007, loss 0.01598, acc 0.984375\n",
            "2019-01-22T06:21:10.482986: step 2008, loss 0.000943042, acc 1\n",
            "2019-01-22T06:21:11.161131: step 2009, loss 0.0194523, acc 0.984375\n",
            "2019-01-22T06:21:11.844452: step 2010, loss 0.000113333, acc 1\n",
            "2019-01-22T06:21:12.524806: step 2011, loss 0.0107623, acc 1\n",
            "2019-01-22T06:21:13.199630: step 2012, loss 0.000530338, acc 1\n",
            "2019-01-22T06:21:13.614623: step 2013, loss 0.00110508, acc 1\n",
            "2019-01-22T06:21:14.296672: step 2014, loss 0.0314976, acc 0.984375\n",
            "2019-01-22T06:21:14.973433: step 2015, loss 0.000289069, acc 1\n",
            "2019-01-22T06:21:15.654810: step 2016, loss 0.000692831, acc 1\n",
            "2019-01-22T06:21:16.333303: step 2017, loss 0.000128698, acc 1\n",
            "2019-01-22T06:21:17.017085: step 2018, loss 0.0824247, acc 0.984375\n",
            "2019-01-22T06:21:17.698490: step 2019, loss 0.000100591, acc 1\n",
            "2019-01-22T06:21:18.383598: step 2020, loss 0.00772143, acc 1\n",
            "2019-01-22T06:21:19.059572: step 2021, loss 0.0082194, acc 1\n",
            "2019-01-22T06:21:19.739802: step 2022, loss 0.000560238, acc 1\n",
            "2019-01-22T06:21:20.423839: step 2023, loss 0.0001017, acc 1\n",
            "2019-01-22T06:21:21.106966: step 2024, loss 0.0286421, acc 0.984375\n",
            "2019-01-22T06:21:21.793658: step 2025, loss 0.00111018, acc 1\n",
            "2019-01-22T06:21:22.468766: step 2026, loss 0.0277302, acc 0.984375\n",
            "2019-01-22T06:21:23.147301: step 2027, loss 0.00225374, acc 1\n",
            "2019-01-22T06:21:23.824855: step 2028, loss 4.0618e-05, acc 1\n",
            "2019-01-22T06:21:24.508697: step 2029, loss 0.00248579, acc 1\n",
            "2019-01-22T06:21:25.189232: step 2030, loss 0.00141601, acc 1\n",
            "2019-01-22T06:21:25.865060: step 2031, loss 0.000356126, acc 1\n",
            "2019-01-22T06:21:26.550414: step 2032, loss 0.00144949, acc 1\n",
            "2019-01-22T06:21:27.231257: step 2033, loss 0.00505846, acc 1\n",
            "2019-01-22T06:21:27.914350: step 2034, loss 0.00147423, acc 1\n",
            "2019-01-22T06:21:28.595114: step 2035, loss 0.0132332, acc 0.984375\n",
            "2019-01-22T06:21:29.278081: step 2036, loss 0.000696068, acc 1\n",
            "2019-01-22T06:21:29.959655: step 2037, loss 0.00245742, acc 1\n",
            "2019-01-22T06:21:30.637975: step 2038, loss 0.000317626, acc 1\n",
            "2019-01-22T06:21:31.330895: step 2039, loss 0.00760311, acc 1\n",
            "2019-01-22T06:21:32.014443: step 2040, loss 0.0100018, acc 1\n",
            "2019-01-22T06:21:32.693772: step 2041, loss 0.0019913, acc 1\n",
            "2019-01-22T06:21:33.379871: step 2042, loss 0.00213482, acc 1\n",
            "2019-01-22T06:21:34.056817: step 2043, loss 0.00748135, acc 1\n",
            "2019-01-22T06:21:34.742147: step 2044, loss 0.0209444, acc 0.984375\n",
            "2019-01-22T06:21:35.429689: step 2045, loss 0.0102052, acc 1\n",
            "2019-01-22T06:21:35.840765: step 2046, loss 0.00826825, acc 1\n",
            "2019-01-22T06:21:36.521207: step 2047, loss 0.0192865, acc 0.984375\n",
            "2019-01-22T06:21:37.201333: step 2048, loss 0.00179113, acc 1\n",
            "2019-01-22T06:21:37.881334: step 2049, loss 0.000884074, acc 1\n",
            "2019-01-22T06:21:38.558936: step 2050, loss 0.000444447, acc 1\n",
            "2019-01-22T06:21:39.236421: step 2051, loss 0.00496245, acc 1\n",
            "2019-01-22T06:21:39.914961: step 2052, loss 0.000654118, acc 1\n",
            "2019-01-22T06:21:40.599595: step 2053, loss 0.00261977, acc 1\n",
            "2019-01-22T06:21:41.277140: step 2054, loss 0.000215407, acc 1\n",
            "2019-01-22T06:21:41.968349: step 2055, loss 0.00019274, acc 1\n",
            "2019-01-22T06:21:42.648877: step 2056, loss 0.000206742, acc 1\n",
            "2019-01-22T06:21:43.327984: step 2057, loss 0.00287196, acc 1\n",
            "2019-01-22T06:21:44.009306: step 2058, loss 0.0394585, acc 0.984375\n",
            "2019-01-22T06:21:44.696509: step 2059, loss 0.000132667, acc 1\n",
            "2019-01-22T06:21:45.383739: step 2060, loss 0.0574409, acc 0.984375\n",
            "2019-01-22T06:21:46.066771: step 2061, loss 0.0152783, acc 0.984375\n",
            "2019-01-22T06:21:46.753365: step 2062, loss 0.00105722, acc 1\n",
            "2019-01-22T06:21:47.434343: step 2063, loss 0.00397588, acc 1\n",
            "2019-01-22T06:21:48.111442: step 2064, loss 0.00126542, acc 1\n",
            "2019-01-22T06:21:48.803139: step 2065, loss 0.0314156, acc 0.984375\n",
            "2019-01-22T06:21:49.484651: step 2066, loss 3.10093e-05, acc 1\n",
            "2019-01-22T06:21:50.163755: step 2067, loss 0.000114921, acc 1\n",
            "2019-01-22T06:21:50.846950: step 2068, loss 9.55486e-05, acc 1\n",
            "2019-01-22T06:21:51.524664: step 2069, loss 0.0326964, acc 0.984375\n",
            "2019-01-22T06:21:52.207868: step 2070, loss 0.000555106, acc 1\n",
            "2019-01-22T06:21:52.895262: step 2071, loss 0.000163342, acc 1\n",
            "2019-01-22T06:21:53.573054: step 2072, loss 0.000238474, acc 1\n",
            "2019-01-22T06:21:54.245869: step 2073, loss 1.85812e-05, acc 1\n",
            "2019-01-22T06:21:54.939390: step 2074, loss 0.0151683, acc 0.984375\n",
            "2019-01-22T06:21:55.620548: step 2075, loss 6.78614e-05, acc 1\n",
            "2019-01-22T06:21:56.299845: step 2076, loss 0.00103524, acc 1\n",
            "2019-01-22T06:21:56.986392: step 2077, loss 0.000725219, acc 1\n",
            "2019-01-22T06:21:57.661078: step 2078, loss 0.00139107, acc 1\n",
            "2019-01-22T06:21:58.072758: step 2079, loss 0.00556385, acc 1\n",
            "2019-01-22T06:21:58.752412: step 2080, loss 0.00696434, acc 1\n",
            "2019-01-22T06:21:59.431897: step 2081, loss 0.0140584, acc 0.984375\n",
            "2019-01-22T06:22:00.109742: step 2082, loss 0.00126925, acc 1\n",
            "2019-01-22T06:22:00.785346: step 2083, loss 0.0391348, acc 0.984375\n",
            "2019-01-22T06:22:01.466451: step 2084, loss 0.00159225, acc 1\n",
            "2019-01-22T06:22:02.156185: step 2085, loss 0.00160873, acc 1\n",
            "2019-01-22T06:22:02.837269: step 2086, loss 0.00397045, acc 1\n",
            "2019-01-22T06:22:03.517570: step 2087, loss 0.0154396, acc 1\n",
            "2019-01-22T06:22:04.206285: step 2088, loss 0.0229153, acc 0.984375\n",
            "2019-01-22T06:22:04.893485: step 2089, loss 0.00256838, acc 1\n",
            "2019-01-22T06:22:05.580352: step 2090, loss 0.00010681, acc 1\n",
            "2019-01-22T06:22:06.272014: step 2091, loss 0.00061477, acc 1\n",
            "2019-01-22T06:22:06.950478: step 2092, loss 0.00113724, acc 1\n",
            "2019-01-22T06:22:07.631344: step 2093, loss 0.0830776, acc 0.984375\n",
            "2019-01-22T06:22:08.315290: step 2094, loss 0.0123396, acc 0.984375\n",
            "2019-01-22T06:22:08.996412: step 2095, loss 0.00915705, acc 1\n",
            "2019-01-22T06:22:09.671620: step 2096, loss 0.00908276, acc 1\n",
            "2019-01-22T06:22:10.354721: step 2097, loss 0.00100361, acc 1\n",
            "2019-01-22T06:22:11.033545: step 2098, loss 0.0097776, acc 1\n",
            "2019-01-22T06:22:11.711129: step 2099, loss 0.000933351, acc 1\n",
            "2019-01-22T06:22:12.403855: step 2100, loss 0.000679826, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:22:12.765624: step 2100, loss 5.10973e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2100\n",
            "\n",
            "2019-01-22T06:22:13.562069: step 2101, loss 0.000125214, acc 1\n",
            "2019-01-22T06:22:14.238710: step 2102, loss 0.000287904, acc 1\n",
            "2019-01-22T06:22:14.917100: step 2103, loss 0.000493108, acc 1\n",
            "2019-01-22T06:22:15.598889: step 2104, loss 0.0393809, acc 0.984375\n",
            "2019-01-22T06:22:16.279670: step 2105, loss 0.00116397, acc 1\n",
            "2019-01-22T06:22:16.963818: step 2106, loss 0.0342056, acc 0.96875\n",
            "2019-01-22T06:22:17.645787: step 2107, loss 0.0107429, acc 1\n",
            "2019-01-22T06:22:18.330501: step 2108, loss 0.000558472, acc 1\n",
            "2019-01-22T06:22:19.016407: step 2109, loss 0.000122492, acc 1\n",
            "2019-01-22T06:22:19.697890: step 2110, loss 0.000206223, acc 1\n",
            "2019-01-22T06:22:20.384608: step 2111, loss 0.00506316, acc 1\n",
            "2019-01-22T06:22:20.799619: step 2112, loss 0.000890981, acc 1\n",
            "2019-01-22T06:22:21.493132: step 2113, loss 0.000379348, acc 1\n",
            "2019-01-22T06:22:22.178963: step 2114, loss 0.000274652, acc 1\n",
            "2019-01-22T06:22:22.863188: step 2115, loss 0.0289084, acc 0.984375\n",
            "2019-01-22T06:22:23.546404: step 2116, loss 0.0443178, acc 0.984375\n",
            "2019-01-22T06:22:24.226877: step 2117, loss 0.0201473, acc 0.984375\n",
            "2019-01-22T06:22:24.906939: step 2118, loss 0.000109819, acc 1\n",
            "2019-01-22T06:22:25.584940: step 2119, loss 1.4961e-05, acc 1\n",
            "2019-01-22T06:22:26.271106: step 2120, loss 4.78974e-05, acc 1\n",
            "2019-01-22T06:22:26.951287: step 2121, loss 8.61646e-05, acc 1\n",
            "2019-01-22T06:22:27.627041: step 2122, loss 0.0386892, acc 0.984375\n",
            "2019-01-22T06:22:28.305899: step 2123, loss 0.000334134, acc 1\n",
            "2019-01-22T06:22:28.985390: step 2124, loss 0.000366698, acc 1\n",
            "2019-01-22T06:22:29.671051: step 2125, loss 0.000894394, acc 1\n",
            "2019-01-22T06:22:30.349739: step 2126, loss 0.0373972, acc 0.984375\n",
            "2019-01-22T06:22:31.033610: step 2127, loss 0.000151942, acc 1\n",
            "2019-01-22T06:22:31.713060: step 2128, loss 5.32109e-05, acc 1\n",
            "2019-01-22T06:22:32.390914: step 2129, loss 0.000634198, acc 1\n",
            "2019-01-22T06:22:33.067259: step 2130, loss 0.000332839, acc 1\n",
            "2019-01-22T06:22:33.749691: step 2131, loss 0.000326072, acc 1\n",
            "2019-01-22T06:22:34.430531: step 2132, loss 0.0375237, acc 0.984375\n",
            "2019-01-22T06:22:35.111549: step 2133, loss 0.000818656, acc 1\n",
            "2019-01-22T06:22:35.796238: step 2134, loss 0.075016, acc 0.984375\n",
            "2019-01-22T06:22:36.477708: step 2135, loss 0.000289091, acc 1\n",
            "2019-01-22T06:22:37.165618: step 2136, loss 0.000493912, acc 1\n",
            "2019-01-22T06:22:37.845342: step 2137, loss 0.000122326, acc 1\n",
            "2019-01-22T06:22:38.524139: step 2138, loss 0.000748777, acc 1\n",
            "2019-01-22T06:22:39.218352: step 2139, loss 0.00160277, acc 1\n",
            "2019-01-22T06:22:39.899322: step 2140, loss 0.00864061, acc 1\n",
            "2019-01-22T06:22:40.583872: step 2141, loss 0.000277084, acc 1\n",
            "2019-01-22T06:22:41.271644: step 2142, loss 0.00149655, acc 1\n",
            "2019-01-22T06:22:41.951120: step 2143, loss 0.00651985, acc 1\n",
            "2019-01-22T06:22:42.638090: step 2144, loss 0.00168376, acc 1\n",
            "2019-01-22T06:22:43.052553: step 2145, loss 0.000127982, acc 1\n",
            "2019-01-22T06:22:43.734431: step 2146, loss 0.000147201, acc 1\n",
            "2019-01-22T06:22:44.416100: step 2147, loss 0.000927549, acc 1\n",
            "2019-01-22T06:22:45.093994: step 2148, loss 8.8829e-05, acc 1\n",
            "2019-01-22T06:22:45.778366: step 2149, loss 0.000364581, acc 1\n",
            "2019-01-22T06:22:46.465787: step 2150, loss 0.000242103, acc 1\n",
            "2019-01-22T06:22:47.148768: step 2151, loss 0.000192054, acc 1\n",
            "2019-01-22T06:22:47.827802: step 2152, loss 0.000226539, acc 1\n",
            "2019-01-22T06:22:48.509856: step 2153, loss 0.00223135, acc 1\n",
            "2019-01-22T06:22:49.191275: step 2154, loss 0.00194636, acc 1\n",
            "2019-01-22T06:22:49.872481: step 2155, loss 0.147579, acc 0.984375\n",
            "2019-01-22T06:22:50.559419: step 2156, loss 0.00044963, acc 1\n",
            "2019-01-22T06:22:51.241047: step 2157, loss 0.00151536, acc 1\n",
            "2019-01-22T06:22:51.924451: step 2158, loss 0.00242224, acc 1\n",
            "2019-01-22T06:22:52.614349: step 2159, loss 0.035752, acc 0.984375\n",
            "2019-01-22T06:22:53.302145: step 2160, loss 0.000664372, acc 1\n",
            "2019-01-22T06:22:53.985030: step 2161, loss 7.37173e-05, acc 1\n",
            "2019-01-22T06:22:54.675469: step 2162, loss 0.00425438, acc 1\n",
            "2019-01-22T06:22:55.357019: step 2163, loss 0.000353385, acc 1\n",
            "2019-01-22T06:22:56.038239: step 2164, loss 0.00389052, acc 1\n",
            "2019-01-22T06:22:56.728312: step 2165, loss 0.0192561, acc 0.984375\n",
            "2019-01-22T06:22:57.410057: step 2166, loss 0.00014071, acc 1\n",
            "2019-01-22T06:22:58.095487: step 2167, loss 0.000473169, acc 1\n",
            "2019-01-22T06:22:58.785777: step 2168, loss 0.000400327, acc 1\n",
            "2019-01-22T06:22:59.467423: step 2169, loss 0.0125448, acc 0.984375\n",
            "2019-01-22T06:23:00.149303: step 2170, loss 0.00221078, acc 1\n",
            "2019-01-22T06:23:00.836605: step 2171, loss 0.0243702, acc 0.984375\n",
            "2019-01-22T06:23:01.519033: step 2172, loss 0.000212142, acc 1\n",
            "2019-01-22T06:23:02.199003: step 2173, loss 0.000629394, acc 1\n",
            "2019-01-22T06:23:02.884966: step 2174, loss 0.000135565, acc 1\n",
            "2019-01-22T06:23:03.565941: step 2175, loss 0.0118328, acc 0.984375\n",
            "2019-01-22T06:23:04.248856: step 2176, loss 0.00860092, acc 1\n",
            "2019-01-22T06:23:04.943260: step 2177, loss 0.0025322, acc 1\n",
            "2019-01-22T06:23:05.359290: step 2178, loss 9.45754e-05, acc 1\n",
            "2019-01-22T06:23:06.040232: step 2179, loss 0.00302041, acc 1\n",
            "2019-01-22T06:23:06.724912: step 2180, loss 0.00192642, acc 1\n",
            "2019-01-22T06:23:07.410308: step 2181, loss 0.000341518, acc 1\n",
            "2019-01-22T06:23:08.096112: step 2182, loss 0.00210482, acc 1\n",
            "2019-01-22T06:23:08.784876: step 2183, loss 0.00106209, acc 1\n",
            "2019-01-22T06:23:09.470384: step 2184, loss 0.00261815, acc 1\n",
            "2019-01-22T06:23:10.153258: step 2185, loss 0.0128135, acc 1\n",
            "2019-01-22T06:23:10.840448: step 2186, loss 7.58197e-05, acc 1\n",
            "2019-01-22T06:23:11.524703: step 2187, loss 0.000496478, acc 1\n",
            "2019-01-22T06:23:12.207849: step 2188, loss 0.00131188, acc 1\n",
            "2019-01-22T06:23:12.895292: step 2189, loss 0.00122708, acc 1\n",
            "2019-01-22T06:23:13.578463: step 2190, loss 0.00670008, acc 1\n",
            "2019-01-22T06:23:14.267912: step 2191, loss 0.000111634, acc 1\n",
            "2019-01-22T06:23:14.950846: step 2192, loss 0.000481401, acc 1\n",
            "2019-01-22T06:23:15.634555: step 2193, loss 0.00300835, acc 1\n",
            "2019-01-22T06:23:16.335983: step 2194, loss 0.000395304, acc 1\n",
            "2019-01-22T06:23:17.017677: step 2195, loss 0.00383597, acc 1\n",
            "2019-01-22T06:23:17.702256: step 2196, loss 0.000267477, acc 1\n",
            "2019-01-22T06:23:18.387463: step 2197, loss 0.00238613, acc 1\n",
            "2019-01-22T06:23:19.065906: step 2198, loss 0.00308692, acc 1\n",
            "2019-01-22T06:23:19.747602: step 2199, loss 0.000906645, acc 1\n",
            "2019-01-22T06:23:20.437394: step 2200, loss 0.00917076, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:23:20.799438: step 2200, loss 6.58742e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2200\n",
            "\n",
            "2019-01-22T06:23:21.646626: step 2201, loss 0.000100546, acc 1\n",
            "2019-01-22T06:23:22.327280: step 2202, loss 0.000662887, acc 1\n",
            "2019-01-22T06:23:23.007761: step 2203, loss 0.000893363, acc 1\n",
            "2019-01-22T06:23:23.691393: step 2204, loss 0.000269145, acc 1\n",
            "2019-01-22T06:23:24.374321: step 2205, loss 0.00949099, acc 1\n",
            "2019-01-22T06:23:25.054863: step 2206, loss 0.0225531, acc 0.984375\n",
            "2019-01-22T06:23:25.736151: step 2207, loss 4.3535e-05, acc 1\n",
            "2019-01-22T06:23:26.422796: step 2208, loss 0.00084428, acc 1\n",
            "2019-01-22T06:23:27.104747: step 2209, loss 0.00327723, acc 1\n",
            "2019-01-22T06:23:27.789239: step 2210, loss 0.00283346, acc 1\n",
            "2019-01-22T06:23:28.207138: step 2211, loss 0.000157279, acc 1\n",
            "2019-01-22T06:23:28.888561: step 2212, loss 6.75782e-05, acc 1\n",
            "2019-01-22T06:23:29.570853: step 2213, loss 0.00590757, acc 1\n",
            "2019-01-22T06:23:30.253974: step 2214, loss 0.000834359, acc 1\n",
            "2019-01-22T06:23:30.938350: step 2215, loss 0.0597301, acc 0.984375\n",
            "2019-01-22T06:23:31.625786: step 2216, loss 0.00206404, acc 1\n",
            "2019-01-22T06:23:32.307053: step 2217, loss 0.000534559, acc 1\n",
            "2019-01-22T06:23:32.992583: step 2218, loss 0.00328911, acc 1\n",
            "2019-01-22T06:23:33.675826: step 2219, loss 0.000136344, acc 1\n",
            "2019-01-22T06:23:34.361019: step 2220, loss 0.00815393, acc 1\n",
            "2019-01-22T06:23:35.046549: step 2221, loss 0.000176891, acc 1\n",
            "2019-01-22T06:23:35.725450: step 2222, loss 0.000112475, acc 1\n",
            "2019-01-22T06:23:36.406688: step 2223, loss 0.000358347, acc 1\n",
            "2019-01-22T06:23:37.090066: step 2224, loss 2.93229e-05, acc 1\n",
            "2019-01-22T06:23:37.773736: step 2225, loss 0.00862304, acc 1\n",
            "2019-01-22T06:23:38.457559: step 2226, loss 0.0331815, acc 0.984375\n",
            "2019-01-22T06:23:39.137739: step 2227, loss 0.0421701, acc 0.984375\n",
            "2019-01-22T06:23:39.816231: step 2228, loss 0.000566564, acc 1\n",
            "2019-01-22T06:23:40.497119: step 2229, loss 0.0024034, acc 1\n",
            "2019-01-22T06:23:41.184681: step 2230, loss 0.00132472, acc 1\n",
            "2019-01-22T06:23:41.865563: step 2231, loss 0.000848402, acc 1\n",
            "2019-01-22T06:23:42.544244: step 2232, loss 0.000159029, acc 1\n",
            "2019-01-22T06:23:43.222363: step 2233, loss 0.000494547, acc 1\n",
            "2019-01-22T06:23:43.895862: step 2234, loss 0.0101039, acc 1\n",
            "2019-01-22T06:23:44.577363: step 2235, loss 0.000756431, acc 1\n",
            "2019-01-22T06:23:45.254483: step 2236, loss 0.0129873, acc 1\n",
            "2019-01-22T06:23:45.931079: step 2237, loss 0.000872859, acc 1\n",
            "2019-01-22T06:23:46.610121: step 2238, loss 5.34776e-05, acc 1\n",
            "2019-01-22T06:23:47.291533: step 2239, loss 0.0736134, acc 0.96875\n",
            "2019-01-22T06:23:47.968786: step 2240, loss 0.112596, acc 0.984375\n",
            "2019-01-22T06:23:48.649081: step 2241, loss 3.00388e-05, acc 1\n",
            "2019-01-22T06:23:49.331336: step 2242, loss 2.64593e-05, acc 1\n",
            "2019-01-22T06:23:50.014985: step 2243, loss 0.000328519, acc 1\n",
            "2019-01-22T06:23:50.428234: step 2244, loss 8.33193e-05, acc 1\n",
            "2019-01-22T06:23:51.109315: step 2245, loss 0.00105401, acc 1\n",
            "2019-01-22T06:23:51.788124: step 2246, loss 0.000224648, acc 1\n",
            "2019-01-22T06:23:52.468300: step 2247, loss 0.00125514, acc 1\n",
            "2019-01-22T06:23:53.154748: step 2248, loss 0.0152171, acc 0.984375\n",
            "2019-01-22T06:23:53.836636: step 2249, loss 0.000386475, acc 1\n",
            "2019-01-22T06:23:54.513891: step 2250, loss 0.000677564, acc 1\n",
            "2019-01-22T06:23:55.193460: step 2251, loss 0.00497687, acc 1\n",
            "2019-01-22T06:23:55.881627: step 2252, loss 1.77299e-05, acc 1\n",
            "2019-01-22T06:23:56.564798: step 2253, loss 0.000669324, acc 1\n",
            "2019-01-22T06:23:57.251376: step 2254, loss 0.00505604, acc 1\n",
            "2019-01-22T06:23:57.928365: step 2255, loss 5.12818e-05, acc 1\n",
            "2019-01-22T06:23:58.611572: step 2256, loss 0.00374415, acc 1\n",
            "2019-01-22T06:23:59.292807: step 2257, loss 0.0244966, acc 0.984375\n",
            "2019-01-22T06:23:59.972381: step 2258, loss 0.0138365, acc 0.984375\n",
            "2019-01-22T06:24:00.656423: step 2259, loss 0.00126242, acc 1\n",
            "2019-01-22T06:24:01.335980: step 2260, loss 0.00164463, acc 1\n",
            "2019-01-22T06:24:02.016021: step 2261, loss 0.000294665, acc 1\n",
            "2019-01-22T06:24:02.695603: step 2262, loss 0.000137098, acc 1\n",
            "2019-01-22T06:24:03.379658: step 2263, loss 0.00160897, acc 1\n",
            "2019-01-22T06:24:04.058636: step 2264, loss 0.00131967, acc 1\n",
            "2019-01-22T06:24:04.741615: step 2265, loss 0.0523262, acc 0.96875\n",
            "2019-01-22T06:24:05.419555: step 2266, loss 0.00373885, acc 1\n",
            "2019-01-22T06:24:06.101329: step 2267, loss 0.003026, acc 1\n",
            "2019-01-22T06:24:06.787033: step 2268, loss 0.000132786, acc 1\n",
            "2019-01-22T06:24:07.470040: step 2269, loss 0.0411361, acc 0.984375\n",
            "2019-01-22T06:24:08.149890: step 2270, loss 0.00470385, acc 1\n",
            "2019-01-22T06:24:08.829120: step 2271, loss 0.00151492, acc 1\n",
            "2019-01-22T06:24:09.517049: step 2272, loss 0.00350325, acc 1\n",
            "2019-01-22T06:24:10.196934: step 2273, loss 0.0271441, acc 0.984375\n",
            "2019-01-22T06:24:10.876622: step 2274, loss 0.113777, acc 0.984375\n",
            "2019-01-22T06:24:11.554361: step 2275, loss 7.84508e-05, acc 1\n",
            "2019-01-22T06:24:12.232014: step 2276, loss 0.000244776, acc 1\n",
            "2019-01-22T06:24:12.644609: step 2277, loss 2.214e-05, acc 1\n",
            "2019-01-22T06:24:13.324307: step 2278, loss 0.0653556, acc 0.984375\n",
            "2019-01-22T06:24:14.008454: step 2279, loss 0.0657634, acc 0.96875\n",
            "2019-01-22T06:24:14.687835: step 2280, loss 0.000234986, acc 1\n",
            "2019-01-22T06:24:15.370368: step 2281, loss 0.000186188, acc 1\n",
            "2019-01-22T06:24:16.054366: step 2282, loss 0.00128349, acc 1\n",
            "2019-01-22T06:24:16.733686: step 2283, loss 0.00934092, acc 1\n",
            "2019-01-22T06:24:17.413195: step 2284, loss 0.00235029, acc 1\n",
            "2019-01-22T06:24:18.099060: step 2285, loss 0.0179539, acc 0.984375\n",
            "2019-01-22T06:24:18.776767: step 2286, loss 0.00205154, acc 1\n",
            "2019-01-22T06:24:19.455706: step 2287, loss 1.65784e-05, acc 1\n",
            "2019-01-22T06:24:20.142911: step 2288, loss 0.000163587, acc 1\n",
            "2019-01-22T06:24:20.821438: step 2289, loss 9.99429e-05, acc 1\n",
            "2019-01-22T06:24:21.502607: step 2290, loss 0.00130931, acc 1\n",
            "2019-01-22T06:24:22.193348: step 2291, loss 0.000170078, acc 1\n",
            "2019-01-22T06:24:22.875716: step 2292, loss 0.00430182, acc 1\n",
            "2019-01-22T06:24:23.559116: step 2293, loss 0.000297992, acc 1\n",
            "2019-01-22T06:24:24.250190: step 2294, loss 0.000689954, acc 1\n",
            "2019-01-22T06:24:24.928638: step 2295, loss 0.0749833, acc 0.984375\n",
            "2019-01-22T06:24:25.609676: step 2296, loss 0.000175996, acc 1\n",
            "2019-01-22T06:24:26.294714: step 2297, loss 0.000770534, acc 1\n",
            "2019-01-22T06:24:26.978640: step 2298, loss 0.000685181, acc 1\n",
            "2019-01-22T06:24:27.656190: step 2299, loss 0.0802257, acc 0.984375\n",
            "2019-01-22T06:24:28.342039: step 2300, loss 0.003791, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:24:28.699924: step 2300, loss 0.000172027, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2300\n",
            "\n",
            "2019-01-22T06:24:29.517338: step 2301, loss 0.0615354, acc 0.984375\n",
            "2019-01-22T06:24:30.202050: step 2302, loss 0.00379164, acc 1\n",
            "2019-01-22T06:24:30.894460: step 2303, loss 0.00657941, acc 1\n",
            "2019-01-22T06:24:31.574386: step 2304, loss 0.00069688, acc 1\n",
            "2019-01-22T06:24:32.256039: step 2305, loss 2.20836e-05, acc 1\n",
            "2019-01-22T06:24:32.941687: step 2306, loss 0.00330399, acc 1\n",
            "2019-01-22T06:24:33.621930: step 2307, loss 0.000483283, acc 1\n",
            "2019-01-22T06:24:34.310966: step 2308, loss 3.50341e-05, acc 1\n",
            "2019-01-22T06:24:34.992988: step 2309, loss 0.0116842, acc 1\n",
            "2019-01-22T06:24:35.404313: step 2310, loss 0.000941208, acc 1\n",
            "2019-01-22T06:24:36.085641: step 2311, loss 0.000287228, acc 1\n",
            "2019-01-22T06:24:36.765465: step 2312, loss 0.00214506, acc 1\n",
            "2019-01-22T06:24:37.453835: step 2313, loss 3.75554e-05, acc 1\n",
            "2019-01-22T06:24:38.131721: step 2314, loss 0.000510262, acc 1\n",
            "2019-01-22T06:24:38.812540: step 2315, loss 0.000298885, acc 1\n",
            "2019-01-22T06:24:39.492921: step 2316, loss 0.000935766, acc 1\n",
            "2019-01-22T06:24:40.171120: step 2317, loss 0.143363, acc 0.984375\n",
            "2019-01-22T06:24:40.849570: step 2318, loss 0.000144999, acc 1\n",
            "2019-01-22T06:24:41.533029: step 2319, loss 0.000485157, acc 1\n",
            "2019-01-22T06:24:42.212330: step 2320, loss 0.0011805, acc 1\n",
            "2019-01-22T06:24:42.890419: step 2321, loss 0.0002909, acc 1\n",
            "2019-01-22T06:24:43.575407: step 2322, loss 0.0222, acc 0.984375\n",
            "2019-01-22T06:24:44.254374: step 2323, loss 2.43651e-05, acc 1\n",
            "2019-01-22T06:24:44.940827: step 2324, loss 0.000106074, acc 1\n",
            "2019-01-22T06:24:45.624005: step 2325, loss 0.000136885, acc 1\n",
            "2019-01-22T06:24:46.309126: step 2326, loss 2.90097e-05, acc 1\n",
            "2019-01-22T06:24:46.990719: step 2327, loss 2.77104e-05, acc 1\n",
            "2019-01-22T06:24:47.670730: step 2328, loss 0.000401049, acc 1\n",
            "2019-01-22T06:24:48.356261: step 2329, loss 0.000118471, acc 1\n",
            "2019-01-22T06:24:49.036536: step 2330, loss 0.00358071, acc 1\n",
            "2019-01-22T06:24:49.716405: step 2331, loss 0.0384025, acc 0.984375\n",
            "2019-01-22T06:24:50.395381: step 2332, loss 0.000837205, acc 1\n",
            "2019-01-22T06:24:51.077136: step 2333, loss 0.00217317, acc 1\n",
            "2019-01-22T06:24:51.754730: step 2334, loss 0.000875748, acc 1\n",
            "2019-01-22T06:24:52.440178: step 2335, loss 0.029465, acc 0.984375\n",
            "2019-01-22T06:24:53.122699: step 2336, loss 0.00545275, acc 1\n",
            "2019-01-22T06:24:53.801242: step 2337, loss 0.0038059, acc 1\n",
            "2019-01-22T06:24:54.487826: step 2338, loss 0.000796773, acc 1\n",
            "2019-01-22T06:24:55.179112: step 2339, loss 0.00228654, acc 1\n",
            "2019-01-22T06:24:55.857523: step 2340, loss 0.000123451, acc 1\n",
            "2019-01-22T06:24:56.538663: step 2341, loss 0.000785933, acc 1\n",
            "2019-01-22T06:24:57.224640: step 2342, loss 0.0105295, acc 1\n",
            "2019-01-22T06:24:57.636125: step 2343, loss 2.18598e-05, acc 1\n",
            "2019-01-22T06:24:58.315577: step 2344, loss 0.00206406, acc 1\n",
            "2019-01-22T06:24:58.999401: step 2345, loss 0.0014767, acc 1\n",
            "2019-01-22T06:24:59.677083: step 2346, loss 0.000204634, acc 1\n",
            "2019-01-22T06:25:00.360995: step 2347, loss 0.00885908, acc 1\n",
            "2019-01-22T06:25:01.040247: step 2348, loss 3.64074e-05, acc 1\n",
            "2019-01-22T06:25:01.724604: step 2349, loss 0.00230905, acc 1\n",
            "2019-01-22T06:25:02.406495: step 2350, loss 6.29473e-05, acc 1\n",
            "2019-01-22T06:25:03.084572: step 2351, loss 0.00867937, acc 1\n",
            "2019-01-22T06:25:03.764676: step 2352, loss 0.00172169, acc 1\n",
            "2019-01-22T06:25:04.451642: step 2353, loss 0.000934879, acc 1\n",
            "2019-01-22T06:25:05.137068: step 2354, loss 0.00488179, acc 1\n",
            "2019-01-22T06:25:05.815434: step 2355, loss 0.0168915, acc 0.984375\n",
            "2019-01-22T06:25:06.503924: step 2356, loss 0.00137324, acc 1\n",
            "2019-01-22T06:25:07.182475: step 2357, loss 0.000126327, acc 1\n",
            "2019-01-22T06:25:07.863350: step 2358, loss 0.000153852, acc 1\n",
            "2019-01-22T06:25:08.558604: step 2359, loss 0.0498857, acc 0.984375\n",
            "2019-01-22T06:25:09.237069: step 2360, loss 0.000149724, acc 1\n",
            "2019-01-22T06:25:09.914089: step 2361, loss 8.95863e-05, acc 1\n",
            "2019-01-22T06:25:10.603106: step 2362, loss 0.00136244, acc 1\n",
            "2019-01-22T06:25:11.281842: step 2363, loss 0.015511, acc 1\n",
            "2019-01-22T06:25:11.968133: step 2364, loss 0.00131731, acc 1\n",
            "2019-01-22T06:25:12.656463: step 2365, loss 0.102666, acc 0.984375\n",
            "2019-01-22T06:25:13.336755: step 2366, loss 0.0018242, acc 1\n",
            "2019-01-22T06:25:14.016730: step 2367, loss 0.000217243, acc 1\n",
            "2019-01-22T06:25:14.703867: step 2368, loss 0.000742351, acc 1\n",
            "2019-01-22T06:25:15.386700: step 2369, loss 0.00194717, acc 1\n",
            "2019-01-22T06:25:16.074194: step 2370, loss 0.00192749, acc 1\n",
            "2019-01-22T06:25:16.766263: step 2371, loss 0.0447111, acc 0.984375\n",
            "2019-01-22T06:25:17.451096: step 2372, loss 1.68081e-05, acc 1\n",
            "2019-01-22T06:25:18.132738: step 2373, loss 0.00307899, acc 1\n",
            "2019-01-22T06:25:18.821890: step 2374, loss 0.000243421, acc 1\n",
            "2019-01-22T06:25:19.501622: step 2375, loss 0.0530545, acc 0.984375\n",
            "2019-01-22T06:25:19.918008: step 2376, loss 0.00475768, acc 1\n",
            "2019-01-22T06:25:20.597127: step 2377, loss 6.81707e-05, acc 1\n",
            "2019-01-22T06:25:21.278563: step 2378, loss 0.000970932, acc 1\n",
            "2019-01-22T06:25:21.957502: step 2379, loss 0.0184692, acc 0.984375\n",
            "2019-01-22T06:25:22.639294: step 2380, loss 0.000701866, acc 1\n",
            "2019-01-22T06:25:23.319402: step 2381, loss 0.000746933, acc 1\n",
            "2019-01-22T06:25:24.003008: step 2382, loss 5.22764e-05, acc 1\n",
            "2019-01-22T06:25:24.685001: step 2383, loss 5.1378e-05, acc 1\n",
            "2019-01-22T06:25:25.362674: step 2384, loss 2.62941e-05, acc 1\n",
            "2019-01-22T06:25:26.047544: step 2385, loss 0.000562074, acc 1\n",
            "2019-01-22T06:25:26.727703: step 2386, loss 7.20565e-05, acc 1\n",
            "2019-01-22T06:25:27.407500: step 2387, loss 0.0214406, acc 0.984375\n",
            "2019-01-22T06:25:28.094018: step 2388, loss 0.110072, acc 0.984375\n",
            "2019-01-22T06:25:28.774002: step 2389, loss 0.0019238, acc 1\n",
            "2019-01-22T06:25:29.452241: step 2390, loss 3.80393e-05, acc 1\n",
            "2019-01-22T06:25:30.141980: step 2391, loss 0.00130763, acc 1\n",
            "2019-01-22T06:25:30.821509: step 2392, loss 0.000100648, acc 1\n",
            "2019-01-22T06:25:31.499925: step 2393, loss 0.00542094, acc 1\n",
            "2019-01-22T06:25:32.189927: step 2394, loss 0.049815, acc 0.984375\n",
            "2019-01-22T06:25:32.867720: step 2395, loss 0.000782988, acc 1\n",
            "2019-01-22T06:25:33.546841: step 2396, loss 0.000595927, acc 1\n",
            "2019-01-22T06:25:34.230257: step 2397, loss 0.102571, acc 0.96875\n",
            "2019-01-22T06:25:34.906876: step 2398, loss 0.00127111, acc 1\n",
            "2019-01-22T06:25:35.587060: step 2399, loss 0.00272206, acc 1\n",
            "2019-01-22T06:25:36.275475: step 2400, loss 0.00126683, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:25:36.634543: step 2400, loss 0.000135784, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2400\n",
            "\n",
            "2019-01-22T06:25:37.468321: step 2401, loss 0.0347541, acc 0.984375\n",
            "2019-01-22T06:25:38.144647: step 2402, loss 0.000447046, acc 1\n",
            "2019-01-22T06:25:38.826257: step 2403, loss 6.00766e-05, acc 1\n",
            "2019-01-22T06:25:39.506818: step 2404, loss 0.000756653, acc 1\n",
            "2019-01-22T06:25:40.189459: step 2405, loss 0.00233568, acc 1\n",
            "2019-01-22T06:25:40.877893: step 2406, loss 0.000147107, acc 1\n",
            "2019-01-22T06:25:41.557658: step 2407, loss 0.0180501, acc 0.984375\n",
            "2019-01-22T06:25:42.240691: step 2408, loss 0.000107354, acc 1\n",
            "2019-01-22T06:25:42.651441: step 2409, loss 9.06856e-05, acc 1\n",
            "2019-01-22T06:25:43.334093: step 2410, loss 0.0322653, acc 0.984375\n",
            "2019-01-22T06:25:44.015282: step 2411, loss 0.000101586, acc 1\n",
            "2019-01-22T06:25:44.699731: step 2412, loss 0.00248325, acc 1\n",
            "2019-01-22T06:25:45.383521: step 2413, loss 0.000932962, acc 1\n",
            "2019-01-22T06:25:46.068270: step 2414, loss 0.00439239, acc 1\n",
            "2019-01-22T06:25:46.748675: step 2415, loss 0.000464078, acc 1\n",
            "2019-01-22T06:25:47.427930: step 2416, loss 0.00461674, acc 1\n",
            "2019-01-22T06:25:48.111891: step 2417, loss 0.00718865, acc 1\n",
            "2019-01-22T06:25:48.791030: step 2418, loss 0.00139075, acc 1\n",
            "2019-01-22T06:25:49.475402: step 2419, loss 0.0892412, acc 0.984375\n",
            "2019-01-22T06:25:50.160098: step 2420, loss 0.000155818, acc 1\n",
            "2019-01-22T06:25:50.840320: step 2421, loss 0.000233852, acc 1\n",
            "2019-01-22T06:25:51.522644: step 2422, loss 0.0166507, acc 0.984375\n",
            "2019-01-22T06:25:52.199703: step 2423, loss 7.80261e-05, acc 1\n",
            "2019-01-22T06:25:52.882900: step 2424, loss 0.00820638, acc 1\n",
            "2019-01-22T06:25:53.565135: step 2425, loss 7.07944e-05, acc 1\n",
            "2019-01-22T06:25:54.243389: step 2426, loss 1.99686e-05, acc 1\n",
            "2019-01-22T06:25:54.924419: step 2427, loss 0.00353343, acc 1\n",
            "2019-01-22T06:25:55.603708: step 2428, loss 0.000406483, acc 1\n",
            "2019-01-22T06:25:56.284133: step 2429, loss 0.000245328, acc 1\n",
            "2019-01-22T06:25:56.967178: step 2430, loss 0.0305468, acc 0.984375\n",
            "2019-01-22T06:25:57.646845: step 2431, loss 0.000142052, acc 1\n",
            "2019-01-22T06:25:58.325712: step 2432, loss 0.000419605, acc 1\n",
            "2019-01-22T06:25:59.014717: step 2433, loss 0.00276848, acc 1\n",
            "2019-01-22T06:25:59.696592: step 2434, loss 0.00112385, acc 1\n",
            "2019-01-22T06:26:00.375445: step 2435, loss 2.38666e-05, acc 1\n",
            "2019-01-22T06:26:01.065464: step 2436, loss 0.000208786, acc 1\n",
            "2019-01-22T06:26:01.743614: step 2437, loss 5.76172e-05, acc 1\n",
            "2019-01-22T06:26:02.422828: step 2438, loss 0.000177353, acc 1\n",
            "2019-01-22T06:26:03.111492: step 2439, loss 0.000313864, acc 1\n",
            "2019-01-22T06:26:03.795524: step 2440, loss 0.000572582, acc 1\n",
            "2019-01-22T06:26:04.481791: step 2441, loss 0.00569737, acc 1\n",
            "2019-01-22T06:26:04.898635: step 2442, loss 0.000169092, acc 1\n",
            "2019-01-22T06:26:05.579795: step 2443, loss 7.86666e-05, acc 1\n",
            "2019-01-22T06:26:06.261543: step 2444, loss 0.000406574, acc 1\n",
            "2019-01-22T06:26:06.940237: step 2445, loss 1.62934e-05, acc 1\n",
            "2019-01-22T06:26:07.620665: step 2446, loss 0.00547803, acc 1\n",
            "2019-01-22T06:26:08.305459: step 2447, loss 0.000163012, acc 1\n",
            "2019-01-22T06:26:08.986647: step 2448, loss 0.00647949, acc 1\n",
            "2019-01-22T06:26:09.667503: step 2449, loss 0.000231826, acc 1\n",
            "2019-01-22T06:26:10.353084: step 2450, loss 0.00304642, acc 1\n",
            "2019-01-22T06:26:11.035028: step 2451, loss 0.034343, acc 0.984375\n",
            "2019-01-22T06:26:11.715115: step 2452, loss 0.000677415, acc 1\n",
            "2019-01-22T06:26:12.401779: step 2453, loss 5.17942e-05, acc 1\n",
            "2019-01-22T06:26:13.079425: step 2454, loss 0.000298063, acc 1\n",
            "2019-01-22T06:26:13.763213: step 2455, loss 1.31048e-05, acc 1\n",
            "2019-01-22T06:26:14.453825: step 2456, loss 0.000110747, acc 1\n",
            "2019-01-22T06:26:15.134357: step 2457, loss 0.000119675, acc 1\n",
            "2019-01-22T06:26:15.811840: step 2458, loss 0.000670013, acc 1\n",
            "2019-01-22T06:26:16.506732: step 2459, loss 0.0589432, acc 0.984375\n",
            "2019-01-22T06:26:17.183964: step 2460, loss 0.00033197, acc 1\n",
            "2019-01-22T06:26:17.861070: step 2461, loss 0.000218591, acc 1\n",
            "2019-01-22T06:26:18.554901: step 2462, loss 0.00220299, acc 1\n",
            "2019-01-22T06:26:19.239628: step 2463, loss 3.03652e-05, acc 1\n",
            "2019-01-22T06:26:19.914565: step 2464, loss 6.38251e-05, acc 1\n",
            "2019-01-22T06:26:20.602496: step 2465, loss 0.000787014, acc 1\n",
            "2019-01-22T06:26:21.279455: step 2466, loss 0.00017487, acc 1\n",
            "2019-01-22T06:26:21.959114: step 2467, loss 5.03643e-06, acc 1\n",
            "2019-01-22T06:26:22.649251: step 2468, loss 0.0186072, acc 0.984375\n",
            "2019-01-22T06:26:23.333924: step 2469, loss 1.8495e-05, acc 1\n",
            "2019-01-22T06:26:24.018576: step 2470, loss 0.000478654, acc 1\n",
            "2019-01-22T06:26:24.707085: step 2471, loss 0.00453886, acc 1\n",
            "2019-01-22T06:26:25.386583: step 2472, loss 2.5709e-05, acc 1\n",
            "2019-01-22T06:26:26.070458: step 2473, loss 0.00046041, acc 1\n",
            "2019-01-22T06:26:26.758805: step 2474, loss 0.00735026, acc 1\n",
            "2019-01-22T06:26:27.174308: step 2475, loss 0.00251376, acc 1\n",
            "2019-01-22T06:26:27.853767: step 2476, loss 0.000109663, acc 1\n",
            "2019-01-22T06:26:28.536849: step 2477, loss 0.000310659, acc 1\n",
            "2019-01-22T06:26:29.218715: step 2478, loss 0.00156619, acc 1\n",
            "2019-01-22T06:26:29.901896: step 2479, loss 0.0023094, acc 1\n",
            "2019-01-22T06:26:30.585786: step 2480, loss 8.77942e-05, acc 1\n",
            "2019-01-22T06:26:31.264834: step 2481, loss 3.73295e-05, acc 1\n",
            "2019-01-22T06:26:31.951837: step 2482, loss 0.000417112, acc 1\n",
            "2019-01-22T06:26:32.634430: step 2483, loss 2.98811e-05, acc 1\n",
            "2019-01-22T06:26:33.314092: step 2484, loss 0.000477375, acc 1\n",
            "2019-01-22T06:26:34.006782: step 2485, loss 0.000386083, acc 1\n",
            "2019-01-22T06:26:34.688841: step 2486, loss 0.0782466, acc 0.984375\n",
            "2019-01-22T06:26:35.370873: step 2487, loss 0.00218418, acc 1\n",
            "2019-01-22T06:26:36.059826: step 2488, loss 0.000240733, acc 1\n",
            "2019-01-22T06:26:36.739313: step 2489, loss 0.0403023, acc 0.984375\n",
            "2019-01-22T06:26:37.420733: step 2490, loss 2.87651e-05, acc 1\n",
            "2019-01-22T06:26:38.107993: step 2491, loss 0.000212927, acc 1\n",
            "2019-01-22T06:26:38.787965: step 2492, loss 0.0051976, acc 1\n",
            "2019-01-22T06:26:39.470248: step 2493, loss 0.00724039, acc 1\n",
            "2019-01-22T06:26:40.156681: step 2494, loss 2.54888e-05, acc 1\n",
            "2019-01-22T06:26:40.836852: step 2495, loss 0.00590559, acc 1\n",
            "2019-01-22T06:26:41.511768: step 2496, loss 0.000331997, acc 1\n",
            "2019-01-22T06:26:42.198462: step 2497, loss 0.00871916, acc 1\n",
            "2019-01-22T06:26:42.877674: step 2498, loss 0.000339226, acc 1\n",
            "2019-01-22T06:26:43.554996: step 2499, loss 0.0153418, acc 0.984375\n",
            "2019-01-22T06:26:44.240281: step 2500, loss 4.53687e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:26:44.603502: step 2500, loss 0.000428938, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2500\n",
            "\n",
            "2019-01-22T06:26:45.439138: step 2501, loss 0.000576511, acc 1\n",
            "2019-01-22T06:26:46.115719: step 2502, loss 0.00452421, acc 1\n",
            "2019-01-22T06:26:46.796909: step 2503, loss 0.00015963, acc 1\n",
            "2019-01-22T06:26:47.476970: step 2504, loss 0.000632538, acc 1\n",
            "2019-01-22T06:26:48.158880: step 2505, loss 0.00254681, acc 1\n",
            "2019-01-22T06:26:48.839458: step 2506, loss 0.00536616, acc 1\n",
            "2019-01-22T06:26:49.516324: step 2507, loss 0.00568648, acc 1\n",
            "2019-01-22T06:26:49.927290: step 2508, loss 0.000145824, acc 1\n",
            "2019-01-22T06:26:50.606440: step 2509, loss 0.000207153, acc 1\n",
            "2019-01-22T06:26:51.284319: step 2510, loss 0.00147152, acc 1\n",
            "2019-01-22T06:26:51.960891: step 2511, loss 0.000384449, acc 1\n",
            "2019-01-22T06:26:52.640741: step 2512, loss 0.00115815, acc 1\n",
            "2019-01-22T06:26:53.321618: step 2513, loss 0.0863734, acc 0.984375\n",
            "2019-01-22T06:26:54.002130: step 2514, loss 0.00311548, acc 1\n",
            "2019-01-22T06:26:54.685681: step 2515, loss 0.00063371, acc 1\n",
            "2019-01-22T06:26:55.364536: step 2516, loss 0.0177789, acc 0.984375\n",
            "2019-01-22T06:26:56.043508: step 2517, loss 0.00110573, acc 1\n",
            "2019-01-22T06:26:56.724822: step 2518, loss 0.000275882, acc 1\n",
            "2019-01-22T06:26:57.405064: step 2519, loss 0.0322261, acc 0.984375\n",
            "2019-01-22T06:26:58.082699: step 2520, loss 3.03493e-05, acc 1\n",
            "2019-01-22T06:26:58.764334: step 2521, loss 0.00633454, acc 1\n",
            "2019-01-22T06:26:59.445529: step 2522, loss 0.00111731, acc 1\n",
            "2019-01-22T06:27:00.123124: step 2523, loss 6.721e-05, acc 1\n",
            "2019-01-22T06:27:00.802526: step 2524, loss 2.29264e-05, acc 1\n",
            "2019-01-22T06:27:01.482030: step 2525, loss 8.10953e-06, acc 1\n",
            "2019-01-22T06:27:02.157936: step 2526, loss 6.65916e-05, acc 1\n",
            "2019-01-22T06:27:02.842101: step 2527, loss 0.0260704, acc 0.984375\n",
            "2019-01-22T06:27:03.522278: step 2528, loss 0.000788962, acc 1\n",
            "2019-01-22T06:27:04.202002: step 2529, loss 0.000556384, acc 1\n",
            "2019-01-22T06:27:04.886832: step 2530, loss 0.00158812, acc 1\n",
            "2019-01-22T06:27:05.570783: step 2531, loss 0.000461985, acc 1\n",
            "2019-01-22T06:27:06.248319: step 2532, loss 4.04673e-05, acc 1\n",
            "2019-01-22T06:27:06.932545: step 2533, loss 8.75119e-05, acc 1\n",
            "2019-01-22T06:27:07.615367: step 2534, loss 0.000526331, acc 1\n",
            "2019-01-22T06:27:08.294056: step 2535, loss 1.17146e-05, acc 1\n",
            "2019-01-22T06:27:08.982419: step 2536, loss 0.00158245, acc 1\n",
            "2019-01-22T06:27:09.662959: step 2537, loss 7.5955e-06, acc 1\n",
            "2019-01-22T06:27:10.343812: step 2538, loss 0.000298453, acc 1\n",
            "2019-01-22T06:27:11.028906: step 2539, loss 0.000615525, acc 1\n",
            "2019-01-22T06:27:11.707344: step 2540, loss 5.80453e-05, acc 1\n",
            "2019-01-22T06:27:12.121073: step 2541, loss 0.000261454, acc 1\n",
            "2019-01-22T06:27:12.801334: step 2542, loss 0.00176501, acc 1\n",
            "2019-01-22T06:27:13.481534: step 2543, loss 0.0551967, acc 0.984375\n",
            "2019-01-22T06:27:14.159581: step 2544, loss 0.00109474, acc 1\n",
            "2019-01-22T06:27:14.840959: step 2545, loss 0.00160188, acc 1\n",
            "2019-01-22T06:27:15.521010: step 2546, loss 5.92392e-05, acc 1\n",
            "2019-01-22T06:27:16.201745: step 2547, loss 0.00116232, acc 1\n",
            "2019-01-22T06:27:16.882432: step 2548, loss 0.005811, acc 1\n",
            "2019-01-22T06:27:17.559394: step 2549, loss 0.00350493, acc 1\n",
            "2019-01-22T06:27:18.251621: step 2550, loss 0.0712006, acc 0.984375\n",
            "2019-01-22T06:27:18.928074: step 2551, loss 0.000777389, acc 1\n",
            "2019-01-22T06:27:19.607451: step 2552, loss 0.000570254, acc 1\n",
            "2019-01-22T06:27:20.297861: step 2553, loss 0.00055599, acc 1\n",
            "2019-01-22T06:27:20.975668: step 2554, loss 0.00092593, acc 1\n",
            "2019-01-22T06:27:21.650304: step 2555, loss 0.000140668, acc 1\n",
            "2019-01-22T06:27:22.335909: step 2556, loss 7.46369e-05, acc 1\n",
            "2019-01-22T06:27:23.020850: step 2557, loss 0.0313833, acc 0.984375\n",
            "2019-01-22T06:27:23.698354: step 2558, loss 0.000149202, acc 1\n",
            "2019-01-22T06:27:24.383743: step 2559, loss 0.000255758, acc 1\n",
            "2019-01-22T06:27:25.069047: step 2560, loss 3.74099e-05, acc 1\n",
            "2019-01-22T06:27:25.751264: step 2561, loss 0.0092689, acc 1\n",
            "2019-01-22T06:27:26.440408: step 2562, loss 0.000718223, acc 1\n",
            "2019-01-22T06:27:27.124333: step 2563, loss 0.000211307, acc 1\n",
            "2019-01-22T06:27:27.804445: step 2564, loss 0.00219336, acc 1\n",
            "2019-01-22T06:27:28.493637: step 2565, loss 0.000563336, acc 1\n",
            "2019-01-22T06:27:29.174367: step 2566, loss 0.00043971, acc 1\n",
            "2019-01-22T06:27:29.854761: step 2567, loss 8.23154e-05, acc 1\n",
            "2019-01-22T06:27:30.548699: step 2568, loss 0.00011864, acc 1\n",
            "2019-01-22T06:27:31.231696: step 2569, loss 0.00202198, acc 1\n",
            "2019-01-22T06:27:31.912858: step 2570, loss 0.00270291, acc 1\n",
            "2019-01-22T06:27:32.604391: step 2571, loss 0.0143882, acc 0.984375\n",
            "2019-01-22T06:27:33.288961: step 2572, loss 0.000225182, acc 1\n",
            "2019-01-22T06:27:33.971649: step 2573, loss 0.00549253, acc 1\n",
            "2019-01-22T06:27:34.389829: step 2574, loss 8.0008e-05, acc 1\n",
            "2019-01-22T06:27:35.071363: step 2575, loss 0.000429855, acc 1\n",
            "2019-01-22T06:27:35.756295: step 2576, loss 0.00017831, acc 1\n",
            "2019-01-22T06:27:36.440456: step 2577, loss 0.0189275, acc 0.984375\n",
            "2019-01-22T06:27:37.121378: step 2578, loss 0.0094378, acc 1\n",
            "2019-01-22T06:27:37.804094: step 2579, loss 7.56483e-05, acc 1\n",
            "2019-01-22T06:27:38.486716: step 2580, loss 9.19385e-05, acc 1\n",
            "2019-01-22T06:27:39.165744: step 2581, loss 5.49277e-06, acc 1\n",
            "2019-01-22T06:27:39.846934: step 2582, loss 0.00280792, acc 1\n",
            "2019-01-22T06:27:40.527637: step 2583, loss 0.000176772, acc 1\n",
            "2019-01-22T06:27:41.206566: step 2584, loss 4.80941e-05, acc 1\n",
            "2019-01-22T06:27:41.897863: step 2585, loss 0.000497438, acc 1\n",
            "2019-01-22T06:27:42.580479: step 2586, loss 0.00423629, acc 1\n",
            "2019-01-22T06:27:43.259500: step 2587, loss 9.23718e-05, acc 1\n",
            "2019-01-22T06:27:43.949617: step 2588, loss 0.000965523, acc 1\n",
            "2019-01-22T06:27:44.627685: step 2589, loss 0.00236259, acc 1\n",
            "2019-01-22T06:27:45.311224: step 2590, loss 0.00103861, acc 1\n",
            "2019-01-22T06:27:45.996411: step 2591, loss 0.000130809, acc 1\n",
            "2019-01-22T06:27:46.678247: step 2592, loss 0.000584555, acc 1\n",
            "2019-01-22T06:27:47.353636: step 2593, loss 9.59701e-06, acc 1\n",
            "2019-01-22T06:27:48.038491: step 2594, loss 0.000342537, acc 1\n",
            "2019-01-22T06:27:48.721685: step 2595, loss 0.000909575, acc 1\n",
            "2019-01-22T06:27:49.400899: step 2596, loss 0.000607242, acc 1\n",
            "2019-01-22T06:27:50.086150: step 2597, loss 0.0191309, acc 0.984375\n",
            "2019-01-22T06:27:50.766519: step 2598, loss 1.51564e-05, acc 1\n",
            "2019-01-22T06:27:51.445981: step 2599, loss 0.00219249, acc 1\n",
            "2019-01-22T06:27:52.136428: step 2600, loss 0.0175107, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:27:52.497228: step 2600, loss 0.00010954, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2600\n",
            "\n",
            "2019-01-22T06:27:53.343853: step 2601, loss 0.0044798, acc 1\n",
            "2019-01-22T06:27:54.024224: step 2602, loss 6.02258e-05, acc 1\n",
            "2019-01-22T06:27:54.706279: step 2603, loss 0.00155491, acc 1\n",
            "2019-01-22T06:27:55.388571: step 2604, loss 0.00138535, acc 1\n",
            "2019-01-22T06:27:56.070422: step 2605, loss 0.000617416, acc 1\n",
            "2019-01-22T06:27:56.748844: step 2606, loss 0.000588693, acc 1\n",
            "2019-01-22T06:27:57.166640: step 2607, loss 3.31462e-05, acc 1\n",
            "2019-01-22T06:27:57.850361: step 2608, loss 0.000905153, acc 1\n",
            "2019-01-22T06:27:58.529405: step 2609, loss 6.76552e-05, acc 1\n",
            "2019-01-22T06:27:59.210053: step 2610, loss 0.00824709, acc 1\n",
            "2019-01-22T06:27:59.889063: step 2611, loss 0.0122254, acc 0.984375\n",
            "2019-01-22T06:28:00.572797: step 2612, loss 0.000200495, acc 1\n",
            "2019-01-22T06:28:01.253278: step 2613, loss 0.00411972, acc 1\n",
            "2019-01-22T06:28:01.934544: step 2614, loss 0.000760103, acc 1\n",
            "2019-01-22T06:28:02.612951: step 2615, loss 0.000406991, acc 1\n",
            "2019-01-22T06:28:03.293735: step 2616, loss 0.000314018, acc 1\n",
            "2019-01-22T06:28:03.975716: step 2617, loss 3.56001e-05, acc 1\n",
            "2019-01-22T06:28:04.655061: step 2618, loss 0.00025819, acc 1\n",
            "2019-01-22T06:28:05.333982: step 2619, loss 4.72517e-05, acc 1\n",
            "2019-01-22T06:28:06.017976: step 2620, loss 0.000483025, acc 1\n",
            "2019-01-22T06:28:06.700921: step 2621, loss 0.00195198, acc 1\n",
            "2019-01-22T06:28:07.379925: step 2622, loss 0.000249915, acc 1\n",
            "2019-01-22T06:28:08.057512: step 2623, loss 0.00125068, acc 1\n",
            "2019-01-22T06:28:08.737786: step 2624, loss 0.000169097, acc 1\n",
            "2019-01-22T06:28:09.422995: step 2625, loss 0.00199154, acc 1\n",
            "2019-01-22T06:28:10.101038: step 2626, loss 2.5811e-05, acc 1\n",
            "2019-01-22T06:28:10.777760: step 2627, loss 0.00354815, acc 1\n",
            "2019-01-22T06:28:11.460709: step 2628, loss 0.000263178, acc 1\n",
            "2019-01-22T06:28:12.138019: step 2629, loss 2.62664e-05, acc 1\n",
            "2019-01-22T06:28:12.820954: step 2630, loss 0.000233721, acc 1\n",
            "2019-01-22T06:28:13.503766: step 2631, loss 4.63782e-06, acc 1\n",
            "2019-01-22T06:28:14.181116: step 2632, loss 0.00308066, acc 1\n",
            "2019-01-22T06:28:14.860235: step 2633, loss 0.000254807, acc 1\n",
            "2019-01-22T06:28:15.545408: step 2634, loss 2.39832e-05, acc 1\n",
            "2019-01-22T06:28:16.230311: step 2635, loss 2.04489e-05, acc 1\n",
            "2019-01-22T06:28:16.910603: step 2636, loss 0.00090958, acc 1\n",
            "2019-01-22T06:28:17.588106: step 2637, loss 1.23901e-05, acc 1\n",
            "2019-01-22T06:28:18.272113: step 2638, loss 0.0030053, acc 1\n",
            "2019-01-22T06:28:18.961094: step 2639, loss 0.0644154, acc 0.96875\n",
            "2019-01-22T06:28:19.373351: step 2640, loss 0.00223886, acc 1\n",
            "2019-01-22T06:28:20.053053: step 2641, loss 0.000396572, acc 1\n",
            "2019-01-22T06:28:20.731989: step 2642, loss 0.0607028, acc 0.984375\n",
            "2019-01-22T06:28:21.412576: step 2643, loss 4.7376e-05, acc 1\n",
            "2019-01-22T06:28:22.089517: step 2644, loss 0.000145854, acc 1\n",
            "2019-01-22T06:28:22.771249: step 2645, loss 0.00376335, acc 1\n",
            "2019-01-22T06:28:23.453109: step 2646, loss 0.00452076, acc 1\n",
            "2019-01-22T06:28:24.132462: step 2647, loss 1.93009e-05, acc 1\n",
            "2019-01-22T06:28:24.811065: step 2648, loss 0.00227414, acc 1\n",
            "2019-01-22T06:28:25.490695: step 2649, loss 0.00439947, acc 1\n",
            "2019-01-22T06:28:26.175904: step 2650, loss 0.00466749, acc 1\n",
            "2019-01-22T06:28:26.855638: step 2651, loss 0.00259711, acc 1\n",
            "2019-01-22T06:28:27.535820: step 2652, loss 0.103927, acc 0.984375\n",
            "2019-01-22T06:28:28.223759: step 2653, loss 0.00575078, acc 1\n",
            "2019-01-22T06:28:28.902513: step 2654, loss 9.18977e-06, acc 1\n",
            "2019-01-22T06:28:29.579529: step 2655, loss 0.000134692, acc 1\n",
            "2019-01-22T06:28:30.270331: step 2656, loss 3.85903e-05, acc 1\n",
            "2019-01-22T06:28:30.951595: step 2657, loss 0.0218345, acc 0.984375\n",
            "2019-01-22T06:28:31.628656: step 2658, loss 0.00171744, acc 1\n",
            "2019-01-22T06:28:32.315914: step 2659, loss 3.42809e-05, acc 1\n",
            "2019-01-22T06:28:32.997990: step 2660, loss 0.00031212, acc 1\n",
            "2019-01-22T06:28:33.678241: step 2661, loss 0.000669159, acc 1\n",
            "2019-01-22T06:28:34.366083: step 2662, loss 0.00364705, acc 1\n",
            "2019-01-22T06:28:35.043771: step 2663, loss 1.16582e-05, acc 1\n",
            "2019-01-22T06:28:35.726742: step 2664, loss 0.000150988, acc 1\n",
            "2019-01-22T06:28:36.412503: step 2665, loss 4.14957e-05, acc 1\n",
            "2019-01-22T06:28:37.092205: step 2666, loss 0.000770802, acc 1\n",
            "2019-01-22T06:28:37.775104: step 2667, loss 0.000868612, acc 1\n",
            "2019-01-22T06:28:38.462054: step 2668, loss 0.00257267, acc 1\n",
            "2019-01-22T06:28:39.144277: step 2669, loss 0.0104316, acc 1\n",
            "2019-01-22T06:28:39.821031: step 2670, loss 5.69772e-05, acc 1\n",
            "2019-01-22T06:28:40.503948: step 2671, loss 9.70944e-06, acc 1\n",
            "2019-01-22T06:28:41.183342: step 2672, loss 6.25188e-05, acc 1\n",
            "2019-01-22T06:28:41.595146: step 2673, loss 4.13434e-05, acc 1\n",
            "2019-01-22T06:28:42.278265: step 2674, loss 0.00159098, acc 1\n",
            "2019-01-22T06:28:42.963931: step 2675, loss 0.00010242, acc 1\n",
            "2019-01-22T06:28:43.646901: step 2676, loss 0.00554508, acc 1\n",
            "2019-01-22T06:28:44.326467: step 2677, loss 0.00057836, acc 1\n",
            "2019-01-22T06:28:45.005929: step 2678, loss 0.00041808, acc 1\n",
            "2019-01-22T06:28:45.689829: step 2679, loss 0.000405583, acc 1\n",
            "2019-01-22T06:28:46.371812: step 2680, loss 0.0120412, acc 0.984375\n",
            "2019-01-22T06:28:47.051483: step 2681, loss 0.00119518, acc 1\n",
            "2019-01-22T06:28:47.734968: step 2682, loss 0.00402928, acc 1\n",
            "2019-01-22T06:28:48.410747: step 2683, loss 5.07053e-05, acc 1\n",
            "2019-01-22T06:28:49.090344: step 2684, loss 0.00010605, acc 1\n",
            "2019-01-22T06:28:49.775398: step 2685, loss 0.000276233, acc 1\n",
            "2019-01-22T06:28:50.452351: step 2686, loss 0.000176521, acc 1\n",
            "2019-01-22T06:28:51.129871: step 2687, loss 0.00142072, acc 1\n",
            "2019-01-22T06:28:51.814929: step 2688, loss 0.0142407, acc 1\n",
            "2019-01-22T06:28:52.498412: step 2689, loss 2.49055e-05, acc 1\n",
            "2019-01-22T06:28:53.178451: step 2690, loss 0.00275732, acc 1\n",
            "2019-01-22T06:28:53.864747: step 2691, loss 0.000108255, acc 1\n",
            "2019-01-22T06:28:54.543474: step 2692, loss 0.0368238, acc 0.984375\n",
            "2019-01-22T06:28:55.222424: step 2693, loss 2.39071e-05, acc 1\n",
            "2019-01-22T06:28:55.912474: step 2694, loss 0.00045661, acc 1\n",
            "2019-01-22T06:28:56.595854: step 2695, loss 0.00184123, acc 1\n",
            "2019-01-22T06:28:57.277534: step 2696, loss 4.10806e-05, acc 1\n",
            "2019-01-22T06:28:57.966004: step 2697, loss 1.87505e-05, acc 1\n",
            "2019-01-22T06:28:58.645597: step 2698, loss 0.000156713, acc 1\n",
            "2019-01-22T06:28:59.324843: step 2699, loss 0.0771917, acc 0.984375\n",
            "2019-01-22T06:29:00.011649: step 2700, loss 0.000845026, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:29:00.370061: step 2700, loss 0.000115081, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2700\n",
            "\n",
            "2019-01-22T06:29:01.204765: step 2701, loss 9.05451e-05, acc 1\n",
            "2019-01-22T06:29:01.885636: step 2702, loss 0.0160189, acc 0.984375\n",
            "2019-01-22T06:29:02.565801: step 2703, loss 0.00369986, acc 1\n",
            "2019-01-22T06:29:03.249359: step 2704, loss 0.000133876, acc 1\n",
            "2019-01-22T06:29:03.928779: step 2705, loss 0.000464422, acc 1\n",
            "2019-01-22T06:29:04.343525: step 2706, loss 1.34855e-05, acc 1\n",
            "2019-01-22T06:29:05.023459: step 2707, loss 0.000192186, acc 1\n",
            "2019-01-22T06:29:05.704675: step 2708, loss 0.00037068, acc 1\n",
            "2019-01-22T06:29:06.384984: step 2709, loss 0.000551242, acc 1\n",
            "2019-01-22T06:29:07.068264: step 2710, loss 0.000129679, acc 1\n",
            "2019-01-22T06:29:07.750440: step 2711, loss 0.0010248, acc 1\n",
            "2019-01-22T06:29:08.432509: step 2712, loss 0.0353702, acc 0.984375\n",
            "2019-01-22T06:29:09.112315: step 2713, loss 0.000404896, acc 1\n",
            "2019-01-22T06:29:09.793947: step 2714, loss 7.13936e-05, acc 1\n",
            "2019-01-22T06:29:10.477744: step 2715, loss 3.16494e-05, acc 1\n",
            "2019-01-22T06:29:11.155898: step 2716, loss 0.00529471, acc 1\n",
            "2019-01-22T06:29:11.838185: step 2717, loss 0.00034089, acc 1\n",
            "2019-01-22T06:29:12.518564: step 2718, loss 0.0368281, acc 0.984375\n",
            "2019-01-22T06:29:13.199916: step 2719, loss 0.000239203, acc 1\n",
            "2019-01-22T06:29:13.883279: step 2720, loss 2.12581e-05, acc 1\n",
            "2019-01-22T06:29:14.563732: step 2721, loss 0.0595768, acc 0.96875\n",
            "2019-01-22T06:29:15.244550: step 2722, loss 5.57891e-05, acc 1\n",
            "2019-01-22T06:29:15.922379: step 2723, loss 0.000414943, acc 1\n",
            "2019-01-22T06:29:16.601350: step 2724, loss 0.00563263, acc 1\n",
            "2019-01-22T06:29:17.281391: step 2725, loss 0.00723262, acc 1\n",
            "2019-01-22T06:29:17.965459: step 2726, loss 0.00017292, acc 1\n",
            "2019-01-22T06:29:18.642962: step 2727, loss 0.000149711, acc 1\n",
            "2019-01-22T06:29:19.324710: step 2728, loss 9.72653e-05, acc 1\n",
            "2019-01-22T06:29:20.009093: step 2729, loss 0.00111552, acc 1\n",
            "2019-01-22T06:29:20.694092: step 2730, loss 0.00249252, acc 1\n",
            "2019-01-22T06:29:21.375594: step 2731, loss 0.000338383, acc 1\n",
            "2019-01-22T06:29:22.061470: step 2732, loss 6.74696e-05, acc 1\n",
            "2019-01-22T06:29:22.746660: step 2733, loss 0.000332464, acc 1\n",
            "2019-01-22T06:29:23.432529: step 2734, loss 4.96423e-05, acc 1\n",
            "2019-01-22T06:29:24.109831: step 2735, loss 0.00041726, acc 1\n",
            "2019-01-22T06:29:24.794856: step 2736, loss 0.000126288, acc 1\n",
            "2019-01-22T06:29:25.472651: step 2737, loss 0.000101021, acc 1\n",
            "2019-01-22T06:29:26.150634: step 2738, loss 2.17852e-05, acc 1\n",
            "2019-01-22T06:29:26.566004: step 2739, loss 0.000400263, acc 1\n",
            "2019-01-22T06:29:27.245174: step 2740, loss 0.000198195, acc 1\n",
            "2019-01-22T06:29:27.921724: step 2741, loss 0.000128463, acc 1\n",
            "2019-01-22T06:29:28.603663: step 2742, loss 9.64888e-05, acc 1\n",
            "2019-01-22T06:29:29.281101: step 2743, loss 0.000676001, acc 1\n",
            "2019-01-22T06:29:29.961252: step 2744, loss 0.000469856, acc 1\n",
            "2019-01-22T06:29:30.639337: step 2745, loss 5.22084e-06, acc 1\n",
            "2019-01-22T06:29:31.319043: step 2746, loss 0.00194537, acc 1\n",
            "2019-01-22T06:29:32.012753: step 2747, loss 0.00154686, acc 1\n",
            "2019-01-22T06:29:32.694990: step 2748, loss 8.15446e-05, acc 1\n",
            "2019-01-22T06:29:33.375348: step 2749, loss 4.85174e-05, acc 1\n",
            "2019-01-22T06:29:34.059288: step 2750, loss 9.6583e-05, acc 1\n",
            "2019-01-22T06:29:34.739323: step 2751, loss 6.00232e-05, acc 1\n",
            "2019-01-22T06:29:35.419478: step 2752, loss 0.00162326, acc 1\n",
            "2019-01-22T06:29:36.105019: step 2753, loss 0.00167736, acc 1\n",
            "2019-01-22T06:29:36.791660: step 2754, loss 0.000160314, acc 1\n",
            "2019-01-22T06:29:37.474251: step 2755, loss 0.000114331, acc 1\n",
            "2019-01-22T06:29:38.161747: step 2756, loss 0.0148014, acc 0.984375\n",
            "2019-01-22T06:29:38.837583: step 2757, loss 1.35271e-05, acc 1\n",
            "2019-01-22T06:29:39.516916: step 2758, loss 0.00057003, acc 1\n",
            "2019-01-22T06:29:40.207827: step 2759, loss 9.74602e-05, acc 1\n",
            "2019-01-22T06:29:40.889560: step 2760, loss 0.004266, acc 1\n",
            "2019-01-22T06:29:41.569929: step 2761, loss 0.0346361, acc 0.984375\n",
            "2019-01-22T06:29:42.256749: step 2762, loss 0.000326252, acc 1\n",
            "2019-01-22T06:29:42.936182: step 2763, loss 0.00021032, acc 1\n",
            "2019-01-22T06:29:43.624004: step 2764, loss 0.00459498, acc 1\n",
            "2019-01-22T06:29:44.311282: step 2765, loss 0.0465182, acc 0.984375\n",
            "2019-01-22T06:29:44.991329: step 2766, loss 0.00288766, acc 1\n",
            "2019-01-22T06:29:45.668633: step 2767, loss 0.00162873, acc 1\n",
            "2019-01-22T06:29:46.357343: step 2768, loss 3.34869e-05, acc 1\n",
            "2019-01-22T06:29:47.035773: step 2769, loss 0.00311788, acc 1\n",
            "2019-01-22T06:29:47.712568: step 2770, loss 0.00740409, acc 1\n",
            "2019-01-22T06:29:48.405605: step 2771, loss 0.000983251, acc 1\n",
            "2019-01-22T06:29:48.827648: step 2772, loss 0.000454494, acc 1\n",
            "2019-01-22T06:29:49.508901: step 2773, loss 2.8644e-05, acc 1\n",
            "2019-01-22T06:29:50.186390: step 2774, loss 0.000808132, acc 1\n",
            "2019-01-22T06:29:50.865387: step 2775, loss 2.10361e-05, acc 1\n",
            "2019-01-22T06:29:51.549974: step 2776, loss 8.49543e-05, acc 1\n",
            "2019-01-22T06:29:52.231473: step 2777, loss 0.000939844, acc 1\n",
            "2019-01-22T06:29:52.909770: step 2778, loss 0.000262429, acc 1\n",
            "2019-01-22T06:29:53.604464: step 2779, loss 0.000157648, acc 1\n",
            "2019-01-22T06:29:54.285356: step 2780, loss 5.18811e-05, acc 1\n",
            "2019-01-22T06:29:54.968215: step 2781, loss 0.000308053, acc 1\n",
            "2019-01-22T06:29:55.651275: step 2782, loss 0.000329904, acc 1\n",
            "2019-01-22T06:29:56.328252: step 2783, loss 0.000368851, acc 1\n",
            "2019-01-22T06:29:57.010810: step 2784, loss 1.45252e-05, acc 1\n",
            "2019-01-22T06:29:57.695367: step 2785, loss 3.62176e-05, acc 1\n",
            "2019-01-22T06:29:58.382291: step 2786, loss 0.00108159, acc 1\n",
            "2019-01-22T06:29:59.064409: step 2787, loss 0.0680984, acc 0.984375\n",
            "2019-01-22T06:29:59.752017: step 2788, loss 5.75853e-05, acc 1\n",
            "2019-01-22T06:30:00.430411: step 2789, loss 0.00140193, acc 1\n",
            "2019-01-22T06:30:01.107962: step 2790, loss 0.0200643, acc 0.984375\n",
            "2019-01-22T06:30:01.797337: step 2791, loss 0.000551521, acc 1\n",
            "2019-01-22T06:30:02.476658: step 2792, loss 0.0500982, acc 0.984375\n",
            "2019-01-22T06:30:03.152365: step 2793, loss 0.00024625, acc 1\n",
            "2019-01-22T06:30:03.835811: step 2794, loss 7.71278e-05, acc 1\n",
            "2019-01-22T06:30:04.514826: step 2795, loss 0.000196211, acc 1\n",
            "2019-01-22T06:30:05.194440: step 2796, loss 7.55332e-05, acc 1\n",
            "2019-01-22T06:30:05.879914: step 2797, loss 0.0419262, acc 0.984375\n",
            "2019-01-22T06:30:06.554613: step 2798, loss 0.00191291, acc 1\n",
            "2019-01-22T06:30:07.233593: step 2799, loss 0.000804246, acc 1\n",
            "2019-01-22T06:30:07.917568: step 2800, loss 0.00102043, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:30:08.277337: step 2800, loss 4.99758e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2800\n",
            "\n",
            "2019-01-22T06:30:09.100910: step 2801, loss 0.000224309, acc 1\n",
            "2019-01-22T06:30:09.778555: step 2802, loss 0.0118429, acc 0.984375\n",
            "2019-01-22T06:30:10.456571: step 2803, loss 0.0143294, acc 0.984375\n",
            "2019-01-22T06:30:11.138397: step 2804, loss 3.52777e-06, acc 1\n",
            "2019-01-22T06:30:11.547242: step 2805, loss 7.561e-06, acc 1\n",
            "2019-01-22T06:30:12.222871: step 2806, loss 0.00267899, acc 1\n",
            "2019-01-22T06:30:12.902697: step 2807, loss 7.80229e-06, acc 1\n",
            "2019-01-22T06:30:13.587285: step 2808, loss 0.0178147, acc 0.984375\n",
            "2019-01-22T06:30:14.266854: step 2809, loss 0.0159598, acc 0.984375\n",
            "2019-01-22T06:30:14.952204: step 2810, loss 7.58404e-06, acc 1\n",
            "2019-01-22T06:30:15.632477: step 2811, loss 0.0295964, acc 0.984375\n",
            "2019-01-22T06:30:16.306891: step 2812, loss 0.000216795, acc 1\n",
            "2019-01-22T06:30:16.980091: step 2813, loss 0.0465166, acc 0.96875\n",
            "2019-01-22T06:30:17.668254: step 2814, loss 6.25363e-05, acc 1\n",
            "2019-01-22T06:30:18.344177: step 2815, loss 0.000499436, acc 1\n",
            "2019-01-22T06:30:19.022902: step 2816, loss 0.00281295, acc 1\n",
            "2019-01-22T06:30:19.701376: step 2817, loss 3.7082e-05, acc 1\n",
            "2019-01-22T06:30:20.383996: step 2818, loss 0.0010471, acc 1\n",
            "2019-01-22T06:30:21.071263: step 2819, loss 0.000151102, acc 1\n",
            "2019-01-22T06:30:21.747808: step 2820, loss 0.0013543, acc 1\n",
            "2019-01-22T06:30:22.428710: step 2821, loss 0.000112553, acc 1\n",
            "2019-01-22T06:30:23.107523: step 2822, loss 0.000242103, acc 1\n",
            "2019-01-22T06:30:23.789756: step 2823, loss 1.45785e-05, acc 1\n",
            "2019-01-22T06:30:24.469398: step 2824, loss 0.000875857, acc 1\n",
            "2019-01-22T06:30:25.149635: step 2825, loss 3.6285e-05, acc 1\n",
            "2019-01-22T06:30:25.829669: step 2826, loss 0.000361984, acc 1\n",
            "2019-01-22T06:30:26.512601: step 2827, loss 9.13016e-05, acc 1\n",
            "2019-01-22T06:30:27.189492: step 2828, loss 0.00387471, acc 1\n",
            "2019-01-22T06:30:27.861383: step 2829, loss 2.83993e-05, acc 1\n",
            "2019-01-22T06:30:28.546083: step 2830, loss 0.000301185, acc 1\n",
            "2019-01-22T06:30:29.228478: step 2831, loss 0.00799979, acc 1\n",
            "2019-01-22T06:30:29.913504: step 2832, loss 0.00505808, acc 1\n",
            "2019-01-22T06:30:30.597964: step 2833, loss 0.000231765, acc 1\n",
            "2019-01-22T06:30:31.276865: step 2834, loss 7.1414e-05, acc 1\n",
            "2019-01-22T06:30:31.955396: step 2835, loss 5.24712e-05, acc 1\n",
            "2019-01-22T06:30:32.640617: step 2836, loss 0.000366086, acc 1\n",
            "2019-01-22T06:30:33.324330: step 2837, loss 0.00312982, acc 1\n",
            "2019-01-22T06:30:33.735889: step 2838, loss 3.93274e-05, acc 1\n",
            "2019-01-22T06:30:34.412646: step 2839, loss 7.67973e-05, acc 1\n",
            "2019-01-22T06:30:35.091964: step 2840, loss 0.000367948, acc 1\n",
            "2019-01-22T06:30:35.770904: step 2841, loss 0.000369246, acc 1\n",
            "2019-01-22T06:30:36.452450: step 2842, loss 0.0342594, acc 0.984375\n",
            "2019-01-22T06:30:37.134704: step 2843, loss 5.76289e-05, acc 1\n",
            "2019-01-22T06:30:37.822934: step 2844, loss 0.0600182, acc 0.984375\n",
            "2019-01-22T06:30:38.503265: step 2845, loss 0.11731, acc 0.984375\n",
            "2019-01-22T06:30:39.178580: step 2846, loss 0.00874629, acc 1\n",
            "2019-01-22T06:30:39.871286: step 2847, loss 0.00351892, acc 1\n",
            "2019-01-22T06:30:40.554250: step 2848, loss 0.000499013, acc 1\n",
            "2019-01-22T06:30:41.231487: step 2849, loss 8.78031e-05, acc 1\n",
            "2019-01-22T06:30:41.919385: step 2850, loss 0.000115599, acc 1\n",
            "2019-01-22T06:30:42.599337: step 2851, loss 3.93057e-05, acc 1\n",
            "2019-01-22T06:30:43.279950: step 2852, loss 0.00228108, acc 1\n",
            "2019-01-22T06:30:43.967122: step 2853, loss 0.00295822, acc 1\n",
            "2019-01-22T06:30:44.650415: step 2854, loss 0.00184784, acc 1\n",
            "2019-01-22T06:30:45.330091: step 2855, loss 0.00170126, acc 1\n",
            "2019-01-22T06:30:46.016002: step 2856, loss 0.000103847, acc 1\n",
            "2019-01-22T06:30:46.697746: step 2857, loss 0.0580045, acc 0.984375\n",
            "2019-01-22T06:30:47.380274: step 2858, loss 0.000321288, acc 1\n",
            "2019-01-22T06:30:48.072640: step 2859, loss 3.11875e-05, acc 1\n",
            "2019-01-22T06:30:48.754379: step 2860, loss 0.000164368, acc 1\n",
            "2019-01-22T06:30:49.437802: step 2861, loss 3.09735e-05, acc 1\n",
            "2019-01-22T06:30:50.125347: step 2862, loss 0.00117313, acc 1\n",
            "2019-01-22T06:30:50.810001: step 2863, loss 0.00150046, acc 1\n",
            "2019-01-22T06:30:51.487326: step 2864, loss 0.0820374, acc 0.984375\n",
            "2019-01-22T06:30:52.176879: step 2865, loss 0.000611698, acc 1\n",
            "2019-01-22T06:30:52.859318: step 2866, loss 6.41946e-05, acc 1\n",
            "2019-01-22T06:30:53.538410: step 2867, loss 0.0107963, acc 1\n",
            "2019-01-22T06:30:54.226574: step 2868, loss 0.0105157, acc 1\n",
            "2019-01-22T06:30:54.902571: step 2869, loss 6.89833e-05, acc 1\n",
            "2019-01-22T06:30:55.582302: step 2870, loss 0.000117024, acc 1\n",
            "2019-01-22T06:30:55.999225: step 2871, loss 8.85552e-07, acc 1\n",
            "2019-01-22T06:30:56.682084: step 2872, loss 5.43727e-05, acc 1\n",
            "2019-01-22T06:30:57.363317: step 2873, loss 0.000141848, acc 1\n",
            "2019-01-22T06:30:58.043934: step 2874, loss 1.75906e-05, acc 1\n",
            "2019-01-22T06:30:58.727273: step 2875, loss 0.000530427, acc 1\n",
            "2019-01-22T06:30:59.413419: step 2876, loss 6.0457e-06, acc 1\n",
            "2019-01-22T06:31:00.089978: step 2877, loss 0.000173461, acc 1\n",
            "2019-01-22T06:31:00.766515: step 2878, loss 8.06349e-05, acc 1\n",
            "2019-01-22T06:31:01.460260: step 2879, loss 9.18721e-05, acc 1\n",
            "2019-01-22T06:31:02.142500: step 2880, loss 0.00343632, acc 1\n",
            "2019-01-22T06:31:02.823010: step 2881, loss 3.04739e-05, acc 1\n",
            "2019-01-22T06:31:03.508442: step 2882, loss 0.000298767, acc 1\n",
            "2019-01-22T06:31:04.186509: step 2883, loss 0.00247771, acc 1\n",
            "2019-01-22T06:31:04.866441: step 2884, loss 0.000235479, acc 1\n",
            "2019-01-22T06:31:05.550358: step 2885, loss 0.00111129, acc 1\n",
            "2019-01-22T06:31:06.228122: step 2886, loss 7.1893e-06, acc 1\n",
            "2019-01-22T06:31:06.903642: step 2887, loss 0.000166415, acc 1\n",
            "2019-01-22T06:31:07.585943: step 2888, loss 0.000626148, acc 1\n",
            "2019-01-22T06:31:08.263972: step 2889, loss 0.000381495, acc 1\n",
            "2019-01-22T06:31:08.937792: step 2890, loss 0.000240734, acc 1\n",
            "2019-01-22T06:31:09.624102: step 2891, loss 0.00439467, acc 1\n",
            "2019-01-22T06:31:10.300239: step 2892, loss 0.000698175, acc 1\n",
            "2019-01-22T06:31:10.980687: step 2893, loss 0.000415297, acc 1\n",
            "2019-01-22T06:31:11.662946: step 2894, loss 0.012993, acc 0.984375\n",
            "2019-01-22T06:31:12.340129: step 2895, loss 8.04307e-05, acc 1\n",
            "2019-01-22T06:31:13.015442: step 2896, loss 0.0010925, acc 1\n",
            "2019-01-22T06:31:13.703404: step 2897, loss 1.89811e-05, acc 1\n",
            "2019-01-22T06:31:14.382088: step 2898, loss 0.000994458, acc 1\n",
            "2019-01-22T06:31:15.064627: step 2899, loss 0.000425671, acc 1\n",
            "2019-01-22T06:31:15.750220: step 2900, loss 3.23932e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:31:16.109025: step 2900, loss 3.14351e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-2900\n",
            "\n",
            "2019-01-22T06:31:16.924247: step 2901, loss 0.0252936, acc 0.984375\n",
            "2019-01-22T06:31:17.608499: step 2902, loss 0.000112401, acc 1\n",
            "2019-01-22T06:31:18.289419: step 2903, loss 0.00388061, acc 1\n",
            "2019-01-22T06:31:18.704422: step 2904, loss 0.00170507, acc 1\n",
            "2019-01-22T06:31:19.384789: step 2905, loss 0.00604843, acc 1\n",
            "2019-01-22T06:31:20.067076: step 2906, loss 3.684e-05, acc 1\n",
            "2019-01-22T06:31:20.746742: step 2907, loss 0.0339918, acc 0.96875\n",
            "2019-01-22T06:31:21.435363: step 2908, loss 0.000431484, acc 1\n",
            "2019-01-22T06:31:22.118216: step 2909, loss 0.000194146, acc 1\n",
            "2019-01-22T06:31:22.797485: step 2910, loss 0.00108774, acc 1\n",
            "2019-01-22T06:31:23.480431: step 2911, loss 0.00192303, acc 1\n",
            "2019-01-22T06:31:24.157078: step 2912, loss 0.00448011, acc 1\n",
            "2019-01-22T06:31:24.840199: step 2913, loss 1.46189e-05, acc 1\n",
            "2019-01-22T06:31:25.517354: step 2914, loss 0.000145524, acc 1\n",
            "2019-01-22T06:31:26.197251: step 2915, loss 0.000678322, acc 1\n",
            "2019-01-22T06:31:26.875910: step 2916, loss 0.000358034, acc 1\n",
            "2019-01-22T06:31:27.554642: step 2917, loss 0.000702544, acc 1\n",
            "2019-01-22T06:31:28.233434: step 2918, loss 0.000177727, acc 1\n",
            "2019-01-22T06:31:28.914462: step 2919, loss 0.0552782, acc 0.984375\n",
            "2019-01-22T06:31:29.591652: step 2920, loss 0.00194758, acc 1\n",
            "2019-01-22T06:31:30.274586: step 2921, loss 1.80673e-06, acc 1\n",
            "2019-01-22T06:31:30.950795: step 2922, loss 0.000127042, acc 1\n",
            "2019-01-22T06:31:31.635010: step 2923, loss 0.00122689, acc 1\n",
            "2019-01-22T06:31:32.324816: step 2924, loss 0.0149309, acc 0.984375\n",
            "2019-01-22T06:31:33.008134: step 2925, loss 0.000342324, acc 1\n",
            "2019-01-22T06:31:33.685746: step 2926, loss 3.73383e-05, acc 1\n",
            "2019-01-22T06:31:34.374222: step 2927, loss 0.000174174, acc 1\n",
            "2019-01-22T06:31:35.054754: step 2928, loss 0.000895919, acc 1\n",
            "2019-01-22T06:31:35.730964: step 2929, loss 0.000158257, acc 1\n",
            "2019-01-22T06:31:36.419583: step 2930, loss 0.00115706, acc 1\n",
            "2019-01-22T06:31:37.103100: step 2931, loss 0.00445449, acc 1\n",
            "2019-01-22T06:31:37.783528: step 2932, loss 0.00240789, acc 1\n",
            "2019-01-22T06:31:38.472727: step 2933, loss 0.000582651, acc 1\n",
            "2019-01-22T06:31:39.155568: step 2934, loss 0.00506288, acc 1\n",
            "2019-01-22T06:31:39.833573: step 2935, loss 0.00185959, acc 1\n",
            "2019-01-22T06:31:40.530376: step 2936, loss 0.000174465, acc 1\n",
            "2019-01-22T06:31:40.943179: step 2937, loss 0.00367577, acc 1\n",
            "2019-01-22T06:31:41.624595: step 2938, loss 0.000114462, acc 1\n",
            "2019-01-22T06:31:42.308281: step 2939, loss 4.36587e-05, acc 1\n",
            "2019-01-22T06:31:42.992555: step 2940, loss 3.72298e-05, acc 1\n",
            "2019-01-22T06:31:43.673938: step 2941, loss 3.21861e-06, acc 1\n",
            "2019-01-22T06:31:44.353368: step 2942, loss 0.000260647, acc 1\n",
            "2019-01-22T06:31:45.035103: step 2943, loss 0.000214101, acc 1\n",
            "2019-01-22T06:31:45.716055: step 2944, loss 0.000485478, acc 1\n",
            "2019-01-22T06:31:46.396537: step 2945, loss 4.15131e-05, acc 1\n",
            "2019-01-22T06:31:47.076449: step 2946, loss 2.82388e-05, acc 1\n",
            "2019-01-22T06:31:47.761019: step 2947, loss 0.000579157, acc 1\n",
            "2019-01-22T06:31:48.445689: step 2948, loss 4.36271e-05, acc 1\n",
            "2019-01-22T06:31:49.121602: step 2949, loss 0.00143109, acc 1\n",
            "2019-01-22T06:31:49.804804: step 2950, loss 0.000216495, acc 1\n",
            "2019-01-22T06:31:50.486749: step 2951, loss 0.000261548, acc 1\n",
            "2019-01-22T06:31:51.165584: step 2952, loss 0.000281163, acc 1\n",
            "2019-01-22T06:31:51.853873: step 2953, loss 6.58551e-05, acc 1\n",
            "2019-01-22T06:31:52.531920: step 2954, loss 0.00588114, acc 1\n",
            "2019-01-22T06:31:53.213897: step 2955, loss 0.000142499, acc 1\n",
            "2019-01-22T06:31:53.901337: step 2956, loss 0.00258096, acc 1\n",
            "2019-01-22T06:31:54.578805: step 2957, loss 0.000346506, acc 1\n",
            "2019-01-22T06:31:55.262747: step 2958, loss 0.00123139, acc 1\n",
            "2019-01-22T06:31:55.948773: step 2959, loss 0.000347825, acc 1\n",
            "2019-01-22T06:31:56.627143: step 2960, loss 0.00932703, acc 1\n",
            "2019-01-22T06:31:57.306607: step 2961, loss 0.000989318, acc 1\n",
            "2019-01-22T06:31:57.998828: step 2962, loss 0.0048565, acc 1\n",
            "2019-01-22T06:31:58.676592: step 2963, loss 0.00152798, acc 1\n",
            "2019-01-22T06:31:59.355395: step 2964, loss 0.00387562, acc 1\n",
            "2019-01-22T06:32:00.045736: step 2965, loss 8.44457e-05, acc 1\n",
            "2019-01-22T06:32:00.724468: step 2966, loss 6.72954e-05, acc 1\n",
            "2019-01-22T06:32:01.402558: step 2967, loss 2.63453e-05, acc 1\n",
            "2019-01-22T06:32:02.093114: step 2968, loss 0.000487503, acc 1\n",
            "2019-01-22T06:32:02.774240: step 2969, loss 0.000136555, acc 1\n",
            "2019-01-22T06:32:03.186014: step 2970, loss 0.00142728, acc 1\n",
            "2019-01-22T06:32:03.867592: step 2971, loss 0.000314271, acc 1\n",
            "2019-01-22T06:32:04.546508: step 2972, loss 8.35847e-05, acc 1\n",
            "2019-01-22T06:32:05.228801: step 2973, loss 7.09938e-06, acc 1\n",
            "2019-01-22T06:32:05.910298: step 2974, loss 0.000234699, acc 1\n",
            "2019-01-22T06:32:06.592319: step 2975, loss 0.00355895, acc 1\n",
            "2019-01-22T06:32:07.279769: step 2976, loss 0.000901867, acc 1\n",
            "2019-01-22T06:32:07.957525: step 2977, loss 0.0565487, acc 0.984375\n",
            "2019-01-22T06:32:08.635807: step 2978, loss 0.000796988, acc 1\n",
            "2019-01-22T06:32:09.321578: step 2979, loss 6.5008e-05, acc 1\n",
            "2019-01-22T06:32:10.003284: step 2980, loss 2.99223e-05, acc 1\n",
            "2019-01-22T06:32:10.678977: step 2981, loss 0.0110868, acc 0.984375\n",
            "2019-01-22T06:32:11.365921: step 2982, loss 0.000100609, acc 1\n",
            "2019-01-22T06:32:12.048192: step 2983, loss 0.000560166, acc 1\n",
            "2019-01-22T06:32:12.724871: step 2984, loss 0.00499508, acc 1\n",
            "2019-01-22T06:32:13.411045: step 2985, loss 0.000151321, acc 1\n",
            "2019-01-22T06:32:14.093731: step 2986, loss 8.19414e-05, acc 1\n",
            "2019-01-22T06:32:14.771502: step 2987, loss 0.000935927, acc 1\n",
            "2019-01-22T06:32:15.461743: step 2988, loss 0.00570987, acc 1\n",
            "2019-01-22T06:32:16.136965: step 2989, loss 8.12424e-05, acc 1\n",
            "2019-01-22T06:32:16.817955: step 2990, loss 0.00391617, acc 1\n",
            "2019-01-22T06:32:17.506705: step 2991, loss 0.00044864, acc 1\n",
            "2019-01-22T06:32:18.181787: step 2992, loss 0.0490864, acc 0.96875\n",
            "2019-01-22T06:32:18.862374: step 2993, loss 3.6706e-05, acc 1\n",
            "2019-01-22T06:32:19.545416: step 2994, loss 0.000784428, acc 1\n",
            "2019-01-22T06:32:20.224404: step 2995, loss 0.0036257, acc 1\n",
            "2019-01-22T06:32:20.910083: step 2996, loss 0.104167, acc 0.984375\n",
            "2019-01-22T06:32:21.601006: step 2997, loss 0.00157173, acc 1\n",
            "2019-01-22T06:32:22.284587: step 2998, loss 3.54282e-05, acc 1\n",
            "2019-01-22T06:32:22.962250: step 2999, loss 1.30158e-05, acc 1\n",
            "2019-01-22T06:32:23.639667: step 3000, loss 0.000514689, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:32:24.000318: step 3000, loss 6.3637e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3000\n",
            "\n",
            "2019-01-22T06:32:24.838324: step 3001, loss 0.000295262, acc 1\n",
            "2019-01-22T06:32:25.514642: step 3002, loss 3.77953e-05, acc 1\n",
            "2019-01-22T06:32:25.922856: step 3003, loss 0.000134078, acc 1\n",
            "2019-01-22T06:32:26.604226: step 3004, loss 3.17752e-05, acc 1\n",
            "2019-01-22T06:32:27.281549: step 3005, loss 0.104242, acc 0.984375\n",
            "2019-01-22T06:32:27.959631: step 3006, loss 0.000212269, acc 1\n",
            "2019-01-22T06:32:28.637672: step 3007, loss 0.00402177, acc 1\n",
            "2019-01-22T06:32:29.331242: step 3008, loss 0.00194027, acc 1\n",
            "2019-01-22T06:32:30.012424: step 3009, loss 0.0875943, acc 0.984375\n",
            "2019-01-22T06:32:30.688044: step 3010, loss 2.48684e-05, acc 1\n",
            "2019-01-22T06:32:31.367811: step 3011, loss 8.28221e-05, acc 1\n",
            "2019-01-22T06:32:32.047395: step 3012, loss 2.64325e-05, acc 1\n",
            "2019-01-22T06:32:32.728649: step 3013, loss 8.055e-05, acc 1\n",
            "2019-01-22T06:32:33.405816: step 3014, loss 6.80701e-05, acc 1\n",
            "2019-01-22T06:32:34.084952: step 3015, loss 0.000460187, acc 1\n",
            "2019-01-22T06:32:34.764702: step 3016, loss 0.000529396, acc 1\n",
            "2019-01-22T06:32:35.447555: step 3017, loss 8.24435e-05, acc 1\n",
            "2019-01-22T06:32:36.124233: step 3018, loss 3.97162e-05, acc 1\n",
            "2019-01-22T06:32:36.800711: step 3019, loss 0.000480413, acc 1\n",
            "2019-01-22T06:32:37.481898: step 3020, loss 1.17239e-05, acc 1\n",
            "2019-01-22T06:32:38.167142: step 3021, loss 0.000184071, acc 1\n",
            "2019-01-22T06:32:38.850050: step 3022, loss 8.21717e-06, acc 1\n",
            "2019-01-22T06:32:39.527652: step 3023, loss 2.01015e-05, acc 1\n",
            "2019-01-22T06:32:40.215480: step 3024, loss 0.000476719, acc 1\n",
            "2019-01-22T06:32:40.895210: step 3025, loss 2.28422e-05, acc 1\n",
            "2019-01-22T06:32:41.579052: step 3026, loss 6.25607e-05, acc 1\n",
            "2019-01-22T06:32:42.265284: step 3027, loss 0.00837908, acc 1\n",
            "2019-01-22T06:32:42.944023: step 3028, loss 6.80419e-05, acc 1\n",
            "2019-01-22T06:32:43.622355: step 3029, loss 6.36396e-05, acc 1\n",
            "2019-01-22T06:32:44.309477: step 3030, loss 4.573e-05, acc 1\n",
            "2019-01-22T06:32:44.989742: step 3031, loss 2.99363e-05, acc 1\n",
            "2019-01-22T06:32:45.668469: step 3032, loss 0.000867228, acc 1\n",
            "2019-01-22T06:32:46.355734: step 3033, loss 0.022909, acc 0.984375\n",
            "2019-01-22T06:32:47.034318: step 3034, loss 4.18335e-05, acc 1\n",
            "2019-01-22T06:32:47.709571: step 3035, loss 0.000449091, acc 1\n",
            "2019-01-22T06:32:48.121778: step 3036, loss 0.0305872, acc 0.971429\n",
            "2019-01-22T06:32:48.801436: step 3037, loss 1.18322e-05, acc 1\n",
            "2019-01-22T06:32:49.480297: step 3038, loss 2.4327e-05, acc 1\n",
            "2019-01-22T06:32:50.158608: step 3039, loss 6.87582e-06, acc 1\n",
            "2019-01-22T06:32:50.838926: step 3040, loss 7.61431e-05, acc 1\n",
            "2019-01-22T06:32:51.528651: step 3041, loss 5.04032e-05, acc 1\n",
            "2019-01-22T06:32:52.210352: step 3042, loss 0.000334387, acc 1\n",
            "2019-01-22T06:32:52.886930: step 3043, loss 0.000447069, acc 1\n",
            "2019-01-22T06:32:53.571142: step 3044, loss 5.20307e-05, acc 1\n",
            "2019-01-22T06:32:54.248574: step 3045, loss 3.66935e-06, acc 1\n",
            "2019-01-22T06:32:54.930367: step 3046, loss 5.5262e-06, acc 1\n",
            "2019-01-22T06:32:55.619395: step 3047, loss 0.000102803, acc 1\n",
            "2019-01-22T06:32:56.302452: step 3048, loss 7.33403e-05, acc 1\n",
            "2019-01-22T06:32:56.979387: step 3049, loss 0.000277067, acc 1\n",
            "2019-01-22T06:32:57.664452: step 3050, loss 2.6355e-05, acc 1\n",
            "2019-01-22T06:32:58.341521: step 3051, loss 0.000313142, acc 1\n",
            "2019-01-22T06:32:59.021966: step 3052, loss 0.00396432, acc 1\n",
            "2019-01-22T06:32:59.706563: step 3053, loss 0.00561806, acc 1\n",
            "2019-01-22T06:33:00.382559: step 3054, loss 2.05817e-05, acc 1\n",
            "2019-01-22T06:33:01.061051: step 3055, loss 0.000794306, acc 1\n",
            "2019-01-22T06:33:01.747533: step 3056, loss 1.23764e-05, acc 1\n",
            "2019-01-22T06:33:02.422533: step 3057, loss 0.000197182, acc 1\n",
            "2019-01-22T06:33:03.100780: step 3058, loss 0.0299567, acc 0.984375\n",
            "2019-01-22T06:33:03.781975: step 3059, loss 0.00317324, acc 1\n",
            "2019-01-22T06:33:04.463912: step 3060, loss 9.31896e-05, acc 1\n",
            "2019-01-22T06:33:05.141362: step 3061, loss 2.69526e-05, acc 1\n",
            "2019-01-22T06:33:05.817554: step 3062, loss 3.20835e-05, acc 1\n",
            "2019-01-22T06:33:06.495682: step 3063, loss 0.0295346, acc 0.984375\n",
            "2019-01-22T06:33:07.178032: step 3064, loss 0.0169687, acc 0.984375\n",
            "2019-01-22T06:33:07.859975: step 3065, loss 0.0358856, acc 0.984375\n",
            "2019-01-22T06:33:08.540602: step 3066, loss 0.00354175, acc 1\n",
            "2019-01-22T06:33:09.220660: step 3067, loss 3.49197e-05, acc 1\n",
            "2019-01-22T06:33:09.900763: step 3068, loss 0.00131952, acc 1\n",
            "2019-01-22T06:33:10.314008: step 3069, loss 0.000353723, acc 1\n",
            "2019-01-22T06:33:11.002813: step 3070, loss 0.00022567, acc 1\n",
            "2019-01-22T06:33:11.682742: step 3071, loss 2.37124e-05, acc 1\n",
            "2019-01-22T06:33:12.360189: step 3072, loss 1.63728e-05, acc 1\n",
            "2019-01-22T06:33:13.047353: step 3073, loss 3.28306e-05, acc 1\n",
            "2019-01-22T06:33:13.729073: step 3074, loss 0.000721006, acc 1\n",
            "2019-01-22T06:33:14.414240: step 3075, loss 6.33486e-05, acc 1\n",
            "2019-01-22T06:33:15.101922: step 3076, loss 2.38512e-05, acc 1\n",
            "2019-01-22T06:33:15.784520: step 3077, loss 0.000132796, acc 1\n",
            "2019-01-22T06:33:16.467112: step 3078, loss 0.0195884, acc 0.984375\n",
            "2019-01-22T06:33:17.153820: step 3079, loss 4.67656e-05, acc 1\n",
            "2019-01-22T06:33:17.831781: step 3080, loss 0.000367575, acc 1\n",
            "2019-01-22T06:33:18.510391: step 3081, loss 0.000423844, acc 1\n",
            "2019-01-22T06:33:19.202801: step 3082, loss 8.36285e-05, acc 1\n",
            "2019-01-22T06:33:19.880098: step 3083, loss 0.0120913, acc 0.984375\n",
            "2019-01-22T06:33:20.562382: step 3084, loss 0.00431189, acc 1\n",
            "2019-01-22T06:33:21.251463: step 3085, loss 0.000498797, acc 1\n",
            "2019-01-22T06:33:21.929628: step 3086, loss 0.000406076, acc 1\n",
            "2019-01-22T06:33:22.609766: step 3087, loss 4.06963e-05, acc 1\n",
            "2019-01-22T06:33:23.291550: step 3088, loss 0.000130531, acc 1\n",
            "2019-01-22T06:33:23.969876: step 3089, loss 4.49738e-05, acc 1\n",
            "2019-01-22T06:33:24.654423: step 3090, loss 0.000156995, acc 1\n",
            "2019-01-22T06:33:25.333844: step 3091, loss 3.94177e-05, acc 1\n",
            "2019-01-22T06:33:26.014423: step 3092, loss 0.000114689, acc 1\n",
            "2019-01-22T06:33:26.694347: step 3093, loss 0.000527892, acc 1\n",
            "2019-01-22T06:33:27.375966: step 3094, loss 0.0070592, acc 1\n",
            "2019-01-22T06:33:28.059633: step 3095, loss 0.000550143, acc 1\n",
            "2019-01-22T06:33:28.738551: step 3096, loss 6.43376e-05, acc 1\n",
            "2019-01-22T06:33:29.420792: step 3097, loss 0.0263562, acc 0.984375\n",
            "2019-01-22T06:33:30.101304: step 3098, loss 0.000122439, acc 1\n",
            "2019-01-22T06:33:30.785881: step 3099, loss 0.000212712, acc 1\n",
            "2019-01-22T06:33:31.468630: step 3100, loss 0.0376572, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:33:31.832013: step 3100, loss 5.21084e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3100\n",
            "\n",
            "2019-01-22T06:33:32.665280: step 3101, loss 4.18423e-05, acc 1\n",
            "2019-01-22T06:33:33.079999: step 3102, loss 2.73162e-05, acc 1\n",
            "2019-01-22T06:33:33.758182: step 3103, loss 0.00546288, acc 1\n",
            "2019-01-22T06:33:34.437659: step 3104, loss 0.000578396, acc 1\n",
            "2019-01-22T06:33:35.114016: step 3105, loss 0.0438026, acc 0.96875\n",
            "2019-01-22T06:33:35.797823: step 3106, loss 0.0374343, acc 0.984375\n",
            "2019-01-22T06:33:36.478469: step 3107, loss 0.000393865, acc 1\n",
            "2019-01-22T06:33:37.155564: step 3108, loss 0.000134401, acc 1\n",
            "2019-01-22T06:33:37.834466: step 3109, loss 0.000311536, acc 1\n",
            "2019-01-22T06:33:38.515325: step 3110, loss 4.58416e-05, acc 1\n",
            "2019-01-22T06:33:39.192747: step 3111, loss 0.000548306, acc 1\n",
            "2019-01-22T06:33:39.866601: step 3112, loss 0.000117617, acc 1\n",
            "2019-01-22T06:33:40.545927: step 3113, loss 3.31515e-05, acc 1\n",
            "2019-01-22T06:33:41.226811: step 3114, loss 2.93013e-05, acc 1\n",
            "2019-01-22T06:33:41.908919: step 3115, loss 0.00708, acc 1\n",
            "2019-01-22T06:33:42.588779: step 3116, loss 6.20831e-05, acc 1\n",
            "2019-01-22T06:33:43.266826: step 3117, loss 0.00686229, acc 1\n",
            "2019-01-22T06:33:43.949558: step 3118, loss 0.000396814, acc 1\n",
            "2019-01-22T06:33:44.633317: step 3119, loss 0.000394018, acc 1\n",
            "2019-01-22T06:33:45.316150: step 3120, loss 0.00626568, acc 1\n",
            "2019-01-22T06:33:46.001853: step 3121, loss 0.000114248, acc 1\n",
            "2019-01-22T06:33:46.685454: step 3122, loss 0.046427, acc 0.984375\n",
            "2019-01-22T06:33:47.367434: step 3123, loss 0.000266269, acc 1\n",
            "2019-01-22T06:33:48.056104: step 3124, loss 0.00413668, acc 1\n",
            "2019-01-22T06:33:48.733349: step 3125, loss 2.13159e-05, acc 1\n",
            "2019-01-22T06:33:49.414964: step 3126, loss 0.000531457, acc 1\n",
            "2019-01-22T06:33:50.107654: step 3127, loss 0.000343149, acc 1\n",
            "2019-01-22T06:33:50.785735: step 3128, loss 0.000971118, acc 1\n",
            "2019-01-22T06:33:51.467111: step 3129, loss 0.000116012, acc 1\n",
            "2019-01-22T06:33:52.150131: step 3130, loss 0.00104167, acc 1\n",
            "2019-01-22T06:33:52.833072: step 3131, loss 0.000716855, acc 1\n",
            "2019-01-22T06:33:53.512505: step 3132, loss 0.000845534, acc 1\n",
            "2019-01-22T06:33:54.201440: step 3133, loss 1.66983e-05, acc 1\n",
            "2019-01-22T06:33:54.888053: step 3134, loss 0.000185509, acc 1\n",
            "2019-01-22T06:33:55.299489: step 3135, loss 2.24068e-05, acc 1\n",
            "2019-01-22T06:33:55.979471: step 3136, loss 0.000141816, acc 1\n",
            "2019-01-22T06:33:56.657120: step 3137, loss 0.00292626, acc 1\n",
            "2019-01-22T06:33:57.341351: step 3138, loss 0.00109598, acc 1\n",
            "2019-01-22T06:33:58.022258: step 3139, loss 0.00212738, acc 1\n",
            "2019-01-22T06:33:58.702501: step 3140, loss 3.04401e-05, acc 1\n",
            "2019-01-22T06:33:59.387468: step 3141, loss 4.16742e-05, acc 1\n",
            "2019-01-22T06:34:00.069207: step 3142, loss 0.000124149, acc 1\n",
            "2019-01-22T06:34:00.751852: step 3143, loss 3.04201e-05, acc 1\n",
            "2019-01-22T06:34:01.437939: step 3144, loss 0.000110038, acc 1\n",
            "2019-01-22T06:34:02.116003: step 3145, loss 0.000102228, acc 1\n",
            "2019-01-22T06:34:02.795131: step 3146, loss 0.000245258, acc 1\n",
            "2019-01-22T06:34:03.482172: step 3147, loss 2.2334e-05, acc 1\n",
            "2019-01-22T06:34:04.162374: step 3148, loss 3.43173e-05, acc 1\n",
            "2019-01-22T06:34:04.844213: step 3149, loss 0.000114338, acc 1\n",
            "2019-01-22T06:34:05.534396: step 3150, loss 8.60584e-05, acc 1\n",
            "2019-01-22T06:34:06.216394: step 3151, loss 0.0060035, acc 1\n",
            "2019-01-22T06:34:06.894525: step 3152, loss 0.000328456, acc 1\n",
            "2019-01-22T06:34:07.584505: step 3153, loss 0.000446233, acc 1\n",
            "2019-01-22T06:34:08.262478: step 3154, loss 0.000545478, acc 1\n",
            "2019-01-22T06:34:08.942833: step 3155, loss 0.000364724, acc 1\n",
            "2019-01-22T06:34:09.631968: step 3156, loss 2.72254e-05, acc 1\n",
            "2019-01-22T06:34:10.309757: step 3157, loss 1.70373e-05, acc 1\n",
            "2019-01-22T06:34:10.989610: step 3158, loss 0.0120771, acc 0.984375\n",
            "2019-01-22T06:34:11.676907: step 3159, loss 5.41644e-05, acc 1\n",
            "2019-01-22T06:34:12.355902: step 3160, loss 8.11958e-05, acc 1\n",
            "2019-01-22T06:34:13.030021: step 3161, loss 0.00013733, acc 1\n",
            "2019-01-22T06:34:13.715430: step 3162, loss 0.000119545, acc 1\n",
            "2019-01-22T06:34:14.395183: step 3163, loss 2.43409e-05, acc 1\n",
            "2019-01-22T06:34:15.075376: step 3164, loss 0.0107869, acc 1\n",
            "2019-01-22T06:34:15.759827: step 3165, loss 0.000117146, acc 1\n",
            "2019-01-22T06:34:16.443776: step 3166, loss 0.00194668, acc 1\n",
            "2019-01-22T06:34:17.127363: step 3167, loss 0.000664496, acc 1\n",
            "2019-01-22T06:34:17.543602: step 3168, loss 0.00226469, acc 1\n",
            "2019-01-22T06:34:18.222189: step 3169, loss 0.00255341, acc 1\n",
            "2019-01-22T06:34:18.906801: step 3170, loss 2.13927e-05, acc 1\n",
            "2019-01-22T06:34:19.591475: step 3171, loss 0.00259676, acc 1\n",
            "2019-01-22T06:34:20.273858: step 3172, loss 0.00270218, acc 1\n",
            "2019-01-22T06:34:20.957948: step 3173, loss 0.000341491, acc 1\n",
            "2019-01-22T06:34:21.639446: step 3174, loss 0.000200733, acc 1\n",
            "2019-01-22T06:34:22.321483: step 3175, loss 0.000665032, acc 1\n",
            "2019-01-22T06:34:23.010404: step 3176, loss 0.000132397, acc 1\n",
            "2019-01-22T06:34:23.693458: step 3177, loss 0.000929026, acc 1\n",
            "2019-01-22T06:34:24.369375: step 3178, loss 1.25576e-05, acc 1\n",
            "2019-01-22T06:34:25.054911: step 3179, loss 9.13408e-05, acc 1\n",
            "2019-01-22T06:34:25.736788: step 3180, loss 4.53279e-05, acc 1\n",
            "2019-01-22T06:34:26.419487: step 3181, loss 0.000408072, acc 1\n",
            "2019-01-22T06:34:27.102051: step 3182, loss 7.04385e-05, acc 1\n",
            "2019-01-22T06:34:27.779446: step 3183, loss 0.000342736, acc 1\n",
            "2019-01-22T06:34:28.462995: step 3184, loss 0.000109266, acc 1\n",
            "2019-01-22T06:34:29.147669: step 3185, loss 0.000320926, acc 1\n",
            "2019-01-22T06:34:29.824680: step 3186, loss 3.80529e-06, acc 1\n",
            "2019-01-22T06:34:30.505659: step 3187, loss 0.0767591, acc 0.984375\n",
            "2019-01-22T06:34:31.188923: step 3188, loss 0.00230588, acc 1\n",
            "2019-01-22T06:34:31.868890: step 3189, loss 0.000650927, acc 1\n",
            "2019-01-22T06:34:32.547273: step 3190, loss 0.000348931, acc 1\n",
            "2019-01-22T06:34:33.230574: step 3191, loss 0.0014799, acc 1\n",
            "2019-01-22T06:34:33.909291: step 3192, loss 0.000380818, acc 1\n",
            "2019-01-22T06:34:34.586692: step 3193, loss 9.79536e-05, acc 1\n",
            "2019-01-22T06:34:35.263148: step 3194, loss 0.000402057, acc 1\n",
            "2019-01-22T06:34:35.947396: step 3195, loss 0.000575839, acc 1\n",
            "2019-01-22T06:34:36.627327: step 3196, loss 0.0398114, acc 0.984375\n",
            "2019-01-22T06:34:37.307477: step 3197, loss 9.93775e-05, acc 1\n",
            "2019-01-22T06:34:37.983675: step 3198, loss 0.000164777, acc 1\n",
            "2019-01-22T06:34:38.665897: step 3199, loss 5.78874e-06, acc 1\n",
            "2019-01-22T06:34:39.345741: step 3200, loss 2.92558e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:34:39.710838: step 3200, loss 3.01804e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3200\n",
            "\n",
            "2019-01-22T06:34:40.275848: step 3201, loss 0.000618468, acc 1\n",
            "2019-01-22T06:34:40.956466: step 3202, loss 0.0308872, acc 0.984375\n",
            "2019-01-22T06:34:41.635880: step 3203, loss 9.59494e-05, acc 1\n",
            "2019-01-22T06:34:42.316977: step 3204, loss 9.49547e-05, acc 1\n",
            "2019-01-22T06:34:42.992876: step 3205, loss 1.71064e-05, acc 1\n",
            "2019-01-22T06:34:43.670427: step 3206, loss 0.00021979, acc 1\n",
            "2019-01-22T06:34:44.349722: step 3207, loss 0.00034615, acc 1\n",
            "2019-01-22T06:34:45.030732: step 3208, loss 0.000332015, acc 1\n",
            "2019-01-22T06:34:45.708000: step 3209, loss 0.000124234, acc 1\n",
            "2019-01-22T06:34:46.389514: step 3210, loss 0.00256965, acc 1\n",
            "2019-01-22T06:34:47.068486: step 3211, loss 4.69113e-05, acc 1\n",
            "2019-01-22T06:34:47.752745: step 3212, loss 0.00133395, acc 1\n",
            "2019-01-22T06:34:48.435227: step 3213, loss 0.000494301, acc 1\n",
            "2019-01-22T06:34:49.120351: step 3214, loss 0.00708406, acc 1\n",
            "2019-01-22T06:34:49.799022: step 3215, loss 0.000102903, acc 1\n",
            "2019-01-22T06:34:50.476022: step 3216, loss 0.000225092, acc 1\n",
            "2019-01-22T06:34:51.151449: step 3217, loss 0.000167818, acc 1\n",
            "2019-01-22T06:34:51.834228: step 3218, loss 0.00194522, acc 1\n",
            "2019-01-22T06:34:52.516684: step 3219, loss 0.0013906, acc 1\n",
            "2019-01-22T06:34:53.196710: step 3220, loss 0.000323584, acc 1\n",
            "2019-01-22T06:34:53.884634: step 3221, loss 0.0012305, acc 1\n",
            "2019-01-22T06:34:54.563455: step 3222, loss 0.00021665, acc 1\n",
            "2019-01-22T06:34:55.242938: step 3223, loss 0.000827958, acc 1\n",
            "2019-01-22T06:34:55.929734: step 3224, loss 0.00115272, acc 1\n",
            "2019-01-22T06:34:56.609731: step 3225, loss 0.00247516, acc 1\n",
            "2019-01-22T06:34:57.287465: step 3226, loss 0.00105406, acc 1\n",
            "2019-01-22T06:34:57.975232: step 3227, loss 0.0008659, acc 1\n",
            "2019-01-22T06:34:58.656105: step 3228, loss 0.000122939, acc 1\n",
            "2019-01-22T06:34:59.338452: step 3229, loss 8.02485e-05, acc 1\n",
            "2019-01-22T06:35:00.027083: step 3230, loss 0.000152307, acc 1\n",
            "2019-01-22T06:35:00.708317: step 3231, loss 0.00167963, acc 1\n",
            "2019-01-22T06:35:01.385906: step 3232, loss 3.13947e-05, acc 1\n",
            "2019-01-22T06:35:02.076756: step 3233, loss 0.00178435, acc 1\n",
            "2019-01-22T06:35:02.491927: step 3234, loss 9.0553e-05, acc 1\n",
            "2019-01-22T06:35:03.171236: step 3235, loss 0.0002311, acc 1\n",
            "2019-01-22T06:35:03.846432: step 3236, loss 0.00193279, acc 1\n",
            "2019-01-22T06:35:04.523099: step 3237, loss 0.00120145, acc 1\n",
            "2019-01-22T06:35:05.212423: step 3238, loss 9.94331e-05, acc 1\n",
            "2019-01-22T06:35:05.892321: step 3239, loss 0.000127921, acc 1\n",
            "2019-01-22T06:35:06.576191: step 3240, loss 8.81585e-05, acc 1\n",
            "2019-01-22T06:35:07.262549: step 3241, loss 0.00306041, acc 1\n",
            "2019-01-22T06:35:07.936441: step 3242, loss 2.80585e-05, acc 1\n",
            "2019-01-22T06:35:08.618321: step 3243, loss 0.000209417, acc 1\n",
            "2019-01-22T06:35:09.309319: step 3244, loss 4.08366e-05, acc 1\n",
            "2019-01-22T06:35:10.001565: step 3245, loss 0.00346081, acc 1\n",
            "2019-01-22T06:35:10.679442: step 3246, loss 0.00251436, acc 1\n",
            "2019-01-22T06:35:11.366484: step 3247, loss 0.00212082, acc 1\n",
            "2019-01-22T06:35:12.048793: step 3248, loss 0.000250222, acc 1\n",
            "2019-01-22T06:35:12.728863: step 3249, loss 0.00351939, acc 1\n",
            "2019-01-22T06:35:13.417892: step 3250, loss 0.000114585, acc 1\n",
            "2019-01-22T06:35:14.094266: step 3251, loss 0.000655878, acc 1\n",
            "2019-01-22T06:35:14.774456: step 3252, loss 0.00135846, acc 1\n",
            "2019-01-22T06:35:15.466180: step 3253, loss 0.0196706, acc 0.984375\n",
            "2019-01-22T06:35:16.144693: step 3254, loss 0.000270311, acc 1\n",
            "2019-01-22T06:35:16.825359: step 3255, loss 7.0907e-05, acc 1\n",
            "2019-01-22T06:35:17.509428: step 3256, loss 0.000233649, acc 1\n",
            "2019-01-22T06:35:18.191512: step 3257, loss 8.77221e-06, acc 1\n",
            "2019-01-22T06:35:18.869668: step 3258, loss 6.13898e-06, acc 1\n",
            "2019-01-22T06:35:19.557807: step 3259, loss 0.00057664, acc 1\n",
            "2019-01-22T06:35:20.238014: step 3260, loss 0.0187085, acc 0.984375\n",
            "2019-01-22T06:35:20.918187: step 3261, loss 0.000765004, acc 1\n",
            "2019-01-22T06:35:21.607003: step 3262, loss 0.00110828, acc 1\n",
            "2019-01-22T06:35:22.287824: step 3263, loss 1.29196e-05, acc 1\n",
            "2019-01-22T06:35:22.967228: step 3264, loss 0.0182035, acc 0.984375\n",
            "2019-01-22T06:35:23.659636: step 3265, loss 0.0023869, acc 1\n",
            "2019-01-22T06:35:24.337370: step 3266, loss 2.37386e-05, acc 1\n",
            "2019-01-22T06:35:24.751210: step 3267, loss 5.39296e-05, acc 1\n",
            "2019-01-22T06:35:25.433726: step 3268, loss 0.000316575, acc 1\n",
            "2019-01-22T06:35:26.113641: step 3269, loss 8.20489e-05, acc 1\n",
            "2019-01-22T06:35:26.799291: step 3270, loss 0.00162792, acc 1\n",
            "2019-01-22T06:35:27.483206: step 3271, loss 5.86319e-06, acc 1\n",
            "2019-01-22T06:35:28.163431: step 3272, loss 0.00407865, acc 1\n",
            "2019-01-22T06:35:28.848864: step 3273, loss 0.00155398, acc 1\n",
            "2019-01-22T06:35:29.525381: step 3274, loss 4.44602e-05, acc 1\n",
            "2019-01-22T06:35:30.206766: step 3275, loss 0.000177404, acc 1\n",
            "2019-01-22T06:35:30.893576: step 3276, loss 0.00535124, acc 1\n",
            "2019-01-22T06:35:31.575757: step 3277, loss 7.73235e-05, acc 1\n",
            "2019-01-22T06:35:32.255529: step 3278, loss 1.61564e-05, acc 1\n",
            "2019-01-22T06:35:32.943694: step 3279, loss 0.00320313, acc 1\n",
            "2019-01-22T06:35:33.624986: step 3280, loss 0.000538052, acc 1\n",
            "2019-01-22T06:35:34.314599: step 3281, loss 0.000189145, acc 1\n",
            "2019-01-22T06:35:35.002305: step 3282, loss 0.000212899, acc 1\n",
            "2019-01-22T06:35:35.688317: step 3283, loss 0.000105247, acc 1\n",
            "2019-01-22T06:35:36.368972: step 3284, loss 4.7849e-05, acc 1\n",
            "2019-01-22T06:35:37.061950: step 3285, loss 0.00279471, acc 1\n",
            "2019-01-22T06:35:37.740453: step 3286, loss 0.000235923, acc 1\n",
            "2019-01-22T06:35:38.426946: step 3287, loss 0.000548817, acc 1\n",
            "2019-01-22T06:35:39.111703: step 3288, loss 8.53493e-05, acc 1\n",
            "2019-01-22T06:35:39.790751: step 3289, loss 0.0980597, acc 0.984375\n",
            "2019-01-22T06:35:40.469658: step 3290, loss 0.00138043, acc 1\n",
            "2019-01-22T06:35:41.156427: step 3291, loss 8.56623e-05, acc 1\n",
            "2019-01-22T06:35:41.841326: step 3292, loss 0.000715214, acc 1\n",
            "2019-01-22T06:35:42.521617: step 3293, loss 0.00147573, acc 1\n",
            "2019-01-22T06:35:43.207919: step 3294, loss 7.81334e-06, acc 1\n",
            "2019-01-22T06:35:43.889448: step 3295, loss 2.96573e-05, acc 1\n",
            "2019-01-22T06:35:44.568422: step 3296, loss 0.000237405, acc 1\n",
            "2019-01-22T06:35:45.257082: step 3297, loss 5.55972e-05, acc 1\n",
            "2019-01-22T06:35:45.935098: step 3298, loss 0.000451879, acc 1\n",
            "2019-01-22T06:35:46.615607: step 3299, loss 0.00148424, acc 1\n",
            "2019-01-22T06:35:47.032487: step 3300, loss 2.70925e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:35:47.394826: step 3300, loss 0.000101675, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3300\n",
            "\n",
            "2019-01-22T06:35:48.236961: step 3301, loss 0.000121285, acc 1\n",
            "2019-01-22T06:35:48.916906: step 3302, loss 3.89637e-05, acc 1\n",
            "2019-01-22T06:35:49.593727: step 3303, loss 1.72523e-05, acc 1\n",
            "2019-01-22T06:35:50.274121: step 3304, loss 0.00560953, acc 1\n",
            "2019-01-22T06:35:50.950966: step 3305, loss 0.000147163, acc 1\n",
            "2019-01-22T06:35:51.626900: step 3306, loss 0.00209795, acc 1\n",
            "2019-01-22T06:35:52.306640: step 3307, loss 0.000780806, acc 1\n",
            "2019-01-22T06:35:52.990295: step 3308, loss 2.14572e-06, acc 1\n",
            "2019-01-22T06:35:53.668351: step 3309, loss 5.21055e-05, acc 1\n",
            "2019-01-22T06:35:54.348420: step 3310, loss 0.0582496, acc 0.96875\n",
            "2019-01-22T06:35:55.027262: step 3311, loss 0.00112392, acc 1\n",
            "2019-01-22T06:35:55.703433: step 3312, loss 0.00543001, acc 1\n",
            "2019-01-22T06:35:56.384421: step 3313, loss 0.015105, acc 0.984375\n",
            "2019-01-22T06:35:57.062735: step 3314, loss 0.00216034, acc 1\n",
            "2019-01-22T06:35:57.742090: step 3315, loss 0.00239338, acc 1\n",
            "2019-01-22T06:35:58.421442: step 3316, loss 7.31581e-05, acc 1\n",
            "2019-01-22T06:35:59.105387: step 3317, loss 9.04082e-05, acc 1\n",
            "2019-01-22T06:35:59.789140: step 3318, loss 0.0011068, acc 1\n",
            "2019-01-22T06:36:00.471641: step 3319, loss 0.000606028, acc 1\n",
            "2019-01-22T06:36:01.149670: step 3320, loss 3.05665e-05, acc 1\n",
            "2019-01-22T06:36:01.829574: step 3321, loss 0.000258432, acc 1\n",
            "2019-01-22T06:36:02.513342: step 3322, loss 7.76847e-05, acc 1\n",
            "2019-01-22T06:36:03.194745: step 3323, loss 0.00450591, acc 1\n",
            "2019-01-22T06:36:03.879122: step 3324, loss 8.20886e-05, acc 1\n",
            "2019-01-22T06:36:04.558375: step 3325, loss 0.00130873, acc 1\n",
            "2019-01-22T06:36:05.241772: step 3326, loss 8.87344e-05, acc 1\n",
            "2019-01-22T06:36:05.924585: step 3327, loss 0.000438058, acc 1\n",
            "2019-01-22T06:36:06.603745: step 3328, loss 0.000542018, acc 1\n",
            "2019-01-22T06:36:07.284602: step 3329, loss 8.0918e-05, acc 1\n",
            "2019-01-22T06:36:07.974007: step 3330, loss 2.6671e-05, acc 1\n",
            "2019-01-22T06:36:08.656334: step 3331, loss 8.48069e-05, acc 1\n",
            "2019-01-22T06:36:09.333945: step 3332, loss 0.000275995, acc 1\n",
            "2019-01-22T06:36:09.747629: step 3333, loss 1.89796e-05, acc 1\n",
            "2019-01-22T06:36:10.430984: step 3334, loss 0.000949478, acc 1\n",
            "2019-01-22T06:36:11.109661: step 3335, loss 0.000499847, acc 1\n",
            "2019-01-22T06:36:11.788712: step 3336, loss 0.000736817, acc 1\n",
            "2019-01-22T06:36:12.474924: step 3337, loss 0.00807048, acc 1\n",
            "2019-01-22T06:36:13.156259: step 3338, loss 0.000195579, acc 1\n",
            "2019-01-22T06:36:13.837327: step 3339, loss 3.28475e-05, acc 1\n",
            "2019-01-22T06:36:14.517451: step 3340, loss 6.85792e-06, acc 1\n",
            "2019-01-22T06:36:15.199675: step 3341, loss 8.22943e-06, acc 1\n",
            "2019-01-22T06:36:15.885000: step 3342, loss 1.57278e-05, acc 1\n",
            "2019-01-22T06:36:16.562123: step 3343, loss 2.43912e-05, acc 1\n",
            "2019-01-22T06:36:17.251086: step 3344, loss 3.00438e-06, acc 1\n",
            "2019-01-22T06:36:17.933853: step 3345, loss 1.66011e-05, acc 1\n",
            "2019-01-22T06:36:18.611046: step 3346, loss 0.000429536, acc 1\n",
            "2019-01-22T06:36:19.298357: step 3347, loss 0.0587694, acc 0.984375\n",
            "2019-01-22T06:36:19.975787: step 3348, loss 0.000106042, acc 1\n",
            "2019-01-22T06:36:20.656697: step 3349, loss 5.89291e-06, acc 1\n",
            "2019-01-22T06:36:21.344240: step 3350, loss 0.000358049, acc 1\n",
            "2019-01-22T06:36:22.023056: step 3351, loss 0.000696389, acc 1\n",
            "2019-01-22T06:36:22.713034: step 3352, loss 4.0533e-05, acc 1\n",
            "2019-01-22T06:36:23.397018: step 3353, loss 0.000184425, acc 1\n",
            "2019-01-22T06:36:24.078439: step 3354, loss 1.33492e-05, acc 1\n",
            "2019-01-22T06:36:24.764643: step 3355, loss 4.88595e-05, acc 1\n",
            "2019-01-22T06:36:25.449279: step 3356, loss 0.000160303, acc 1\n",
            "2019-01-22T06:36:26.130836: step 3357, loss 0.00146875, acc 1\n",
            "2019-01-22T06:36:26.814466: step 3358, loss 8.28578e-05, acc 1\n",
            "2019-01-22T06:36:27.504639: step 3359, loss 0.00109247, acc 1\n",
            "2019-01-22T06:36:28.184642: step 3360, loss 0.00243722, acc 1\n",
            "2019-01-22T06:36:28.865703: step 3361, loss 3.83246e-05, acc 1\n",
            "2019-01-22T06:36:29.557362: step 3362, loss 0.00114182, acc 1\n",
            "2019-01-22T06:36:30.238976: step 3363, loss 7.2995e-06, acc 1\n",
            "2019-01-22T06:36:30.916583: step 3364, loss 0.000755695, acc 1\n",
            "2019-01-22T06:36:31.600877: step 3365, loss 9.10613e-05, acc 1\n",
            "2019-01-22T06:36:32.017028: step 3366, loss 0.00980786, acc 1\n",
            "2019-01-22T06:36:32.694611: step 3367, loss 2.24634e-05, acc 1\n",
            "2019-01-22T06:36:33.375339: step 3368, loss 0.000616662, acc 1\n",
            "2019-01-22T06:36:34.055576: step 3369, loss 0.00025313, acc 1\n",
            "2019-01-22T06:36:34.739220: step 3370, loss 0.00224695, acc 1\n",
            "2019-01-22T06:36:35.416810: step 3371, loss 0.00349496, acc 1\n",
            "2019-01-22T06:36:36.094821: step 3372, loss 0.0427696, acc 0.984375\n",
            "2019-01-22T06:36:36.782961: step 3373, loss 1.99512e-05, acc 1\n",
            "2019-01-22T06:36:37.466265: step 3374, loss 5.66991e-05, acc 1\n",
            "2019-01-22T06:36:38.146048: step 3375, loss 1.40399e-05, acc 1\n",
            "2019-01-22T06:36:38.832768: step 3376, loss 0.00131796, acc 1\n",
            "2019-01-22T06:36:39.511726: step 3377, loss 0.00721095, acc 1\n",
            "2019-01-22T06:36:40.193091: step 3378, loss 0.0111129, acc 1\n",
            "2019-01-22T06:36:40.876271: step 3379, loss 0.00533273, acc 1\n",
            "2019-01-22T06:36:41.554692: step 3380, loss 0.00588554, acc 1\n",
            "2019-01-22T06:36:42.236125: step 3381, loss 0.00118932, acc 1\n",
            "2019-01-22T06:36:42.922618: step 3382, loss 0.00147486, acc 1\n",
            "2019-01-22T06:36:43.607400: step 3383, loss 6.60487e-05, acc 1\n",
            "2019-01-22T06:36:44.287903: step 3384, loss 1.23143e-05, acc 1\n",
            "2019-01-22T06:36:44.972597: step 3385, loss 0.0119721, acc 0.984375\n",
            "2019-01-22T06:36:45.652459: step 3386, loss 0.000698363, acc 1\n",
            "2019-01-22T06:36:46.332243: step 3387, loss 0.000119677, acc 1\n",
            "2019-01-22T06:36:47.023680: step 3388, loss 0.00075604, acc 1\n",
            "2019-01-22T06:36:47.703470: step 3389, loss 0.000194516, acc 1\n",
            "2019-01-22T06:36:48.380657: step 3390, loss 0.000340601, acc 1\n",
            "2019-01-22T06:36:49.067773: step 3391, loss 7.87446e-05, acc 1\n",
            "2019-01-22T06:36:49.749322: step 3392, loss 0.00119108, acc 1\n",
            "2019-01-22T06:36:50.433435: step 3393, loss 1.24171e-05, acc 1\n",
            "2019-01-22T06:36:51.121595: step 3394, loss 0.000437362, acc 1\n",
            "2019-01-22T06:36:51.804829: step 3395, loss 7.97907e-06, acc 1\n",
            "2019-01-22T06:36:52.484821: step 3396, loss 0.000199517, acc 1\n",
            "2019-01-22T06:36:53.171984: step 3397, loss 1.04244e-05, acc 1\n",
            "2019-01-22T06:36:53.851621: step 3398, loss 0.000294457, acc 1\n",
            "2019-01-22T06:36:54.265601: step 3399, loss 2.56126e-06, acc 1\n",
            "2019-01-22T06:36:54.944109: step 3400, loss 0.0517318, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:36:55.302472: step 3400, loss 3.14896e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3400\n",
            "\n",
            "2019-01-22T06:36:56.122677: step 3401, loss 1.71498e-05, acc 1\n",
            "2019-01-22T06:36:56.800679: step 3402, loss 8.57884e-05, acc 1\n",
            "2019-01-22T06:36:57.477572: step 3403, loss 0.0129473, acc 0.984375\n",
            "2019-01-22T06:36:58.155574: step 3404, loss 0.00020614, acc 1\n",
            "2019-01-22T06:36:58.834435: step 3405, loss 0.00012279, acc 1\n",
            "2019-01-22T06:36:59.512986: step 3406, loss 7.64289e-05, acc 1\n",
            "2019-01-22T06:37:00.190055: step 3407, loss 0.000518126, acc 1\n",
            "2019-01-22T06:37:00.872928: step 3408, loss 0.000468344, acc 1\n",
            "2019-01-22T06:37:01.549582: step 3409, loss 0.011231, acc 0.984375\n",
            "2019-01-22T06:37:02.226943: step 3410, loss 4.63404e-06, acc 1\n",
            "2019-01-22T06:37:02.901989: step 3411, loss 4.13766e-05, acc 1\n",
            "2019-01-22T06:37:03.580834: step 3412, loss 3.67469e-06, acc 1\n",
            "2019-01-22T06:37:04.260017: step 3413, loss 0.00759913, acc 1\n",
            "2019-01-22T06:37:04.936603: step 3414, loss 6.22033e-06, acc 1\n",
            "2019-01-22T06:37:05.614481: step 3415, loss 7.75196e-06, acc 1\n",
            "2019-01-22T06:37:06.290259: step 3416, loss 0.000285634, acc 1\n",
            "2019-01-22T06:37:06.964560: step 3417, loss 0.0010564, acc 1\n",
            "2019-01-22T06:37:07.647556: step 3418, loss 8.2188e-05, acc 1\n",
            "2019-01-22T06:37:08.323932: step 3419, loss 7.55704e-05, acc 1\n",
            "2019-01-22T06:37:08.998927: step 3420, loss 2.67861e-05, acc 1\n",
            "2019-01-22T06:37:09.682931: step 3421, loss 7.48712e-06, acc 1\n",
            "2019-01-22T06:37:10.356403: step 3422, loss 1.44002e-05, acc 1\n",
            "2019-01-22T06:37:11.030101: step 3423, loss 0.000190801, acc 1\n",
            "2019-01-22T06:37:11.717797: step 3424, loss 2.14574e-06, acc 1\n",
            "2019-01-22T06:37:12.394531: step 3425, loss 0.000244732, acc 1\n",
            "2019-01-22T06:37:13.071324: step 3426, loss 0.000534038, acc 1\n",
            "2019-01-22T06:37:13.754976: step 3427, loss 0.000804506, acc 1\n",
            "2019-01-22T06:37:14.432921: step 3428, loss 1.29859e-05, acc 1\n",
            "2019-01-22T06:37:15.108997: step 3429, loss 7.29031e-05, acc 1\n",
            "2019-01-22T06:37:15.785121: step 3430, loss 0.00274258, acc 1\n",
            "2019-01-22T06:37:16.469588: step 3431, loss 0.000326717, acc 1\n",
            "2019-01-22T06:37:16.888824: step 3432, loss 1.35474e-05, acc 1\n",
            "2019-01-22T06:37:17.566261: step 3433, loss 9.12081e-06, acc 1\n",
            "2019-01-22T06:37:18.247049: step 3434, loss 4.17192e-06, acc 1\n",
            "2019-01-22T06:37:18.934610: step 3435, loss 2.49236e-05, acc 1\n",
            "2019-01-22T06:37:19.608681: step 3436, loss 4.77648e-05, acc 1\n",
            "2019-01-22T06:37:20.286250: step 3437, loss 0.0689453, acc 0.984375\n",
            "2019-01-22T06:37:20.968946: step 3438, loss 1.45843e-06, acc 1\n",
            "2019-01-22T06:37:21.651341: step 3439, loss 5.0732e-05, acc 1\n",
            "2019-01-22T06:37:22.329446: step 3440, loss 0.000257265, acc 1\n",
            "2019-01-22T06:37:23.014091: step 3441, loss 0.000103717, acc 1\n",
            "2019-01-22T06:37:23.692939: step 3442, loss 1.46703e-05, acc 1\n",
            "2019-01-22T06:37:24.372666: step 3443, loss 9.31034e-05, acc 1\n",
            "2019-01-22T06:37:25.058855: step 3444, loss 0.000289114, acc 1\n",
            "2019-01-22T06:37:25.738948: step 3445, loss 0.000494526, acc 1\n",
            "2019-01-22T06:37:26.419911: step 3446, loss 2.32952e-05, acc 1\n",
            "2019-01-22T06:37:27.104561: step 3447, loss 9.13912e-05, acc 1\n",
            "2019-01-22T06:37:27.788124: step 3448, loss 0.00106437, acc 1\n",
            "2019-01-22T06:37:28.470854: step 3449, loss 0.000154314, acc 1\n",
            "2019-01-22T06:37:29.152256: step 3450, loss 0.0237894, acc 0.984375\n",
            "2019-01-22T06:37:29.829210: step 3451, loss 4.20721e-05, acc 1\n",
            "2019-01-22T06:37:30.507946: step 3452, loss 4.3459e-05, acc 1\n",
            "2019-01-22T06:37:31.190741: step 3453, loss 0.000509272, acc 1\n",
            "2019-01-22T06:37:31.872534: step 3454, loss 0.0050719, acc 1\n",
            "2019-01-22T06:37:32.551302: step 3455, loss 3.4879e-05, acc 1\n",
            "2019-01-22T06:37:33.228634: step 3456, loss 0.000150126, acc 1\n",
            "2019-01-22T06:37:33.910845: step 3457, loss 0.0307016, acc 0.984375\n",
            "2019-01-22T06:37:34.590727: step 3458, loss 0.0156815, acc 0.984375\n",
            "2019-01-22T06:37:35.269442: step 3459, loss 0.00101926, acc 1\n",
            "2019-01-22T06:37:35.951949: step 3460, loss 0.000142742, acc 1\n",
            "2019-01-22T06:37:36.630588: step 3461, loss 0.0487156, acc 0.984375\n",
            "2019-01-22T06:37:37.307852: step 3462, loss 5.92196e-05, acc 1\n",
            "2019-01-22T06:37:37.987805: step 3463, loss 1.97471e-05, acc 1\n",
            "2019-01-22T06:37:38.663635: step 3464, loss 0.00242978, acc 1\n",
            "2019-01-22T06:37:39.076383: step 3465, loss 4.57732e-06, acc 1\n",
            "2019-01-22T06:37:39.754297: step 3466, loss 0.0924653, acc 0.96875\n",
            "2019-01-22T06:37:40.431088: step 3467, loss 0.000143151, acc 1\n",
            "2019-01-22T06:37:41.109753: step 3468, loss 0.0535167, acc 0.984375\n",
            "2019-01-22T06:37:41.783275: step 3469, loss 0.00888851, acc 1\n",
            "2019-01-22T06:37:42.465467: step 3470, loss 0.000264323, acc 1\n",
            "2019-01-22T06:37:43.148696: step 3471, loss 0.0606279, acc 0.984375\n",
            "2019-01-22T06:37:43.826148: step 3472, loss 0.000208379, acc 1\n",
            "2019-01-22T06:37:44.505913: step 3473, loss 0.000113108, acc 1\n",
            "2019-01-22T06:37:45.185324: step 3474, loss 3.84185e-05, acc 1\n",
            "2019-01-22T06:37:45.866895: step 3475, loss 2.12907e-05, acc 1\n",
            "2019-01-22T06:37:46.544842: step 3476, loss 2.25205e-05, acc 1\n",
            "2019-01-22T06:37:47.223915: step 3477, loss 0.00150218, acc 1\n",
            "2019-01-22T06:37:47.902681: step 3478, loss 3.55256e-05, acc 1\n",
            "2019-01-22T06:37:48.588864: step 3479, loss 0.000269247, acc 1\n",
            "2019-01-22T06:37:49.271214: step 3480, loss 5.43131e-06, acc 1\n",
            "2019-01-22T06:37:49.948434: step 3481, loss 0.000101097, acc 1\n",
            "2019-01-22T06:37:50.627524: step 3482, loss 5.11063e-05, acc 1\n",
            "2019-01-22T06:37:51.308673: step 3483, loss 0.0250324, acc 0.984375\n",
            "2019-01-22T06:37:51.992405: step 3484, loss 0.000312525, acc 1\n",
            "2019-01-22T06:37:52.669769: step 3485, loss 0.000185551, acc 1\n",
            "2019-01-22T06:37:53.346454: step 3486, loss 2.02079e-05, acc 1\n",
            "2019-01-22T06:37:54.027272: step 3487, loss 0.00071982, acc 1\n",
            "2019-01-22T06:37:54.706888: step 3488, loss 0.000271598, acc 1\n",
            "2019-01-22T06:37:55.383676: step 3489, loss 0.0128889, acc 0.984375\n",
            "2019-01-22T06:37:56.064222: step 3490, loss 0.000222584, acc 1\n",
            "2019-01-22T06:37:56.743035: step 3491, loss 0.00107691, acc 1\n",
            "2019-01-22T06:37:57.423057: step 3492, loss 4.76452e-06, acc 1\n",
            "2019-01-22T06:37:58.100425: step 3493, loss 0.000501144, acc 1\n",
            "2019-01-22T06:37:58.783001: step 3494, loss 0.00282546, acc 1\n",
            "2019-01-22T06:37:59.461922: step 3495, loss 1.24551e-05, acc 1\n",
            "2019-01-22T06:38:00.142699: step 3496, loss 0.0124683, acc 0.984375\n",
            "2019-01-22T06:38:00.823612: step 3497, loss 0.00057884, acc 1\n",
            "2019-01-22T06:38:01.236362: step 3498, loss 5.04404e-06, acc 1\n",
            "2019-01-22T06:38:01.913461: step 3499, loss 9.39093e-05, acc 1\n",
            "2019-01-22T06:38:02.589793: step 3500, loss 0.000105219, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:38:02.950779: step 3500, loss 3.57633e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3500\n",
            "\n",
            "2019-01-22T06:38:03.777110: step 3501, loss 0.000178075, acc 1\n",
            "2019-01-22T06:38:04.454451: step 3502, loss 0.00929572, acc 1\n",
            "2019-01-22T06:38:05.144755: step 3503, loss 0.00436001, acc 1\n",
            "2019-01-22T06:38:05.822756: step 3504, loss 0.0980028, acc 0.984375\n",
            "2019-01-22T06:38:06.500471: step 3505, loss 0.0359666, acc 0.984375\n",
            "2019-01-22T06:38:07.179759: step 3506, loss 3.53299e-05, acc 1\n",
            "2019-01-22T06:38:07.859547: step 3507, loss 6.21393e-05, acc 1\n",
            "2019-01-22T06:38:08.537027: step 3508, loss 4.42547e-06, acc 1\n",
            "2019-01-22T06:38:09.213595: step 3509, loss 0.0133116, acc 0.984375\n",
            "2019-01-22T06:38:09.892523: step 3510, loss 0.0010345, acc 1\n",
            "2019-01-22T06:38:10.572473: step 3511, loss 0.00315043, acc 1\n",
            "2019-01-22T06:38:11.250784: step 3512, loss 9.67201e-05, acc 1\n",
            "2019-01-22T06:38:11.934071: step 3513, loss 0.000793059, acc 1\n",
            "2019-01-22T06:38:12.613646: step 3514, loss 1.08602e-05, acc 1\n",
            "2019-01-22T06:38:13.289211: step 3515, loss 1.56217e-05, acc 1\n",
            "2019-01-22T06:38:13.970464: step 3516, loss 2.36224e-05, acc 1\n",
            "2019-01-22T06:38:14.651894: step 3517, loss 0.000794459, acc 1\n",
            "2019-01-22T06:38:15.329659: step 3518, loss 4.98804e-06, acc 1\n",
            "2019-01-22T06:38:16.007373: step 3519, loss 2.57624e-05, acc 1\n",
            "2019-01-22T06:38:16.688954: step 3520, loss 3.91754e-05, acc 1\n",
            "2019-01-22T06:38:17.376149: step 3521, loss 0.00494435, acc 1\n",
            "2019-01-22T06:38:18.061339: step 3522, loss 1.28981e-05, acc 1\n",
            "2019-01-22T06:38:18.739506: step 3523, loss 0.000648949, acc 1\n",
            "2019-01-22T06:38:19.417272: step 3524, loss 0.00071298, acc 1\n",
            "2019-01-22T06:38:20.098039: step 3525, loss 4.86298e-06, acc 1\n",
            "2019-01-22T06:38:20.774973: step 3526, loss 0.000119405, acc 1\n",
            "2019-01-22T06:38:21.455076: step 3527, loss 0.000101016, acc 1\n",
            "2019-01-22T06:38:22.131373: step 3528, loss 0.000126641, acc 1\n",
            "2019-01-22T06:38:22.813585: step 3529, loss 0.0131996, acc 0.984375\n",
            "2019-01-22T06:38:23.490576: step 3530, loss 0.000205945, acc 1\n",
            "2019-01-22T06:38:23.904682: step 3531, loss 2.67458e-05, acc 1\n",
            "2019-01-22T06:38:24.584211: step 3532, loss 0.000885696, acc 1\n",
            "2019-01-22T06:38:25.264612: step 3533, loss 3.0695e-05, acc 1\n",
            "2019-01-22T06:38:25.941570: step 3534, loss 0.000396731, acc 1\n",
            "2019-01-22T06:38:26.620451: step 3535, loss 6.38681e-05, acc 1\n",
            "2019-01-22T06:38:27.297771: step 3536, loss 6.92948e-05, acc 1\n",
            "2019-01-22T06:38:27.978041: step 3537, loss 9.78734e-06, acc 1\n",
            "2019-01-22T06:38:28.655481: step 3538, loss 8.28029e-05, acc 1\n",
            "2019-01-22T06:38:29.337695: step 3539, loss 1.54137e-05, acc 1\n",
            "2019-01-22T06:38:30.015916: step 3540, loss 1.38621e-05, acc 1\n",
            "2019-01-22T06:38:30.691825: step 3541, loss 0.0013579, acc 1\n",
            "2019-01-22T06:38:31.369148: step 3542, loss 1.17072e-05, acc 1\n",
            "2019-01-22T06:38:32.050685: step 3543, loss 6.42483e-05, acc 1\n",
            "2019-01-22T06:38:32.726616: step 3544, loss 0.000340755, acc 1\n",
            "2019-01-22T06:38:33.405439: step 3545, loss 0.00528807, acc 1\n",
            "2019-01-22T06:38:34.087810: step 3546, loss 0.000104021, acc 1\n",
            "2019-01-22T06:38:34.770091: step 3547, loss 1.77703e-05, acc 1\n",
            "2019-01-22T06:38:35.450290: step 3548, loss 3.41851e-05, acc 1\n",
            "2019-01-22T06:38:36.128750: step 3549, loss 2.82371e-05, acc 1\n",
            "2019-01-22T06:38:36.811585: step 3550, loss 3.11741e-05, acc 1\n",
            "2019-01-22T06:38:37.491760: step 3551, loss 0.000424759, acc 1\n",
            "2019-01-22T06:38:38.180512: step 3552, loss 0.00458591, acc 1\n",
            "2019-01-22T06:38:38.863341: step 3553, loss 1.74165e-05, acc 1\n",
            "2019-01-22T06:38:39.542308: step 3554, loss 0.0440964, acc 0.984375\n",
            "2019-01-22T06:38:40.231828: step 3555, loss 5.39556e-05, acc 1\n",
            "2019-01-22T06:38:40.909353: step 3556, loss 0.0173909, acc 0.984375\n",
            "2019-01-22T06:38:41.594447: step 3557, loss 2.45663e-05, acc 1\n",
            "2019-01-22T06:38:42.278794: step 3558, loss 0.00139279, acc 1\n",
            "2019-01-22T06:38:42.956857: step 3559, loss 0.000360829, acc 1\n",
            "2019-01-22T06:38:43.634462: step 3560, loss 0.00474588, acc 1\n",
            "2019-01-22T06:38:44.326972: step 3561, loss 2.04049e-05, acc 1\n",
            "2019-01-22T06:38:45.011691: step 3562, loss 8.96048e-06, acc 1\n",
            "2019-01-22T06:38:45.690881: step 3563, loss 3.34764e-05, acc 1\n",
            "2019-01-22T06:38:46.104476: step 3564, loss 3.69372e-05, acc 1\n",
            "2019-01-22T06:38:46.786981: step 3565, loss 5.68314e-05, acc 1\n",
            "2019-01-22T06:38:47.468597: step 3566, loss 0.00108319, acc 1\n",
            "2019-01-22T06:38:48.146968: step 3567, loss 8.7475e-05, acc 1\n",
            "2019-01-22T06:38:48.831386: step 3568, loss 0.000668415, acc 1\n",
            "2019-01-22T06:38:49.515326: step 3569, loss 2.99779e-05, acc 1\n",
            "2019-01-22T06:38:50.194461: step 3570, loss 1.70533e-05, acc 1\n",
            "2019-01-22T06:38:50.876075: step 3571, loss 8.2379e-06, acc 1\n",
            "2019-01-22T06:38:51.561933: step 3572, loss 1.23744e-05, acc 1\n",
            "2019-01-22T06:38:52.246707: step 3573, loss 0.00634332, acc 1\n",
            "2019-01-22T06:38:52.925718: step 3574, loss 0.000209486, acc 1\n",
            "2019-01-22T06:38:53.614527: step 3575, loss 0.000346156, acc 1\n",
            "2019-01-22T06:38:54.295365: step 3576, loss 1.79704e-05, acc 1\n",
            "2019-01-22T06:38:54.977253: step 3577, loss 0.000354869, acc 1\n",
            "2019-01-22T06:38:55.666920: step 3578, loss 0.00071904, acc 1\n",
            "2019-01-22T06:38:56.345693: step 3579, loss 0.000527786, acc 1\n",
            "2019-01-22T06:38:57.026915: step 3580, loss 0.00199426, acc 1\n",
            "2019-01-22T06:38:57.719393: step 3581, loss 0.00841841, acc 1\n",
            "2019-01-22T06:38:58.397440: step 3582, loss 1.14108e-05, acc 1\n",
            "2019-01-22T06:38:59.083499: step 3583, loss 0.00258799, acc 1\n",
            "2019-01-22T06:38:59.769996: step 3584, loss 8.70518e-05, acc 1\n",
            "2019-01-22T06:39:00.448902: step 3585, loss 0.000432653, acc 1\n",
            "2019-01-22T06:39:01.128183: step 3586, loss 0.00131776, acc 1\n",
            "2019-01-22T06:39:01.813834: step 3587, loss 1.11843e-05, acc 1\n",
            "2019-01-22T06:39:02.491255: step 3588, loss 0.000262614, acc 1\n",
            "2019-01-22T06:39:03.170485: step 3589, loss 0.00375486, acc 1\n",
            "2019-01-22T06:39:03.857645: step 3590, loss 0.000298401, acc 1\n",
            "2019-01-22T06:39:04.536674: step 3591, loss 0.014157, acc 0.984375\n",
            "2019-01-22T06:39:05.212225: step 3592, loss 0.000208968, acc 1\n",
            "2019-01-22T06:39:05.892421: step 3593, loss 0.000103748, acc 1\n",
            "2019-01-22T06:39:06.572302: step 3594, loss 1.99671e-05, acc 1\n",
            "2019-01-22T06:39:07.256312: step 3595, loss 1.71364e-05, acc 1\n",
            "2019-01-22T06:39:07.939317: step 3596, loss 0.0168158, acc 0.984375\n",
            "2019-01-22T06:39:08.351521: step 3597, loss 2.35006e-06, acc 1\n",
            "2019-01-22T06:39:09.039076: step 3598, loss 1.39882e-06, acc 1\n",
            "2019-01-22T06:39:09.720920: step 3599, loss 5.76162e-05, acc 1\n",
            "2019-01-22T06:39:10.398874: step 3600, loss 8.17619e-06, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:39:10.762228: step 3600, loss 2.4277e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3600\n",
            "\n",
            "2019-01-22T06:39:11.607147: step 3601, loss 0.00101362, acc 1\n",
            "2019-01-22T06:39:12.286929: step 3602, loss 1.28047e-05, acc 1\n",
            "2019-01-22T06:39:12.968192: step 3603, loss 0.000159614, acc 1\n",
            "2019-01-22T06:39:13.659299: step 3604, loss 8.03892e-05, acc 1\n",
            "2019-01-22T06:39:14.341009: step 3605, loss 0.000266538, acc 1\n",
            "2019-01-22T06:39:15.020664: step 3606, loss 7.78944e-05, acc 1\n",
            "2019-01-22T06:39:15.700286: step 3607, loss 0.00013714, acc 1\n",
            "2019-01-22T06:39:16.378491: step 3608, loss 1.52063e-05, acc 1\n",
            "2019-01-22T06:39:17.055950: step 3609, loss 0.00746819, acc 1\n",
            "2019-01-22T06:39:17.736499: step 3610, loss 0.0033431, acc 1\n",
            "2019-01-22T06:39:18.418533: step 3611, loss 0.0112147, acc 0.984375\n",
            "2019-01-22T06:39:19.097809: step 3612, loss 9.34319e-05, acc 1\n",
            "2019-01-22T06:39:19.775139: step 3613, loss 4.96912e-06, acc 1\n",
            "2019-01-22T06:39:20.458404: step 3614, loss 1.27215e-05, acc 1\n",
            "2019-01-22T06:39:21.134607: step 3615, loss 3.00301e-05, acc 1\n",
            "2019-01-22T06:39:21.812041: step 3616, loss 5.57998e-05, acc 1\n",
            "2019-01-22T06:39:22.490325: step 3617, loss 0.000632748, acc 1\n",
            "2019-01-22T06:39:23.177222: step 3618, loss 6.32671e-05, acc 1\n",
            "2019-01-22T06:39:23.856587: step 3619, loss 0.000199358, acc 1\n",
            "2019-01-22T06:39:24.534398: step 3620, loss 0.0772568, acc 0.984375\n",
            "2019-01-22T06:39:25.213295: step 3621, loss 9.0959e-05, acc 1\n",
            "2019-01-22T06:39:25.895023: step 3622, loss 2.62709e-05, acc 1\n",
            "2019-01-22T06:39:26.573850: step 3623, loss 0.000518829, acc 1\n",
            "2019-01-22T06:39:27.254676: step 3624, loss 6.76893e-05, acc 1\n",
            "2019-01-22T06:39:27.933135: step 3625, loss 2.9561e-05, acc 1\n",
            "2019-01-22T06:39:28.618565: step 3626, loss 0.000710914, acc 1\n",
            "2019-01-22T06:39:29.298626: step 3627, loss 0.00191661, acc 1\n",
            "2019-01-22T06:39:29.980980: step 3628, loss 0.00232183, acc 1\n",
            "2019-01-22T06:39:30.665039: step 3629, loss 0.000418776, acc 1\n",
            "2019-01-22T06:39:31.077004: step 3630, loss 4.97011e-05, acc 1\n",
            "2019-01-22T06:39:31.757721: step 3631, loss 0.000855935, acc 1\n",
            "2019-01-22T06:39:32.439667: step 3632, loss 0.00225797, acc 1\n",
            "2019-01-22T06:39:33.125503: step 3633, loss 5.24624e-05, acc 1\n",
            "2019-01-22T06:39:33.806684: step 3634, loss 0.00356728, acc 1\n",
            "2019-01-22T06:39:34.485860: step 3635, loss 6.4551e-05, acc 1\n",
            "2019-01-22T06:39:35.163418: step 3636, loss 0.000140016, acc 1\n",
            "2019-01-22T06:39:35.843878: step 3637, loss 0.00839871, acc 1\n",
            "2019-01-22T06:39:36.523763: step 3638, loss 0.000104381, acc 1\n",
            "2019-01-22T06:39:37.197868: step 3639, loss 0.000213583, acc 1\n",
            "2019-01-22T06:39:37.884686: step 3640, loss 3.65052e-05, acc 1\n",
            "2019-01-22T06:39:38.561434: step 3641, loss 0.000190625, acc 1\n",
            "2019-01-22T06:39:39.242754: step 3642, loss 0.000365937, acc 1\n",
            "2019-01-22T06:39:39.927378: step 3643, loss 8.12593e-05, acc 1\n",
            "2019-01-22T06:39:40.605419: step 3644, loss 0.00071099, acc 1\n",
            "2019-01-22T06:39:41.284400: step 3645, loss 0.035163, acc 0.984375\n",
            "2019-01-22T06:39:41.971279: step 3646, loss 0.00557427, acc 1\n",
            "2019-01-22T06:39:42.653039: step 3647, loss 0.0258151, acc 0.984375\n",
            "2019-01-22T06:39:43.326720: step 3648, loss 2.03879e-05, acc 1\n",
            "2019-01-22T06:39:44.010374: step 3649, loss 1.08096e-05, acc 1\n",
            "2019-01-22T06:39:44.687422: step 3650, loss 3.34881e-06, acc 1\n",
            "2019-01-22T06:39:45.367102: step 3651, loss 0.000551517, acc 1\n",
            "2019-01-22T06:39:46.045076: step 3652, loss 2.55922e-06, acc 1\n",
            "2019-01-22T06:39:46.722714: step 3653, loss 9.71068e-06, acc 1\n",
            "2019-01-22T06:39:47.396432: step 3654, loss 0.000139576, acc 1\n",
            "2019-01-22T06:39:48.077909: step 3655, loss 3.12544e-06, acc 1\n",
            "2019-01-22T06:39:48.753037: step 3656, loss 1.34629e-05, acc 1\n",
            "2019-01-22T06:39:49.426272: step 3657, loss 4.0233e-07, acc 1\n",
            "2019-01-22T06:39:50.104714: step 3658, loss 0.000354079, acc 1\n",
            "2019-01-22T06:39:50.783551: step 3659, loss 0.000388873, acc 1\n",
            "2019-01-22T06:39:51.462922: step 3660, loss 4.04446e-05, acc 1\n",
            "2019-01-22T06:39:52.138342: step 3661, loss 0.00872228, acc 1\n",
            "2019-01-22T06:39:52.817220: step 3662, loss 3.54634e-06, acc 1\n",
            "2019-01-22T06:39:53.234501: step 3663, loss 9.33233e-07, acc 1\n",
            "2019-01-22T06:39:53.913026: step 3664, loss 0.00336626, acc 1\n",
            "2019-01-22T06:39:54.590344: step 3665, loss 0.000113963, acc 1\n",
            "2019-01-22T06:39:55.265406: step 3666, loss 3.35448e-06, acc 1\n",
            "2019-01-22T06:39:55.939461: step 3667, loss 0.000851821, acc 1\n",
            "2019-01-22T06:39:56.621349: step 3668, loss 0.000408489, acc 1\n",
            "2019-01-22T06:39:57.297967: step 3669, loss 0.00101663, acc 1\n",
            "2019-01-22T06:39:57.971066: step 3670, loss 1.01015e-05, acc 1\n",
            "2019-01-22T06:39:58.646340: step 3671, loss 0.000647943, acc 1\n",
            "2019-01-22T06:39:59.327923: step 3672, loss 8.5078e-06, acc 1\n",
            "2019-01-22T06:40:00.003096: step 3673, loss 0.000294983, acc 1\n",
            "2019-01-22T06:40:00.679406: step 3674, loss 1.93695e-05, acc 1\n",
            "2019-01-22T06:40:01.356147: step 3675, loss 3.20355e-05, acc 1\n",
            "2019-01-22T06:40:02.036771: step 3676, loss 1.21783e-05, acc 1\n",
            "2019-01-22T06:40:02.714105: step 3677, loss 7.65053e-06, acc 1\n",
            "2019-01-22T06:40:03.390729: step 3678, loss 4.32099e-06, acc 1\n",
            "2019-01-22T06:40:04.070309: step 3679, loss 7.79665e-06, acc 1\n",
            "2019-01-22T06:40:04.750775: step 3680, loss 3.87406e-06, acc 1\n",
            "2019-01-22T06:40:05.431769: step 3681, loss 0.00332824, acc 1\n",
            "2019-01-22T06:40:06.108249: step 3682, loss 2.77843e-05, acc 1\n",
            "2019-01-22T06:40:06.785344: step 3683, loss 1.34116e-05, acc 1\n",
            "2019-01-22T06:40:07.464932: step 3684, loss 2.54275e-05, acc 1\n",
            "2019-01-22T06:40:08.143591: step 3685, loss 0.000149461, acc 1\n",
            "2019-01-22T06:40:08.816307: step 3686, loss 0.00376654, acc 1\n",
            "2019-01-22T06:40:09.493592: step 3687, loss 0.000182197, acc 1\n",
            "2019-01-22T06:40:10.175670: step 3688, loss 0.0230681, acc 0.984375\n",
            "2019-01-22T06:40:10.851916: step 3689, loss 0.000219824, acc 1\n",
            "2019-01-22T06:40:11.531791: step 3690, loss 4.4472e-05, acc 1\n",
            "2019-01-22T06:40:12.208361: step 3691, loss 4.73119e-05, acc 1\n",
            "2019-01-22T06:40:12.889373: step 3692, loss 3.23905e-06, acc 1\n",
            "2019-01-22T06:40:13.564483: step 3693, loss 0.0256949, acc 0.984375\n",
            "2019-01-22T06:40:14.238276: step 3694, loss 2.67326e-05, acc 1\n",
            "2019-01-22T06:40:14.922684: step 3695, loss 9.64082e-05, acc 1\n",
            "2019-01-22T06:40:15.333590: step 3696, loss 8.79375e-06, acc 1\n",
            "2019-01-22T06:40:16.010259: step 3697, loss 0.0010819, acc 1\n",
            "2019-01-22T06:40:16.689060: step 3698, loss 0.000420305, acc 1\n",
            "2019-01-22T06:40:17.364926: step 3699, loss 5.42e-05, acc 1\n",
            "2019-01-22T06:40:18.039468: step 3700, loss 2.71406e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:40:18.397389: step 3700, loss 4.20108e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3700\n",
            "\n",
            "2019-01-22T06:40:19.239745: step 3701, loss 2.57179e-05, acc 1\n",
            "2019-01-22T06:40:19.926083: step 3702, loss 6.76342e-05, acc 1\n",
            "2019-01-22T06:40:20.607767: step 3703, loss 0.0083624, acc 1\n",
            "2019-01-22T06:40:21.283443: step 3704, loss 0.00111064, acc 1\n",
            "2019-01-22T06:40:21.961977: step 3705, loss 0.000190816, acc 1\n",
            "2019-01-22T06:40:22.643269: step 3706, loss 0.000527784, acc 1\n",
            "2019-01-22T06:40:23.341801: step 3707, loss 7.09172e-05, acc 1\n",
            "2019-01-22T06:40:24.023331: step 3708, loss 0.000358632, acc 1\n",
            "2019-01-22T06:40:24.702965: step 3709, loss 4.32303e-06, acc 1\n",
            "2019-01-22T06:40:25.384987: step 3710, loss 0.000958978, acc 1\n",
            "2019-01-22T06:40:26.061814: step 3711, loss 0.00179566, acc 1\n",
            "2019-01-22T06:40:26.745343: step 3712, loss 1.77733e-05, acc 1\n",
            "2019-01-22T06:40:27.420939: step 3713, loss 0.0489627, acc 0.984375\n",
            "2019-01-22T06:40:28.103882: step 3714, loss 0.00159467, acc 1\n",
            "2019-01-22T06:40:28.781927: step 3715, loss 0.000189899, acc 1\n",
            "2019-01-22T06:40:29.464468: step 3716, loss 7.38266e-05, acc 1\n",
            "2019-01-22T06:40:30.141831: step 3717, loss 6.90793e-06, acc 1\n",
            "2019-01-22T06:40:30.819191: step 3718, loss 0.0410572, acc 0.984375\n",
            "2019-01-22T06:40:31.496861: step 3719, loss 0.000102683, acc 1\n",
            "2019-01-22T06:40:32.173863: step 3720, loss 0.00049425, acc 1\n",
            "2019-01-22T06:40:32.849710: step 3721, loss 3.03305e-05, acc 1\n",
            "2019-01-22T06:40:33.526374: step 3722, loss 9.0649e-05, acc 1\n",
            "2019-01-22T06:40:34.204292: step 3723, loss 0.00239483, acc 1\n",
            "2019-01-22T06:40:34.883296: step 3724, loss 7.01227e-05, acc 1\n",
            "2019-01-22T06:40:35.561388: step 3725, loss 0.000130968, acc 1\n",
            "2019-01-22T06:40:36.236931: step 3726, loss 0.00660192, acc 1\n",
            "2019-01-22T06:40:36.913482: step 3727, loss 7.41267e-05, acc 1\n",
            "2019-01-22T06:40:37.596990: step 3728, loss 0.000152414, acc 1\n",
            "2019-01-22T06:40:38.006857: step 3729, loss 0.00231058, acc 1\n",
            "2019-01-22T06:40:38.685089: step 3730, loss 0.00149846, acc 1\n",
            "2019-01-22T06:40:39.362464: step 3731, loss 0.00303111, acc 1\n",
            "2019-01-22T06:40:40.040597: step 3732, loss 0.00714289, acc 1\n",
            "2019-01-22T06:40:40.716379: step 3733, loss 3.93181e-06, acc 1\n",
            "2019-01-22T06:40:41.402000: step 3734, loss 0.000896429, acc 1\n",
            "2019-01-22T06:40:42.076442: step 3735, loss 0.000156053, acc 1\n",
            "2019-01-22T06:40:42.757223: step 3736, loss 0.000169858, acc 1\n",
            "2019-01-22T06:40:43.434485: step 3737, loss 0.000249974, acc 1\n",
            "2019-01-22T06:40:44.113515: step 3738, loss 0.000354793, acc 1\n",
            "2019-01-22T06:40:44.796886: step 3739, loss 1.42676e-06, acc 1\n",
            "2019-01-22T06:40:45.475385: step 3740, loss 0.0202265, acc 0.984375\n",
            "2019-01-22T06:40:46.152646: step 3741, loss 0.0446877, acc 0.984375\n",
            "2019-01-22T06:40:46.837429: step 3742, loss 9.38523e-06, acc 1\n",
            "2019-01-22T06:40:47.524017: step 3743, loss 0.000402833, acc 1\n",
            "2019-01-22T06:40:48.199235: step 3744, loss 2.13642e-06, acc 1\n",
            "2019-01-22T06:40:48.881952: step 3745, loss 0.000450205, acc 1\n",
            "2019-01-22T06:40:49.558434: step 3746, loss 0.00020099, acc 1\n",
            "2019-01-22T06:40:50.235379: step 3747, loss 3.41078e-05, acc 1\n",
            "2019-01-22T06:40:50.920890: step 3748, loss 0.00229936, acc 1\n",
            "2019-01-22T06:40:51.597493: step 3749, loss 2.43946e-05, acc 1\n",
            "2019-01-22T06:40:52.271833: step 3750, loss 1.37834e-06, acc 1\n",
            "2019-01-22T06:40:52.963234: step 3751, loss 0.000179059, acc 1\n",
            "2019-01-22T06:40:53.642505: step 3752, loss 7.61169e-05, acc 1\n",
            "2019-01-22T06:40:54.317680: step 3753, loss 4.91331e-06, acc 1\n",
            "2019-01-22T06:40:54.996058: step 3754, loss 0.000908009, acc 1\n",
            "2019-01-22T06:40:55.672483: step 3755, loss 1.71987e-05, acc 1\n",
            "2019-01-22T06:40:56.344583: step 3756, loss 0.000484886, acc 1\n",
            "2019-01-22T06:40:57.016400: step 3757, loss 2.16962e-05, acc 1\n",
            "2019-01-22T06:40:57.693285: step 3758, loss 3.02769e-05, acc 1\n",
            "2019-01-22T06:40:58.372132: step 3759, loss 0.00289223, acc 1\n",
            "2019-01-22T06:40:59.044845: step 3760, loss 9.11067e-05, acc 1\n",
            "2019-01-22T06:40:59.718444: step 3761, loss 0.00076239, acc 1\n",
            "2019-01-22T06:41:00.128248: step 3762, loss 0.000228511, acc 1\n",
            "2019-01-22T06:41:00.808744: step 3763, loss 0.000124404, acc 1\n",
            "2019-01-22T06:41:01.491079: step 3764, loss 0.0390934, acc 0.984375\n",
            "2019-01-22T06:41:02.165960: step 3765, loss 0.000690244, acc 1\n",
            "2019-01-22T06:41:02.844360: step 3766, loss 1.90684e-05, acc 1\n",
            "2019-01-22T06:41:03.517799: step 3767, loss 7.57282e-05, acc 1\n",
            "2019-01-22T06:41:04.193843: step 3768, loss 0.000150466, acc 1\n",
            "2019-01-22T06:41:04.874449: step 3769, loss 0.000409337, acc 1\n",
            "2019-01-22T06:41:05.551040: step 3770, loss 0.000116056, acc 1\n",
            "2019-01-22T06:41:06.223319: step 3771, loss 5.41288e-05, acc 1\n",
            "2019-01-22T06:41:06.903116: step 3772, loss 6.03466e-06, acc 1\n",
            "2019-01-22T06:41:07.578606: step 3773, loss 0.000112984, acc 1\n",
            "2019-01-22T06:41:08.250984: step 3774, loss 2.75499e-05, acc 1\n",
            "2019-01-22T06:41:08.928580: step 3775, loss 0.000142263, acc 1\n",
            "2019-01-22T06:41:09.602769: step 3776, loss 0.0245912, acc 0.984375\n",
            "2019-01-22T06:41:10.278334: step 3777, loss 0.00016485, acc 1\n",
            "2019-01-22T06:41:10.953182: step 3778, loss 0.000192169, acc 1\n",
            "2019-01-22T06:41:11.640576: step 3779, loss 0.000366994, acc 1\n",
            "2019-01-22T06:41:12.312955: step 3780, loss 0.000276648, acc 1\n",
            "2019-01-22T06:41:12.990803: step 3781, loss 0.000659014, acc 1\n",
            "2019-01-22T06:41:13.664886: step 3782, loss 6.26539e-06, acc 1\n",
            "2019-01-22T06:41:14.342003: step 3783, loss 3.14507e-05, acc 1\n",
            "2019-01-22T06:41:15.018097: step 3784, loss 0.000973274, acc 1\n",
            "2019-01-22T06:41:15.696433: step 3785, loss 0.000408162, acc 1\n",
            "2019-01-22T06:41:16.370334: step 3786, loss 0.00175155, acc 1\n",
            "2019-01-22T06:41:17.052806: step 3787, loss 0.000113147, acc 1\n",
            "2019-01-22T06:41:17.727483: step 3788, loss 0.000221514, acc 1\n",
            "2019-01-22T06:41:18.403262: step 3789, loss 1.18244e-05, acc 1\n",
            "2019-01-22T06:41:19.079897: step 3790, loss 5.48006e-05, acc 1\n",
            "2019-01-22T06:41:19.758888: step 3791, loss 0.0002877, acc 1\n",
            "2019-01-22T06:41:20.438141: step 3792, loss 0.000330547, acc 1\n",
            "2019-01-22T06:41:21.113972: step 3793, loss 5.47302e-05, acc 1\n",
            "2019-01-22T06:41:21.794554: step 3794, loss 3.54474e-05, acc 1\n",
            "2019-01-22T06:41:22.203597: step 3795, loss 0.05587, acc 0.971429\n",
            "2019-01-22T06:41:22.881765: step 3796, loss 1.64654e-06, acc 1\n",
            "2019-01-22T06:41:23.556458: step 3797, loss 0.000677403, acc 1\n",
            "2019-01-22T06:41:24.233120: step 3798, loss 0.000764978, acc 1\n",
            "2019-01-22T06:41:24.914371: step 3799, loss 0.00130436, acc 1\n",
            "2019-01-22T06:41:25.593325: step 3800, loss 1.77483e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:41:25.956037: step 3800, loss 0.000130639, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3800\n",
            "\n",
            "2019-01-22T06:41:26.783066: step 3801, loss 6.06676e-05, acc 1\n",
            "2019-01-22T06:41:27.463246: step 3802, loss 0.0243736, acc 0.984375\n",
            "2019-01-22T06:41:28.140341: step 3803, loss 9.0561e-05, acc 1\n",
            "2019-01-22T06:41:28.817614: step 3804, loss 0.0029436, acc 1\n",
            "2019-01-22T06:41:29.499814: step 3805, loss 0.00537189, acc 1\n",
            "2019-01-22T06:41:30.175435: step 3806, loss 0.000486815, acc 1\n",
            "2019-01-22T06:41:30.854740: step 3807, loss 1.07243e-05, acc 1\n",
            "2019-01-22T06:41:31.534346: step 3808, loss 0.0119328, acc 1\n",
            "2019-01-22T06:41:32.212986: step 3809, loss 3.01332e-05, acc 1\n",
            "2019-01-22T06:41:32.885786: step 3810, loss 0.0243301, acc 0.984375\n",
            "2019-01-22T06:41:33.565227: step 3811, loss 0.000659824, acc 1\n",
            "2019-01-22T06:41:34.241214: step 3812, loss 6.14615e-05, acc 1\n",
            "2019-01-22T06:41:34.916480: step 3813, loss 1.40927e-05, acc 1\n",
            "2019-01-22T06:41:35.593555: step 3814, loss 0.000269374, acc 1\n",
            "2019-01-22T06:41:36.265733: step 3815, loss 0.000276906, acc 1\n",
            "2019-01-22T06:41:36.945386: step 3816, loss 3.00252e-06, acc 1\n",
            "2019-01-22T06:41:37.618254: step 3817, loss 4.06393e-05, acc 1\n",
            "2019-01-22T06:41:38.303392: step 3818, loss 2.23752e-05, acc 1\n",
            "2019-01-22T06:41:38.980786: step 3819, loss 1.4454e-06, acc 1\n",
            "2019-01-22T06:41:39.652597: step 3820, loss 0.00554091, acc 1\n",
            "2019-01-22T06:41:40.334003: step 3821, loss 3.81749e-05, acc 1\n",
            "2019-01-22T06:41:41.005921: step 3822, loss 2.29795e-05, acc 1\n",
            "2019-01-22T06:41:41.683372: step 3823, loss 0.0822611, acc 0.984375\n",
            "2019-01-22T06:41:42.369973: step 3824, loss 0.000344161, acc 1\n",
            "2019-01-22T06:41:43.044933: step 3825, loss 2.93292e-05, acc 1\n",
            "2019-01-22T06:41:43.721707: step 3826, loss 8.78759e-05, acc 1\n",
            "2019-01-22T06:41:44.406003: step 3827, loss 0.000307173, acc 1\n",
            "2019-01-22T06:41:44.817662: step 3828, loss 8.41686e-05, acc 1\n",
            "2019-01-22T06:41:45.499281: step 3829, loss 0.199855, acc 0.984375\n",
            "2019-01-22T06:41:46.175905: step 3830, loss 0.0491835, acc 0.984375\n",
            "2019-01-22T06:41:46.850219: step 3831, loss 4.60229e-06, acc 1\n",
            "2019-01-22T06:41:47.531738: step 3832, loss 0.0818178, acc 0.984375\n",
            "2019-01-22T06:41:48.205912: step 3833, loss 0.00059093, acc 1\n",
            "2019-01-22T06:41:48.883902: step 3834, loss 0.000262784, acc 1\n",
            "2019-01-22T06:41:49.570013: step 3835, loss 6.93535e-05, acc 1\n",
            "2019-01-22T06:41:50.245970: step 3836, loss 5.68515e-05, acc 1\n",
            "2019-01-22T06:41:50.919181: step 3837, loss 0.00266872, acc 1\n",
            "2019-01-22T06:41:51.612627: step 3838, loss 2.64621e-05, acc 1\n",
            "2019-01-22T06:41:52.285388: step 3839, loss 0.0163915, acc 0.984375\n",
            "2019-01-22T06:41:52.960516: step 3840, loss 3.33379e-05, acc 1\n",
            "2019-01-22T06:41:53.641404: step 3841, loss 0.00279826, acc 1\n",
            "2019-01-22T06:41:54.316891: step 3842, loss 0.000738858, acc 1\n",
            "2019-01-22T06:41:54.992985: step 3843, loss 0.000201277, acc 1\n",
            "2019-01-22T06:41:55.671467: step 3844, loss 3.72709e-05, acc 1\n",
            "2019-01-22T06:41:56.348905: step 3845, loss 1.27087e-05, acc 1\n",
            "2019-01-22T06:41:57.025727: step 3846, loss 8.27371e-05, acc 1\n",
            "2019-01-22T06:41:57.699270: step 3847, loss 1.75779e-05, acc 1\n",
            "2019-01-22T06:41:58.377463: step 3848, loss 0.00226381, acc 1\n",
            "2019-01-22T06:41:59.050904: step 3849, loss 8.08521e-06, acc 1\n",
            "2019-01-22T06:41:59.728940: step 3850, loss 7.96709e-05, acc 1\n",
            "2019-01-22T06:42:00.405531: step 3851, loss 0.000444903, acc 1\n",
            "2019-01-22T06:42:01.081044: step 3852, loss 0.00369664, acc 1\n",
            "2019-01-22T06:42:01.756731: step 3853, loss 0.000806216, acc 1\n",
            "2019-01-22T06:42:02.433856: step 3854, loss 3.8163e-06, acc 1\n",
            "2019-01-22T06:42:03.111558: step 3855, loss 8.52394e-06, acc 1\n",
            "2019-01-22T06:42:03.788657: step 3856, loss 2.29656e-06, acc 1\n",
            "2019-01-22T06:42:04.462325: step 3857, loss 0.000931501, acc 1\n",
            "2019-01-22T06:42:05.140748: step 3858, loss 0.007217, acc 1\n",
            "2019-01-22T06:42:05.814511: step 3859, loss 0.000537816, acc 1\n",
            "2019-01-22T06:42:06.493355: step 3860, loss 6.62957e-05, acc 1\n",
            "2019-01-22T06:42:06.901005: step 3861, loss 4.38986e-05, acc 1\n",
            "2019-01-22T06:42:07.576018: step 3862, loss 0.000508654, acc 1\n",
            "2019-01-22T06:42:08.250713: step 3863, loss 3.37701e-05, acc 1\n",
            "2019-01-22T06:42:08.925666: step 3864, loss 0.000488003, acc 1\n",
            "2019-01-22T06:42:09.598862: step 3865, loss 3.94733e-05, acc 1\n",
            "2019-01-22T06:42:10.281386: step 3866, loss 0.00165362, acc 1\n",
            "2019-01-22T06:42:10.957038: step 3867, loss 1.4616e-05, acc 1\n",
            "2019-01-22T06:42:11.632295: step 3868, loss 0.000100998, acc 1\n",
            "2019-01-22T06:42:12.306022: step 3869, loss 1.95086e-05, acc 1\n",
            "2019-01-22T06:42:12.987059: step 3870, loss 3.89203e-05, acc 1\n",
            "2019-01-22T06:42:13.656875: step 3871, loss 1.38951e-06, acc 1\n",
            "2019-01-22T06:42:14.333554: step 3872, loss 1.82941e-05, acc 1\n",
            "2019-01-22T06:42:15.025257: step 3873, loss 3.68854e-05, acc 1\n",
            "2019-01-22T06:42:15.703005: step 3874, loss 3.02057e-05, acc 1\n",
            "2019-01-22T06:42:16.379647: step 3875, loss 0.00037386, acc 1\n",
            "2019-01-22T06:42:17.053886: step 3876, loss 3.1033e-05, acc 1\n",
            "2019-01-22T06:42:17.728884: step 3877, loss 0.00728417, acc 1\n",
            "2019-01-22T06:42:18.416642: step 3878, loss 7.18044e-05, acc 1\n",
            "2019-01-22T06:42:19.091419: step 3879, loss 0.000242599, acc 1\n",
            "2019-01-22T06:42:19.768470: step 3880, loss 0.000218213, acc 1\n",
            "2019-01-22T06:42:20.450755: step 3881, loss 0.000954536, acc 1\n",
            "2019-01-22T06:42:21.126120: step 3882, loss 1.9853e-05, acc 1\n",
            "2019-01-22T06:42:21.800447: step 3883, loss 0.000112599, acc 1\n",
            "2019-01-22T06:42:22.485265: step 3884, loss 0.000200843, acc 1\n",
            "2019-01-22T06:42:23.165296: step 3885, loss 0.000128663, acc 1\n",
            "2019-01-22T06:42:23.841951: step 3886, loss 0.00380697, acc 1\n",
            "2019-01-22T06:42:24.515113: step 3887, loss 1.05458e-05, acc 1\n",
            "2019-01-22T06:42:25.200058: step 3888, loss 2.76544e-05, acc 1\n",
            "2019-01-22T06:42:25.882595: step 3889, loss 1.63286e-05, acc 1\n",
            "2019-01-22T06:42:26.556677: step 3890, loss 0.000573367, acc 1\n",
            "2019-01-22T06:42:27.235503: step 3891, loss 0.00568155, acc 1\n",
            "2019-01-22T06:42:27.910104: step 3892, loss 5.11091e-05, acc 1\n",
            "2019-01-22T06:42:28.587881: step 3893, loss 0.00149048, acc 1\n",
            "2019-01-22T06:42:29.001437: step 3894, loss 0.0193788, acc 1\n",
            "2019-01-22T06:42:29.689678: step 3895, loss 0.000149831, acc 1\n",
            "2019-01-22T06:42:30.366242: step 3896, loss 0.000143703, acc 1\n",
            "2019-01-22T06:42:31.039452: step 3897, loss 1.26061e-05, acc 1\n",
            "2019-01-22T06:42:31.725697: step 3898, loss 0.00122196, acc 1\n",
            "2019-01-22T06:42:32.407538: step 3899, loss 0.000481919, acc 1\n",
            "2019-01-22T06:42:33.085920: step 3900, loss 0.000592226, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:42:33.447805: step 3900, loss 0.000223497, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-3900\n",
            "\n",
            "2019-01-22T06:42:34.274259: step 3901, loss 9.3696e-06, acc 1\n",
            "2019-01-22T06:42:34.955269: step 3902, loss 1.88189e-05, acc 1\n",
            "2019-01-22T06:42:35.634400: step 3903, loss 3.91171e-05, acc 1\n",
            "2019-01-22T06:42:36.313498: step 3904, loss 2.9737e-05, acc 1\n",
            "2019-01-22T06:42:36.992221: step 3905, loss 0.00111257, acc 1\n",
            "2019-01-22T06:42:37.670336: step 3906, loss 9.07841e-05, acc 1\n",
            "2019-01-22T06:42:38.347782: step 3907, loss 0.00025584, acc 1\n",
            "2019-01-22T06:42:39.026465: step 3908, loss 0.000658322, acc 1\n",
            "2019-01-22T06:42:39.702377: step 3909, loss 0.000294241, acc 1\n",
            "2019-01-22T06:42:40.379514: step 3910, loss 0.000512987, acc 1\n",
            "2019-01-22T06:42:41.067594: step 3911, loss 5.49046e-05, acc 1\n",
            "2019-01-22T06:42:41.743603: step 3912, loss 0.000333112, acc 1\n",
            "2019-01-22T06:42:42.425805: step 3913, loss 2.92435e-05, acc 1\n",
            "2019-01-22T06:42:43.112261: step 3914, loss 5.88566e-05, acc 1\n",
            "2019-01-22T06:42:43.793245: step 3915, loss 0.000304975, acc 1\n",
            "2019-01-22T06:42:44.470638: step 3916, loss 0.000237862, acc 1\n",
            "2019-01-22T06:42:45.152997: step 3917, loss 3.1201e-05, acc 1\n",
            "2019-01-22T06:42:45.835232: step 3918, loss 0.000966253, acc 1\n",
            "2019-01-22T06:42:46.517791: step 3919, loss 0.00013214, acc 1\n",
            "2019-01-22T06:42:47.208551: step 3920, loss 0.00205126, acc 1\n",
            "2019-01-22T06:42:47.888936: step 3921, loss 4.74202e-06, acc 1\n",
            "2019-01-22T06:42:48.568935: step 3922, loss 0.000127521, acc 1\n",
            "2019-01-22T06:42:49.259441: step 3923, loss 0.0749136, acc 0.984375\n",
            "2019-01-22T06:42:49.937041: step 3924, loss 0.00317774, acc 1\n",
            "2019-01-22T06:42:50.616151: step 3925, loss 0.000821488, acc 1\n",
            "2019-01-22T06:42:51.302618: step 3926, loss 2.02261e-05, acc 1\n",
            "2019-01-22T06:42:51.712776: step 3927, loss 0.00125953, acc 1\n",
            "2019-01-22T06:42:52.394435: step 3928, loss 2.18191e-05, acc 1\n",
            "2019-01-22T06:42:53.071018: step 3929, loss 7.81098e-05, acc 1\n",
            "2019-01-22T06:42:53.750603: step 3930, loss 3.64874e-05, acc 1\n",
            "2019-01-22T06:42:54.436566: step 3931, loss 0.000318915, acc 1\n",
            "2019-01-22T06:42:55.114967: step 3932, loss 0.00152216, acc 1\n",
            "2019-01-22T06:42:55.793562: step 3933, loss 0.000322164, acc 1\n",
            "2019-01-22T06:42:56.485275: step 3934, loss 9.0937e-06, acc 1\n",
            "2019-01-22T06:42:57.163655: step 3935, loss 0.000573925, acc 1\n",
            "2019-01-22T06:42:57.843628: step 3936, loss 1.94522e-05, acc 1\n",
            "2019-01-22T06:42:58.529765: step 3937, loss 5.28518e-05, acc 1\n",
            "2019-01-22T06:42:59.205401: step 3938, loss 4.3527e-05, acc 1\n",
            "2019-01-22T06:42:59.886480: step 3939, loss 0.0918441, acc 0.984375\n",
            "2019-01-22T06:43:00.571396: step 3940, loss 0.026832, acc 0.984375\n",
            "2019-01-22T06:43:01.245948: step 3941, loss 0.000213306, acc 1\n",
            "2019-01-22T06:43:01.922900: step 3942, loss 5.63468e-05, acc 1\n",
            "2019-01-22T06:43:02.613427: step 3943, loss 0.000314136, acc 1\n",
            "2019-01-22T06:43:03.294790: step 3944, loss 5.43922e-05, acc 1\n",
            "2019-01-22T06:43:03.975236: step 3945, loss 2.24492e-05, acc 1\n",
            "2019-01-22T06:43:04.666367: step 3946, loss 0.000586849, acc 1\n",
            "2019-01-22T06:43:05.346247: step 3947, loss 0.000424565, acc 1\n",
            "2019-01-22T06:43:06.024139: step 3948, loss 0.000250551, acc 1\n",
            "2019-01-22T06:43:06.711244: step 3949, loss 3.97698e-05, acc 1\n",
            "2019-01-22T06:43:07.390216: step 3950, loss 0.000503145, acc 1\n",
            "2019-01-22T06:43:08.066626: step 3951, loss 0.00326173, acc 1\n",
            "2019-01-22T06:43:08.751008: step 3952, loss 0.00403936, acc 1\n",
            "2019-01-22T06:43:09.435436: step 3953, loss 0.00236457, acc 1\n",
            "2019-01-22T06:43:10.119912: step 3954, loss 1.95812e-05, acc 1\n",
            "2019-01-22T06:43:10.805865: step 3955, loss 0.0020509, acc 1\n",
            "2019-01-22T06:43:11.487255: step 3956, loss 0.000608711, acc 1\n",
            "2019-01-22T06:43:12.164733: step 3957, loss 0.00168487, acc 1\n",
            "2019-01-22T06:43:12.847610: step 3958, loss 0.0177885, acc 0.984375\n",
            "2019-01-22T06:43:13.527987: step 3959, loss 1.12116e-05, acc 1\n",
            "2019-01-22T06:43:13.949582: step 3960, loss 7.9023e-05, acc 1\n",
            "2019-01-22T06:43:14.625645: step 3961, loss 0.000592104, acc 1\n",
            "2019-01-22T06:43:15.303781: step 3962, loss 4.56573e-05, acc 1\n",
            "2019-01-22T06:43:15.988718: step 3963, loss 1.22993e-05, acc 1\n",
            "2019-01-22T06:43:16.662568: step 3964, loss 5.04382e-06, acc 1\n",
            "2019-01-22T06:43:17.337777: step 3965, loss 0.000336983, acc 1\n",
            "2019-01-22T06:43:18.026428: step 3966, loss 0.00179726, acc 1\n",
            "2019-01-22T06:43:18.700864: step 3967, loss 5.96346e-06, acc 1\n",
            "2019-01-22T06:43:19.377469: step 3968, loss 3.25961e-07, acc 1\n",
            "2019-01-22T06:43:20.057738: step 3969, loss 1.2625e-05, acc 1\n",
            "2019-01-22T06:43:20.738402: step 3970, loss 0.000207903, acc 1\n",
            "2019-01-22T06:43:21.419138: step 3971, loss 0.000158445, acc 1\n",
            "2019-01-22T06:43:22.095897: step 3972, loss 4.15819e-05, acc 1\n",
            "2019-01-22T06:43:22.788007: step 3973, loss 0.00385691, acc 1\n",
            "2019-01-22T06:43:23.469279: step 3974, loss 0.00215026, acc 1\n",
            "2019-01-22T06:43:24.144530: step 3975, loss 0.000150319, acc 1\n",
            "2019-01-22T06:43:24.818019: step 3976, loss 5.22982e-06, acc 1\n",
            "2019-01-22T06:43:25.506706: step 3977, loss 0.000404369, acc 1\n",
            "2019-01-22T06:43:26.181849: step 3978, loss 0.000152855, acc 1\n",
            "2019-01-22T06:43:26.857578: step 3979, loss 7.96422e-05, acc 1\n",
            "2019-01-22T06:43:27.538172: step 3980, loss 0.000815963, acc 1\n",
            "2019-01-22T06:43:28.216470: step 3981, loss 1.52188e-05, acc 1\n",
            "2019-01-22T06:43:28.895340: step 3982, loss 0.000212512, acc 1\n",
            "2019-01-22T06:43:29.574911: step 3983, loss 0.00118925, acc 1\n",
            "2019-01-22T06:43:30.256692: step 3984, loss 0.000142035, acc 1\n",
            "2019-01-22T06:43:30.934882: step 3985, loss 2.87396e-06, acc 1\n",
            "2019-01-22T06:43:31.615209: step 3986, loss 8.77762e-06, acc 1\n",
            "2019-01-22T06:43:32.291377: step 3987, loss 0.000106816, acc 1\n",
            "2019-01-22T06:43:32.968146: step 3988, loss 4.72315e-05, acc 1\n",
            "2019-01-22T06:43:33.645655: step 3989, loss 8.60621e-06, acc 1\n",
            "2019-01-22T06:43:34.323728: step 3990, loss 9.27224e-05, acc 1\n",
            "2019-01-22T06:43:35.000739: step 3991, loss 6.96297e-05, acc 1\n",
            "2019-01-22T06:43:35.677249: step 3992, loss 0.0868011, acc 0.984375\n",
            "2019-01-22T06:43:36.091658: step 3993, loss 0.00323329, acc 1\n",
            "2019-01-22T06:43:36.770026: step 3994, loss 2.92435e-07, acc 1\n",
            "2019-01-22T06:43:37.451878: step 3995, loss 0.00113138, acc 1\n",
            "2019-01-22T06:43:38.131116: step 3996, loss 0.0291216, acc 0.984375\n",
            "2019-01-22T06:43:38.810578: step 3997, loss 2.7674e-05, acc 1\n",
            "2019-01-22T06:43:39.492767: step 3998, loss 1.24223e-05, acc 1\n",
            "2019-01-22T06:43:40.172320: step 3999, loss 0.000264084, acc 1\n",
            "2019-01-22T06:43:40.853263: step 4000, loss 0.000544113, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:43:41.217054: step 4000, loss 8.13597e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4000\n",
            "\n",
            "2019-01-22T06:43:42.065053: step 4001, loss 5.34577e-07, acc 1\n",
            "2019-01-22T06:43:42.750456: step 4002, loss 3.89048e-05, acc 1\n",
            "2019-01-22T06:43:43.430428: step 4003, loss 0.00016331, acc 1\n",
            "2019-01-22T06:43:44.110879: step 4004, loss 3.17979e-05, acc 1\n",
            "2019-01-22T06:43:44.800244: step 4005, loss 8.03733e-05, acc 1\n",
            "2019-01-22T06:43:45.482869: step 4006, loss 0.000666427, acc 1\n",
            "2019-01-22T06:43:46.160526: step 4007, loss 2.42321e-06, acc 1\n",
            "2019-01-22T06:43:46.847276: step 4008, loss 0.0010477, acc 1\n",
            "2019-01-22T06:43:47.530353: step 4009, loss 3.77216e-05, acc 1\n",
            "2019-01-22T06:43:48.206491: step 4010, loss 0.000226321, acc 1\n",
            "2019-01-22T06:43:48.900064: step 4011, loss 0.0110704, acc 0.984375\n",
            "2019-01-22T06:43:49.578405: step 4012, loss 0.000116642, acc 1\n",
            "2019-01-22T06:43:50.258134: step 4013, loss 2.40463e-06, acc 1\n",
            "2019-01-22T06:43:50.944517: step 4014, loss 8.57895e-05, acc 1\n",
            "2019-01-22T06:43:51.622402: step 4015, loss 4.15882e-06, acc 1\n",
            "2019-01-22T06:43:52.302653: step 4016, loss 4.08604e-05, acc 1\n",
            "2019-01-22T06:43:52.988755: step 4017, loss 4.7683e-05, acc 1\n",
            "2019-01-22T06:43:53.670497: step 4018, loss 0.0066117, acc 1\n",
            "2019-01-22T06:43:54.350275: step 4019, loss 1.90732e-06, acc 1\n",
            "2019-01-22T06:43:55.032550: step 4020, loss 1.41344e-05, acc 1\n",
            "2019-01-22T06:43:55.719739: step 4021, loss 6.40806e-05, acc 1\n",
            "2019-01-22T06:43:56.398858: step 4022, loss 0.000168506, acc 1\n",
            "2019-01-22T06:43:57.078884: step 4023, loss 0.000110968, acc 1\n",
            "2019-01-22T06:43:57.761604: step 4024, loss 0.00554701, acc 1\n",
            "2019-01-22T06:43:58.444395: step 4025, loss 8.18523e-06, acc 1\n",
            "2019-01-22T06:43:58.858876: step 4026, loss 2.50332e-06, acc 1\n",
            "2019-01-22T06:43:59.540448: step 4027, loss 0.0019172, acc 1\n",
            "2019-01-22T06:44:00.228822: step 4028, loss 0.000559067, acc 1\n",
            "2019-01-22T06:44:00.908271: step 4029, loss 0.0010879, acc 1\n",
            "2019-01-22T06:44:01.590299: step 4030, loss 6.76428e-06, acc 1\n",
            "2019-01-22T06:44:02.281298: step 4031, loss 0.00431165, acc 1\n",
            "2019-01-22T06:44:02.962242: step 4032, loss 0.00157845, acc 1\n",
            "2019-01-22T06:44:03.642055: step 4033, loss 0.0627409, acc 0.96875\n",
            "2019-01-22T06:44:04.329783: step 4034, loss 1.10842e-05, acc 1\n",
            "2019-01-22T06:44:05.009726: step 4035, loss 5.93033e-06, acc 1\n",
            "2019-01-22T06:44:05.692382: step 4036, loss 0.00149806, acc 1\n",
            "2019-01-22T06:44:06.385545: step 4037, loss 0.000165114, acc 1\n",
            "2019-01-22T06:44:07.064396: step 4038, loss 0.000282691, acc 1\n",
            "2019-01-22T06:44:07.746429: step 4039, loss 0.00015828, acc 1\n",
            "2019-01-22T06:44:08.433076: step 4040, loss 0.00107028, acc 1\n",
            "2019-01-22T06:44:09.132466: step 4041, loss 0.00323829, acc 1\n",
            "2019-01-22T06:44:09.811518: step 4042, loss 0.00291975, acc 1\n",
            "2019-01-22T06:44:10.503228: step 4043, loss 0.00677958, acc 1\n",
            "2019-01-22T06:44:11.188660: step 4044, loss 0.025299, acc 0.984375\n",
            "2019-01-22T06:44:11.871672: step 4045, loss 0.000202102, acc 1\n",
            "2019-01-22T06:44:12.558036: step 4046, loss 0.0126492, acc 0.984375\n",
            "2019-01-22T06:44:13.239969: step 4047, loss 0.000243687, acc 1\n",
            "2019-01-22T06:44:13.918788: step 4048, loss 6.08632e-05, acc 1\n",
            "2019-01-22T06:44:14.610359: step 4049, loss 0.00596593, acc 1\n",
            "2019-01-22T06:44:15.288531: step 4050, loss 1.36136e-05, acc 1\n",
            "2019-01-22T06:44:15.976861: step 4051, loss 0.000703224, acc 1\n",
            "2019-01-22T06:44:16.667280: step 4052, loss 0.000134976, acc 1\n",
            "2019-01-22T06:44:17.350214: step 4053, loss 0.000303906, acc 1\n",
            "2019-01-22T06:44:18.029643: step 4054, loss 0.000287569, acc 1\n",
            "2019-01-22T06:44:18.717134: step 4055, loss 9.2945e-05, acc 1\n",
            "2019-01-22T06:44:19.398074: step 4056, loss 1.7476e-05, acc 1\n",
            "2019-01-22T06:44:20.076705: step 4057, loss 1.33843e-05, acc 1\n",
            "2019-01-22T06:44:20.763909: step 4058, loss 0.000197602, acc 1\n",
            "2019-01-22T06:44:21.179099: step 4059, loss 6.31749e-06, acc 1\n",
            "2019-01-22T06:44:21.867637: step 4060, loss 0.000152279, acc 1\n",
            "2019-01-22T06:44:22.545655: step 4061, loss 7.20841e-07, acc 1\n",
            "2019-01-22T06:44:23.223871: step 4062, loss 0.0148735, acc 0.984375\n",
            "2019-01-22T06:44:23.915047: step 4063, loss 0.00954544, acc 1\n",
            "2019-01-22T06:44:24.594356: step 4064, loss 4.52839e-05, acc 1\n",
            "2019-01-22T06:44:25.276099: step 4065, loss 6.02954e-05, acc 1\n",
            "2019-01-22T06:44:25.964385: step 4066, loss 0.000139048, acc 1\n",
            "2019-01-22T06:44:26.645302: step 4067, loss 3.47026e-05, acc 1\n",
            "2019-01-22T06:44:27.323393: step 4068, loss 0.000376712, acc 1\n",
            "2019-01-22T06:44:28.009965: step 4069, loss 1.5656e-05, acc 1\n",
            "2019-01-22T06:44:28.688760: step 4070, loss 0.0216508, acc 0.984375\n",
            "2019-01-22T06:44:29.365818: step 4071, loss 1.20308e-05, acc 1\n",
            "2019-01-22T06:44:30.054889: step 4072, loss 0.0214217, acc 0.984375\n",
            "2019-01-22T06:44:30.734515: step 4073, loss 0.00195003, acc 1\n",
            "2019-01-22T06:44:31.417082: step 4074, loss 0.000369423, acc 1\n",
            "2019-01-22T06:44:32.105296: step 4075, loss 0.00741626, acc 1\n",
            "2019-01-22T06:44:32.784255: step 4076, loss 0.000642591, acc 1\n",
            "2019-01-22T06:44:33.460895: step 4077, loss 0.0300439, acc 0.984375\n",
            "2019-01-22T06:44:34.148803: step 4078, loss 0.000189049, acc 1\n",
            "2019-01-22T06:44:34.834016: step 4079, loss 1.17744e-05, acc 1\n",
            "2019-01-22T06:44:35.512207: step 4080, loss 0.000641074, acc 1\n",
            "2019-01-22T06:44:36.199674: step 4081, loss 0.0002576, acc 1\n",
            "2019-01-22T06:44:36.881070: step 4082, loss 0.0693947, acc 0.984375\n",
            "2019-01-22T06:44:37.556616: step 4083, loss 1.20174e-05, acc 1\n",
            "2019-01-22T06:44:38.236083: step 4084, loss 0.000233984, acc 1\n",
            "2019-01-22T06:44:38.917713: step 4085, loss 6.21544e-06, acc 1\n",
            "2019-01-22T06:44:39.600000: step 4086, loss 0.00510343, acc 1\n",
            "2019-01-22T06:44:40.280804: step 4087, loss 0.000181174, acc 1\n",
            "2019-01-22T06:44:40.959525: step 4088, loss 0.00724781, acc 1\n",
            "2019-01-22T06:44:41.638720: step 4089, loss 0.000211899, acc 1\n",
            "2019-01-22T06:44:42.324093: step 4090, loss 2.87994e-05, acc 1\n",
            "2019-01-22T06:44:43.002397: step 4091, loss 0.000215835, acc 1\n",
            "2019-01-22T06:44:43.425526: step 4092, loss 0.0902556, acc 0.971429\n",
            "2019-01-22T06:44:44.106129: step 4093, loss 0.0822472, acc 0.984375\n",
            "2019-01-22T06:44:44.783970: step 4094, loss 9.21998e-07, acc 1\n",
            "2019-01-22T06:44:45.470866: step 4095, loss 0.000464485, acc 1\n",
            "2019-01-22T06:44:46.146907: step 4096, loss 0.000671191, acc 1\n",
            "2019-01-22T06:44:46.824514: step 4097, loss 0.00777007, acc 1\n",
            "2019-01-22T06:44:47.516749: step 4098, loss 0.00500155, acc 1\n",
            "2019-01-22T06:44:48.195406: step 4099, loss 0.00960296, acc 1\n",
            "2019-01-22T06:44:48.870932: step 4100, loss 0.014613, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:44:49.225651: step 4100, loss 0.0143439, acc 0.991342\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4100\n",
            "\n",
            "2019-01-22T06:44:50.065649: step 4101, loss 0.00421689, acc 1\n",
            "2019-01-22T06:44:50.742971: step 4102, loss 0.0515993, acc 0.984375\n",
            "2019-01-22T06:44:51.419190: step 4103, loss 0.0234531, acc 1\n",
            "2019-01-22T06:44:52.096849: step 4104, loss 2.2308e-05, acc 1\n",
            "2019-01-22T06:44:52.773017: step 4105, loss 0.00144106, acc 1\n",
            "2019-01-22T06:44:53.451966: step 4106, loss 0.0123925, acc 0.984375\n",
            "2019-01-22T06:44:54.129225: step 4107, loss 3.95754e-05, acc 1\n",
            "2019-01-22T06:44:54.810513: step 4108, loss 0.0028726, acc 1\n",
            "2019-01-22T06:44:55.487821: step 4109, loss 3.3282e-05, acc 1\n",
            "2019-01-22T06:44:56.168682: step 4110, loss 0.00198254, acc 1\n",
            "2019-01-22T06:44:56.853299: step 4111, loss 0.00131103, acc 1\n",
            "2019-01-22T06:44:57.535419: step 4112, loss 0.0453494, acc 0.984375\n",
            "2019-01-22T06:44:58.215442: step 4113, loss 5.59327e-05, acc 1\n",
            "2019-01-22T06:44:58.902094: step 4114, loss 0.012106, acc 1\n",
            "2019-01-22T06:44:59.583518: step 4115, loss 0.002989, acc 1\n",
            "2019-01-22T06:45:00.262742: step 4116, loss 0.00478951, acc 1\n",
            "2019-01-22T06:45:00.949658: step 4117, loss 0.000114184, acc 1\n",
            "2019-01-22T06:45:01.634291: step 4118, loss 0.0499482, acc 0.984375\n",
            "2019-01-22T06:45:02.312487: step 4119, loss 0.000196641, acc 1\n",
            "2019-01-22T06:45:03.001045: step 4120, loss 0.000108803, acc 1\n",
            "2019-01-22T06:45:03.677972: step 4121, loss 7.24851e-06, acc 1\n",
            "2019-01-22T06:45:04.354146: step 4122, loss 3.45433e-05, acc 1\n",
            "2019-01-22T06:45:05.037362: step 4123, loss 1.21241e-05, acc 1\n",
            "2019-01-22T06:45:05.720525: step 4124, loss 1.16363e-05, acc 1\n",
            "2019-01-22T06:45:06.132068: step 4125, loss 0.000578755, acc 1\n",
            "2019-01-22T06:45:06.810430: step 4126, loss 6.24067e-05, acc 1\n",
            "2019-01-22T06:45:07.489296: step 4127, loss 2.19414e-06, acc 1\n",
            "2019-01-22T06:45:08.173028: step 4128, loss 0.00115866, acc 1\n",
            "2019-01-22T06:45:08.849974: step 4129, loss 0.000201249, acc 1\n",
            "2019-01-22T06:45:09.531323: step 4130, loss 5.04422e-05, acc 1\n",
            "2019-01-22T06:45:10.215890: step 4131, loss 2.13123e-05, acc 1\n",
            "2019-01-22T06:45:10.893580: step 4132, loss 0.000192041, acc 1\n",
            "2019-01-22T06:45:11.571077: step 4133, loss 2.67656e-06, acc 1\n",
            "2019-01-22T06:45:12.261693: step 4134, loss 2.21563e-05, acc 1\n",
            "2019-01-22T06:45:12.941599: step 4135, loss 0.000241923, acc 1\n",
            "2019-01-22T06:45:13.619893: step 4136, loss 0.0119111, acc 0.984375\n",
            "2019-01-22T06:45:14.306913: step 4137, loss 1.95577e-07, acc 1\n",
            "2019-01-22T06:45:14.984423: step 4138, loss 1.04491e-06, acc 1\n",
            "2019-01-22T06:45:15.665649: step 4139, loss 6.42281e-05, acc 1\n",
            "2019-01-22T06:45:16.355640: step 4140, loss 3.54253e-05, acc 1\n",
            "2019-01-22T06:45:17.035987: step 4141, loss 2.14993e-05, acc 1\n",
            "2019-01-22T06:45:17.719801: step 4142, loss 1.9664e-05, acc 1\n",
            "2019-01-22T06:45:18.405104: step 4143, loss 3.80147e-06, acc 1\n",
            "2019-01-22T06:45:19.078352: step 4144, loss 8.25822e-05, acc 1\n",
            "2019-01-22T06:45:19.755227: step 4145, loss 2.43064e-05, acc 1\n",
            "2019-01-22T06:45:20.439328: step 4146, loss 5.29345e-06, acc 1\n",
            "2019-01-22T06:45:21.118528: step 4147, loss 0.000124704, acc 1\n",
            "2019-01-22T06:45:21.795801: step 4148, loss 0.000362151, acc 1\n",
            "2019-01-22T06:45:22.474794: step 4149, loss 8.08397e-06, acc 1\n",
            "2019-01-22T06:45:23.151900: step 4150, loss 2.36852e-05, acc 1\n",
            "2019-01-22T06:45:23.831273: step 4151, loss 1.05177e-05, acc 1\n",
            "2019-01-22T06:45:24.510860: step 4152, loss 0.0345388, acc 0.984375\n",
            "2019-01-22T06:45:25.190551: step 4153, loss 4.09744e-05, acc 1\n",
            "2019-01-22T06:45:25.871420: step 4154, loss 0.000146206, acc 1\n",
            "2019-01-22T06:45:26.549847: step 4155, loss 0.000209358, acc 1\n",
            "2019-01-22T06:45:27.227315: step 4156, loss 4.59656e-05, acc 1\n",
            "2019-01-22T06:45:27.911567: step 4157, loss 0.000103171, acc 1\n",
            "2019-01-22T06:45:28.325666: step 4158, loss 0.00376882, acc 1\n",
            "2019-01-22T06:45:29.003022: step 4159, loss 0.083595, acc 0.984375\n",
            "2019-01-22T06:45:29.685438: step 4160, loss 1.68477e-05, acc 1\n",
            "2019-01-22T06:45:30.366840: step 4161, loss 4.20892e-05, acc 1\n",
            "2019-01-22T06:45:31.041742: step 4162, loss 3.60783e-06, acc 1\n",
            "2019-01-22T06:45:31.722425: step 4163, loss 0.000280541, acc 1\n",
            "2019-01-22T06:45:32.398014: step 4164, loss 0.000914918, acc 1\n",
            "2019-01-22T06:45:33.077061: step 4165, loss 1.80853e-05, acc 1\n",
            "2019-01-22T06:45:33.755032: step 4166, loss 0.00211683, acc 1\n",
            "2019-01-22T06:45:34.436542: step 4167, loss 0.000434808, acc 1\n",
            "2019-01-22T06:45:35.115823: step 4168, loss 0.000424459, acc 1\n",
            "2019-01-22T06:45:35.790997: step 4169, loss 0.00221687, acc 1\n",
            "2019-01-22T06:45:36.470506: step 4170, loss 1.10252e-05, acc 1\n",
            "2019-01-22T06:45:37.159558: step 4171, loss 0.00420755, acc 1\n",
            "2019-01-22T06:45:37.836579: step 4172, loss 3.28371e-06, acc 1\n",
            "2019-01-22T06:45:38.516375: step 4173, loss 1.78437e-06, acc 1\n",
            "2019-01-22T06:45:39.192621: step 4174, loss 2.78308e-05, acc 1\n",
            "2019-01-22T06:45:39.869667: step 4175, loss 1.50524e-05, acc 1\n",
            "2019-01-22T06:45:40.547699: step 4176, loss 5.38977e-06, acc 1\n",
            "2019-01-22T06:45:41.225700: step 4177, loss 0.0596056, acc 0.984375\n",
            "2019-01-22T06:45:41.905814: step 4178, loss 7.03646e-05, acc 1\n",
            "2019-01-22T06:45:42.587522: step 4179, loss 0.000230832, acc 1\n",
            "2019-01-22T06:45:43.266075: step 4180, loss 0.000157484, acc 1\n",
            "2019-01-22T06:45:43.943076: step 4181, loss 0.000168085, acc 1\n",
            "2019-01-22T06:45:44.622959: step 4182, loss 4.66391e-06, acc 1\n",
            "2019-01-22T06:45:45.302051: step 4183, loss 0.0063069, acc 1\n",
            "2019-01-22T06:45:45.983217: step 4184, loss 0.00123453, acc 1\n",
            "2019-01-22T06:45:46.660599: step 4185, loss 3.51005e-05, acc 1\n",
            "2019-01-22T06:45:47.339251: step 4186, loss 3.79474e-05, acc 1\n",
            "2019-01-22T06:45:48.018726: step 4187, loss 5.27875e-05, acc 1\n",
            "2019-01-22T06:45:48.701831: step 4188, loss 0.000333386, acc 1\n",
            "2019-01-22T06:45:49.379469: step 4189, loss 0.0165428, acc 0.984375\n",
            "2019-01-22T06:45:50.055993: step 4190, loss 0.00289569, acc 1\n",
            "2019-01-22T06:45:50.471578: step 4191, loss 1.43644e-05, acc 1\n",
            "2019-01-22T06:45:51.152884: step 4192, loss 0.000110049, acc 1\n",
            "2019-01-22T06:45:51.829680: step 4193, loss 0.0388305, acc 0.984375\n",
            "2019-01-22T06:45:52.509206: step 4194, loss 4.9963e-05, acc 1\n",
            "2019-01-22T06:45:53.187935: step 4195, loss 1.55732e-05, acc 1\n",
            "2019-01-22T06:45:53.864776: step 4196, loss 1.88719e-05, acc 1\n",
            "2019-01-22T06:45:54.541345: step 4197, loss 2.35627e-05, acc 1\n",
            "2019-01-22T06:45:55.219352: step 4198, loss 0.0261032, acc 0.984375\n",
            "2019-01-22T06:45:55.898681: step 4199, loss 0.0097944, acc 1\n",
            "2019-01-22T06:45:56.575766: step 4200, loss 2.50933e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:45:56.935747: step 4200, loss 0.00010857, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4200\n",
            "\n",
            "2019-01-22T06:45:57.789524: step 4201, loss 1.78574e-05, acc 1\n",
            "2019-01-22T06:45:58.466137: step 4202, loss 0.000236499, acc 1\n",
            "2019-01-22T06:45:59.145233: step 4203, loss 3.8583e-05, acc 1\n",
            "2019-01-22T06:45:59.822081: step 4204, loss 1.17134e-05, acc 1\n",
            "2019-01-22T06:46:00.502639: step 4205, loss 6.15014e-05, acc 1\n",
            "2019-01-22T06:46:01.181707: step 4206, loss 0.000123734, acc 1\n",
            "2019-01-22T06:46:01.857608: step 4207, loss 0.000378488, acc 1\n",
            "2019-01-22T06:46:02.538278: step 4208, loss 8.5842e-06, acc 1\n",
            "2019-01-22T06:46:03.217847: step 4209, loss 0.0759916, acc 0.984375\n",
            "2019-01-22T06:46:03.900013: step 4210, loss 0.000455198, acc 1\n",
            "2019-01-22T06:46:04.575090: step 4211, loss 0.0318765, acc 0.984375\n",
            "2019-01-22T06:46:05.252638: step 4212, loss 0.000877061, acc 1\n",
            "2019-01-22T06:46:05.927695: step 4213, loss 0.00155121, acc 1\n",
            "2019-01-22T06:46:06.601174: step 4214, loss 1.34427e-05, acc 1\n",
            "2019-01-22T06:46:07.281113: step 4215, loss 3.0755e-05, acc 1\n",
            "2019-01-22T06:46:07.954851: step 4216, loss 0.000188399, acc 1\n",
            "2019-01-22T06:46:08.629586: step 4217, loss 7.28681e-05, acc 1\n",
            "2019-01-22T06:46:09.304901: step 4218, loss 0.000101241, acc 1\n",
            "2019-01-22T06:46:09.981297: step 4219, loss 0.000310573, acc 1\n",
            "2019-01-22T06:46:10.655097: step 4220, loss 0.00256888, acc 1\n",
            "2019-01-22T06:46:11.333400: step 4221, loss 5.80295e-05, acc 1\n",
            "2019-01-22T06:46:12.013120: step 4222, loss 5.61169e-05, acc 1\n",
            "2019-01-22T06:46:12.694356: step 4223, loss 0.00940315, acc 1\n",
            "2019-01-22T06:46:13.105375: step 4224, loss 3.00738e-06, acc 1\n",
            "2019-01-22T06:46:13.780902: step 4225, loss 0.000830575, acc 1\n",
            "2019-01-22T06:46:14.450654: step 4226, loss 0.000342045, acc 1\n",
            "2019-01-22T06:46:15.127917: step 4227, loss 1.37023e-05, acc 1\n",
            "2019-01-22T06:46:15.806550: step 4228, loss 5.0735e-05, acc 1\n",
            "2019-01-22T06:46:16.484947: step 4229, loss 6.9286e-05, acc 1\n",
            "2019-01-22T06:46:17.166646: step 4230, loss 0.0941259, acc 0.984375\n",
            "2019-01-22T06:46:17.842973: step 4231, loss 7.55897e-06, acc 1\n",
            "2019-01-22T06:46:18.525135: step 4232, loss 0.0139809, acc 0.984375\n",
            "2019-01-22T06:46:19.205583: step 4233, loss 0.000337997, acc 1\n",
            "2019-01-22T06:46:19.879749: step 4234, loss 0.000224494, acc 1\n",
            "2019-01-22T06:46:20.553434: step 4235, loss 3.43499e-05, acc 1\n",
            "2019-01-22T06:46:21.245299: step 4236, loss 5.36081e-05, acc 1\n",
            "2019-01-22T06:46:21.920151: step 4237, loss 1.64467e-06, acc 1\n",
            "2019-01-22T06:46:22.601814: step 4238, loss 0.0236249, acc 0.984375\n",
            "2019-01-22T06:46:23.288309: step 4239, loss 0.00108173, acc 1\n",
            "2019-01-22T06:46:23.964434: step 4240, loss 0.000706763, acc 1\n",
            "2019-01-22T06:46:24.645188: step 4241, loss 4.4869e-06, acc 1\n",
            "2019-01-22T06:46:25.328810: step 4242, loss 5.24685e-06, acc 1\n",
            "2019-01-22T06:46:26.000802: step 4243, loss 0.00100626, acc 1\n",
            "2019-01-22T06:46:26.675685: step 4244, loss 3.46009e-05, acc 1\n",
            "2019-01-22T06:46:27.349595: step 4245, loss 1.26381e-05, acc 1\n",
            "2019-01-22T06:46:28.030532: step 4246, loss 4.43661e-06, acc 1\n",
            "2019-01-22T06:46:28.704325: step 4247, loss 0.0119276, acc 0.984375\n",
            "2019-01-22T06:46:29.381967: step 4248, loss 5.03988e-06, acc 1\n",
            "2019-01-22T06:46:30.055433: step 4249, loss 0.000551407, acc 1\n",
            "2019-01-22T06:46:30.732011: step 4250, loss 1.49851e-05, acc 1\n",
            "2019-01-22T06:46:31.407142: step 4251, loss 4.37685e-05, acc 1\n",
            "2019-01-22T06:46:32.091029: step 4252, loss 4.29841e-05, acc 1\n",
            "2019-01-22T06:46:32.767670: step 4253, loss 0.000335929, acc 1\n",
            "2019-01-22T06:46:33.445140: step 4254, loss 0.000494456, acc 1\n",
            "2019-01-22T06:46:34.128839: step 4255, loss 1.07446e-05, acc 1\n",
            "2019-01-22T06:46:34.805009: step 4256, loss 1.6574e-05, acc 1\n",
            "2019-01-22T06:46:35.215021: step 4257, loss 0.000765075, acc 1\n",
            "2019-01-22T06:46:35.889970: step 4258, loss 0.000308389, acc 1\n",
            "2019-01-22T06:46:36.566377: step 4259, loss 6.31687e-05, acc 1\n",
            "2019-01-22T06:46:37.241043: step 4260, loss 1.54137e-05, acc 1\n",
            "2019-01-22T06:46:37.918414: step 4261, loss 0.12379, acc 0.984375\n",
            "2019-01-22T06:46:38.595303: step 4262, loss 0.00382723, acc 1\n",
            "2019-01-22T06:46:39.269498: step 4263, loss 0.0006287, acc 1\n",
            "2019-01-22T06:46:39.945324: step 4264, loss 0.0027794, acc 1\n",
            "2019-01-22T06:46:40.626596: step 4265, loss 0.0188971, acc 0.984375\n",
            "2019-01-22T06:46:41.303324: step 4266, loss 5.47227e-05, acc 1\n",
            "2019-01-22T06:46:41.981976: step 4267, loss 0.0817882, acc 0.984375\n",
            "2019-01-22T06:46:42.660023: step 4268, loss 9.04297e-05, acc 1\n",
            "2019-01-22T06:46:43.332739: step 4269, loss 0.00985831, acc 1\n",
            "2019-01-22T06:46:44.003445: step 4270, loss 0.000179587, acc 1\n",
            "2019-01-22T06:46:44.676947: step 4271, loss 0.0299904, acc 0.984375\n",
            "2019-01-22T06:46:45.352913: step 4272, loss 0.00012915, acc 1\n",
            "2019-01-22T06:46:46.025967: step 4273, loss 8.69607e-06, acc 1\n",
            "2019-01-22T06:46:46.700100: step 4274, loss 0.0118479, acc 0.984375\n",
            "2019-01-22T06:46:47.374599: step 4275, loss 0.0125094, acc 0.984375\n",
            "2019-01-22T06:46:48.050474: step 4276, loss 0.00269384, acc 1\n",
            "2019-01-22T06:46:48.725973: step 4277, loss 2.97622e-05, acc 1\n",
            "2019-01-22T06:46:49.399420: step 4278, loss 0.000219853, acc 1\n",
            "2019-01-22T06:46:50.078171: step 4279, loss 1.80934e-05, acc 1\n",
            "2019-01-22T06:46:50.753926: step 4280, loss 0.000254603, acc 1\n",
            "2019-01-22T06:46:51.424809: step 4281, loss 0.00453957, acc 1\n",
            "2019-01-22T06:46:52.104872: step 4282, loss 2.68943e-05, acc 1\n",
            "2019-01-22T06:46:52.785045: step 4283, loss 0.0092986, acc 1\n",
            "2019-01-22T06:46:53.460763: step 4284, loss 1.43886e-05, acc 1\n",
            "2019-01-22T06:46:54.144186: step 4285, loss 0.00036996, acc 1\n",
            "2019-01-22T06:46:54.817547: step 4286, loss 8.37713e-05, acc 1\n",
            "2019-01-22T06:46:55.492604: step 4287, loss 5.63301e-05, acc 1\n",
            "2019-01-22T06:46:56.181927: step 4288, loss 0.0142445, acc 0.984375\n",
            "2019-01-22T06:46:56.854214: step 4289, loss 0.000277117, acc 1\n",
            "2019-01-22T06:46:57.266301: step 4290, loss 0.000104768, acc 1\n",
            "2019-01-22T06:46:57.944687: step 4291, loss 0.000381089, acc 1\n",
            "2019-01-22T06:46:58.618033: step 4292, loss 4.31552e-06, acc 1\n",
            "2019-01-22T06:46:59.303657: step 4293, loss 2.77518e-06, acc 1\n",
            "2019-01-22T06:46:59.984009: step 4294, loss 4.50014e-05, acc 1\n",
            "2019-01-22T06:47:00.661673: step 4295, loss 2.05996e-05, acc 1\n",
            "2019-01-22T06:47:01.342633: step 4296, loss 0.000366687, acc 1\n",
            "2019-01-22T06:47:02.018947: step 4297, loss 4.03684e-05, acc 1\n",
            "2019-01-22T06:47:02.693797: step 4298, loss 9.06858e-06, acc 1\n",
            "2019-01-22T06:47:03.378288: step 4299, loss 6.43055e-05, acc 1\n",
            "2019-01-22T06:47:04.053289: step 4300, loss 0.0410666, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:47:04.412312: step 4300, loss 9.61837e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4300\n",
            "\n",
            "2019-01-22T06:47:05.216496: step 4301, loss 2.41799e-05, acc 1\n",
            "2019-01-22T06:47:05.891526: step 4302, loss 5.38384e-05, acc 1\n",
            "2019-01-22T06:47:06.567202: step 4303, loss 1.99199e-05, acc 1\n",
            "2019-01-22T06:47:07.240340: step 4304, loss 5.96046e-08, acc 1\n",
            "2019-01-22T06:47:07.917798: step 4305, loss 0.000478066, acc 1\n",
            "2019-01-22T06:47:08.594512: step 4306, loss 1.03593e-05, acc 1\n",
            "2019-01-22T06:47:09.270986: step 4307, loss 0.0027279, acc 1\n",
            "2019-01-22T06:47:09.947236: step 4308, loss 0.0428952, acc 0.96875\n",
            "2019-01-22T06:47:10.630482: step 4309, loss 2.01235e-05, acc 1\n",
            "2019-01-22T06:47:11.304565: step 4310, loss 4.24251e-05, acc 1\n",
            "2019-01-22T06:47:11.978264: step 4311, loss 0.000149681, acc 1\n",
            "2019-01-22T06:47:12.673043: step 4312, loss 0.00726326, acc 1\n",
            "2019-01-22T06:47:13.351311: step 4313, loss 0.000647595, acc 1\n",
            "2019-01-22T06:47:14.020888: step 4314, loss 5.60072e-05, acc 1\n",
            "2019-01-22T06:47:14.698753: step 4315, loss 4.38957e-05, acc 1\n",
            "2019-01-22T06:47:15.371834: step 4316, loss 3.8778e-06, acc 1\n",
            "2019-01-22T06:47:16.042730: step 4317, loss 0.000652643, acc 1\n",
            "2019-01-22T06:47:16.717109: step 4318, loss 2.74028e-05, acc 1\n",
            "2019-01-22T06:47:17.390746: step 4319, loss 1.8698e-05, acc 1\n",
            "2019-01-22T06:47:18.074259: step 4320, loss 8.42785e-05, acc 1\n",
            "2019-01-22T06:47:18.750846: step 4321, loss 0.0117071, acc 1\n",
            "2019-01-22T06:47:19.426097: step 4322, loss 0.0574784, acc 0.984375\n",
            "2019-01-22T06:47:19.843383: step 4323, loss 4.15118e-05, acc 1\n",
            "2019-01-22T06:47:20.525434: step 4324, loss 0.000147799, acc 1\n",
            "2019-01-22T06:47:21.196841: step 4325, loss 3.74025e-05, acc 1\n",
            "2019-01-22T06:47:21.878010: step 4326, loss 0.000197093, acc 1\n",
            "2019-01-22T06:47:22.558821: step 4327, loss 5.16118e-06, acc 1\n",
            "2019-01-22T06:47:23.235902: step 4328, loss 0.000145458, acc 1\n",
            "2019-01-22T06:47:23.909759: step 4329, loss 0.00266289, acc 1\n",
            "2019-01-22T06:47:24.584428: step 4330, loss 0.00070661, acc 1\n",
            "2019-01-22T06:47:25.261336: step 4331, loss 5.56905e-06, acc 1\n",
            "2019-01-22T06:47:25.936361: step 4332, loss 9.47954e-06, acc 1\n",
            "2019-01-22T06:47:26.619212: step 4333, loss 6.90538e-05, acc 1\n",
            "2019-01-22T06:47:27.297893: step 4334, loss 0.000192446, acc 1\n",
            "2019-01-22T06:47:27.972705: step 4335, loss 6.83586e-07, acc 1\n",
            "2019-01-22T06:47:28.654218: step 4336, loss 0.0516738, acc 0.984375\n",
            "2019-01-22T06:47:29.331471: step 4337, loss 9.66741e-06, acc 1\n",
            "2019-01-22T06:47:30.002643: step 4338, loss 5.82355e-06, acc 1\n",
            "2019-01-22T06:47:30.677253: step 4339, loss 2.47061e-05, acc 1\n",
            "2019-01-22T06:47:31.351249: step 4340, loss 0.000159137, acc 1\n",
            "2019-01-22T06:47:32.036911: step 4341, loss 0.0001261, acc 1\n",
            "2019-01-22T06:47:32.709522: step 4342, loss 2.41267e-05, acc 1\n",
            "2019-01-22T06:47:33.385023: step 4343, loss 0.000207828, acc 1\n",
            "2019-01-22T06:47:34.057082: step 4344, loss 1.11699e-05, acc 1\n",
            "2019-01-22T06:47:34.739954: step 4345, loss 1.70705e-05, acc 1\n",
            "2019-01-22T06:47:35.414102: step 4346, loss 7.06273e-06, acc 1\n",
            "2019-01-22T06:47:36.092031: step 4347, loss 2.64942e-05, acc 1\n",
            "2019-01-22T06:47:36.774142: step 4348, loss 0.000173929, acc 1\n",
            "2019-01-22T06:47:37.454790: step 4349, loss 0.000911937, acc 1\n",
            "2019-01-22T06:47:38.129532: step 4350, loss 2.55417e-05, acc 1\n",
            "2019-01-22T06:47:38.804813: step 4351, loss 0.0543782, acc 0.984375\n",
            "2019-01-22T06:47:39.484094: step 4352, loss 3.37601e-05, acc 1\n",
            "2019-01-22T06:47:40.161143: step 4353, loss 2.54221e-05, acc 1\n",
            "2019-01-22T06:47:40.836393: step 4354, loss 0.0538448, acc 0.984375\n",
            "2019-01-22T06:47:41.509366: step 4355, loss 2.77285e-05, acc 1\n",
            "2019-01-22T06:47:41.921769: step 4356, loss 1.77166e-05, acc 1\n",
            "2019-01-22T06:47:42.597702: step 4357, loss 0.00332443, acc 1\n",
            "2019-01-22T06:47:43.278963: step 4358, loss 1.8648e-05, acc 1\n",
            "2019-01-22T06:47:43.959012: step 4359, loss 1.53834e-05, acc 1\n",
            "2019-01-22T06:47:44.638091: step 4360, loss 3.6487e-06, acc 1\n",
            "2019-01-22T06:47:45.311789: step 4361, loss 0.00108649, acc 1\n",
            "2019-01-22T06:47:45.989957: step 4362, loss 1.9653e-05, acc 1\n",
            "2019-01-22T06:47:46.666395: step 4363, loss 3.26695e-06, acc 1\n",
            "2019-01-22T06:47:47.346349: step 4364, loss 0.00065385, acc 1\n",
            "2019-01-22T06:47:48.029910: step 4365, loss 0.000284493, acc 1\n",
            "2019-01-22T06:47:48.712770: step 4366, loss 0.000445433, acc 1\n",
            "2019-01-22T06:47:49.391558: step 4367, loss 0.000219264, acc 1\n",
            "2019-01-22T06:47:50.070438: step 4368, loss 0.00032227, acc 1\n",
            "2019-01-22T06:47:50.756445: step 4369, loss 4.70866e-06, acc 1\n",
            "2019-01-22T06:47:51.442601: step 4370, loss 3.2801e-05, acc 1\n",
            "2019-01-22T06:47:52.122555: step 4371, loss 4.53079e-05, acc 1\n",
            "2019-01-22T06:47:52.806684: step 4372, loss 0.000517352, acc 1\n",
            "2019-01-22T06:47:53.486399: step 4373, loss 1.33516e-05, acc 1\n",
            "2019-01-22T06:47:54.165834: step 4374, loss 0.00854527, acc 1\n",
            "2019-01-22T06:47:54.851399: step 4375, loss 4.77933e-06, acc 1\n",
            "2019-01-22T06:47:55.533886: step 4376, loss 0.00013692, acc 1\n",
            "2019-01-22T06:47:56.211011: step 4377, loss 3.8427e-05, acc 1\n",
            "2019-01-22T06:47:56.891633: step 4378, loss 1.0378e-05, acc 1\n",
            "2019-01-22T06:47:57.570537: step 4379, loss 2.42985e-05, acc 1\n",
            "2019-01-22T06:47:58.248523: step 4380, loss 4.9472e-05, acc 1\n",
            "2019-01-22T06:47:58.930483: step 4381, loss 0.00062274, acc 1\n",
            "2019-01-22T06:47:59.612252: step 4382, loss 0.00053574, acc 1\n",
            "2019-01-22T06:48:00.287305: step 4383, loss 0.00157611, acc 1\n",
            "2019-01-22T06:48:00.969338: step 4384, loss 3.15349e-05, acc 1\n",
            "2019-01-22T06:48:01.643417: step 4385, loss 2.81624e-05, acc 1\n",
            "2019-01-22T06:48:02.318777: step 4386, loss 0.0218538, acc 0.984375\n",
            "2019-01-22T06:48:02.996675: step 4387, loss 3.55006e-05, acc 1\n",
            "2019-01-22T06:48:03.678481: step 4388, loss 0.0223479, acc 0.984375\n",
            "2019-01-22T06:48:04.096975: step 4389, loss 0.0024722, acc 1\n",
            "2019-01-22T06:48:04.778598: step 4390, loss 0.00481618, acc 1\n",
            "2019-01-22T06:48:05.455878: step 4391, loss 5.68817e-05, acc 1\n",
            "2019-01-22T06:48:06.141136: step 4392, loss 0.00106681, acc 1\n",
            "2019-01-22T06:48:06.821368: step 4393, loss 0.000653405, acc 1\n",
            "2019-01-22T06:48:07.497208: step 4394, loss 1.42788e-05, acc 1\n",
            "2019-01-22T06:48:08.184327: step 4395, loss 0.000205492, acc 1\n",
            "2019-01-22T06:48:08.856397: step 4396, loss 4.34871e-06, acc 1\n",
            "2019-01-22T06:48:09.531319: step 4397, loss 5.94328e-06, acc 1\n",
            "2019-01-22T06:48:10.211990: step 4398, loss 0.0025175, acc 1\n",
            "2019-01-22T06:48:10.899835: step 4399, loss 0.000796209, acc 1\n",
            "2019-01-22T06:48:11.579452: step 4400, loss 0.0163589, acc 0.984375\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:48:11.940795: step 4400, loss 0.000646636, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4400\n",
            "\n",
            "2019-01-22T06:48:12.762970: step 4401, loss 4.16279e-06, acc 1\n",
            "2019-01-22T06:48:13.441076: step 4402, loss 1.54386e-05, acc 1\n",
            "2019-01-22T06:48:14.121042: step 4403, loss 0.000108297, acc 1\n",
            "2019-01-22T06:48:14.794469: step 4404, loss 0.000869452, acc 1\n",
            "2019-01-22T06:48:15.492112: step 4405, loss 0.000373258, acc 1\n",
            "2019-01-22T06:48:16.251117: step 4406, loss 5.28311e-05, acc 1\n",
            "2019-01-22T06:48:17.093882: step 4407, loss 4.02493e-06, acc 1\n",
            "2019-01-22T06:48:17.773988: step 4408, loss 0.0561629, acc 0.984375\n",
            "2019-01-22T06:48:18.448800: step 4409, loss 3.23549e-05, acc 1\n",
            "2019-01-22T06:48:19.128519: step 4410, loss 0.000644497, acc 1\n",
            "2019-01-22T06:48:19.805790: step 4411, loss 2.20277e-05, acc 1\n",
            "2019-01-22T06:48:20.487648: step 4412, loss 1.22417e-05, acc 1\n",
            "2019-01-22T06:48:21.166968: step 4413, loss 1.27533e-05, acc 1\n",
            "2019-01-22T06:48:21.847067: step 4414, loss 4.03434e-05, acc 1\n",
            "2019-01-22T06:48:22.527419: step 4415, loss 0.000500823, acc 1\n",
            "2019-01-22T06:48:23.205450: step 4416, loss 2.33943e-06, acc 1\n",
            "2019-01-22T06:48:23.885878: step 4417, loss 0.000215742, acc 1\n",
            "2019-01-22T06:48:24.564519: step 4418, loss 1.87006e-06, acc 1\n",
            "2019-01-22T06:48:25.243718: step 4419, loss 2.15318e-06, acc 1\n",
            "2019-01-22T06:48:25.919895: step 4420, loss 3.88362e-05, acc 1\n",
            "2019-01-22T06:48:26.597816: step 4421, loss 8.10156e-06, acc 1\n",
            "2019-01-22T06:48:27.010261: step 4422, loss 4.95873e-06, acc 1\n",
            "2019-01-22T06:48:27.684922: step 4423, loss 0.000131151, acc 1\n",
            "2019-01-22T06:48:28.362406: step 4424, loss 2.42139e-06, acc 1\n",
            "2019-01-22T06:48:29.039487: step 4425, loss 1.83464e-05, acc 1\n",
            "2019-01-22T06:48:29.720498: step 4426, loss 0.023248, acc 0.984375\n",
            "2019-01-22T06:48:30.394872: step 4427, loss 0.0216213, acc 0.984375\n",
            "2019-01-22T06:48:31.073639: step 4428, loss 0.000179858, acc 1\n",
            "2019-01-22T06:48:31.752341: step 4429, loss 0.00043846, acc 1\n",
            "2019-01-22T06:48:32.429257: step 4430, loss 0.000312933, acc 1\n",
            "2019-01-22T06:48:33.110119: step 4431, loss 2.71523e-05, acc 1\n",
            "2019-01-22T06:48:33.791211: step 4432, loss 1.70541e-05, acc 1\n",
            "2019-01-22T06:48:34.470683: step 4433, loss 0.000742236, acc 1\n",
            "2019-01-22T06:48:35.150765: step 4434, loss 0.00112502, acc 1\n",
            "2019-01-22T06:48:35.828921: step 4435, loss 0.00227531, acc 1\n",
            "2019-01-22T06:48:36.509742: step 4436, loss 3.75853e-06, acc 1\n",
            "2019-01-22T06:48:37.186698: step 4437, loss 7.22897e-05, acc 1\n",
            "2019-01-22T06:48:37.863783: step 4438, loss 0.00106181, acc 1\n",
            "2019-01-22T06:48:38.542815: step 4439, loss 8.13494e-05, acc 1\n",
            "2019-01-22T06:48:39.220827: step 4440, loss 9.28579e-05, acc 1\n",
            "2019-01-22T06:48:39.899938: step 4441, loss 0.000640389, acc 1\n",
            "2019-01-22T06:48:40.577206: step 4442, loss 0.00531163, acc 1\n",
            "2019-01-22T06:48:41.253961: step 4443, loss 2.36739e-06, acc 1\n",
            "2019-01-22T06:48:41.935186: step 4444, loss 5.47616e-07, acc 1\n",
            "2019-01-22T06:48:42.609994: step 4445, loss 4.57257e-06, acc 1\n",
            "2019-01-22T06:48:43.294948: step 4446, loss 8.66089e-05, acc 1\n",
            "2019-01-22T06:48:43.969324: step 4447, loss 2.196e-06, acc 1\n",
            "2019-01-22T06:48:44.650292: step 4448, loss 0.000359025, acc 1\n",
            "2019-01-22T06:48:45.333896: step 4449, loss 9.73168e-05, acc 1\n",
            "2019-01-22T06:48:46.011705: step 4450, loss 0.000132876, acc 1\n",
            "2019-01-22T06:48:46.689518: step 4451, loss 0.000927984, acc 1\n",
            "2019-01-22T06:48:47.378678: step 4452, loss 8.0176e-05, acc 1\n",
            "2019-01-22T06:48:48.062105: step 4453, loss 7.7031e-06, acc 1\n",
            "2019-01-22T06:48:48.739396: step 4454, loss 0.0118499, acc 0.984375\n",
            "2019-01-22T06:48:49.151325: step 4455, loss 5.44954e-07, acc 1\n",
            "2019-01-22T06:48:49.825805: step 4456, loss 7.3289e-05, acc 1\n",
            "2019-01-22T06:48:50.505381: step 4457, loss 0.000532668, acc 1\n",
            "2019-01-22T06:48:51.184410: step 4458, loss 9.95924e-05, acc 1\n",
            "2019-01-22T06:48:51.864294: step 4459, loss 2.41582e-06, acc 1\n",
            "2019-01-22T06:48:52.548619: step 4460, loss 7.61383e-06, acc 1\n",
            "2019-01-22T06:48:53.228410: step 4461, loss 5.2085e-05, acc 1\n",
            "2019-01-22T06:48:53.907821: step 4462, loss 2.42698e-05, acc 1\n",
            "2019-01-22T06:48:54.595745: step 4463, loss 8.2854e-05, acc 1\n",
            "2019-01-22T06:48:55.277470: step 4464, loss 1.09735e-05, acc 1\n",
            "2019-01-22T06:48:55.955611: step 4465, loss 4.98043e-06, acc 1\n",
            "2019-01-22T06:48:56.639764: step 4466, loss 2.75102e-06, acc 1\n",
            "2019-01-22T06:48:57.319144: step 4467, loss 0.000101814, acc 1\n",
            "2019-01-22T06:48:58.000433: step 4468, loss 2.06789e-05, acc 1\n",
            "2019-01-22T06:48:58.686813: step 4469, loss 0.000339665, acc 1\n",
            "2019-01-22T06:48:59.363344: step 4470, loss 0.00187385, acc 1\n",
            "2019-01-22T06:49:00.044403: step 4471, loss 3.07737e-05, acc 1\n",
            "2019-01-22T06:49:00.732260: step 4472, loss 2.90929e-06, acc 1\n",
            "2019-01-22T06:49:01.415371: step 4473, loss 0.026193, acc 0.984375\n",
            "2019-01-22T06:49:02.097063: step 4474, loss 0.005072, acc 1\n",
            "2019-01-22T06:49:02.778063: step 4475, loss 0.00057501, acc 1\n",
            "2019-01-22T06:49:03.455149: step 4476, loss 2.81543e-05, acc 1\n",
            "2019-01-22T06:49:04.132608: step 4477, loss 9.513e-06, acc 1\n",
            "2019-01-22T06:49:04.809456: step 4478, loss 6.22223e-06, acc 1\n",
            "2019-01-22T06:49:05.491979: step 4479, loss 0.00145054, acc 1\n",
            "2019-01-22T06:49:06.167962: step 4480, loss 1.22579e-05, acc 1\n",
            "2019-01-22T06:49:06.843996: step 4481, loss 1.24423e-06, acc 1\n",
            "2019-01-22T06:49:07.519873: step 4482, loss 3.55764e-07, acc 1\n",
            "2019-01-22T06:49:08.194296: step 4483, loss 0.000421751, acc 1\n",
            "2019-01-22T06:49:08.868804: step 4484, loss 2.23242e-05, acc 1\n",
            "2019-01-22T06:49:09.550342: step 4485, loss 3.87427e-07, acc 1\n",
            "2019-01-22T06:49:10.229761: step 4486, loss 8.54695e-05, acc 1\n",
            "2019-01-22T06:49:10.909025: step 4487, loss 3.94569e-05, acc 1\n",
            "2019-01-22T06:49:11.321815: step 4488, loss 1.18282e-05, acc 1\n",
            "2019-01-22T06:49:11.994857: step 4489, loss 1.41563e-05, acc 1\n",
            "2019-01-22T06:49:12.669208: step 4490, loss 0.0126864, acc 0.984375\n",
            "2019-01-22T06:49:13.347637: step 4491, loss 4.53787e-05, acc 1\n",
            "2019-01-22T06:49:14.026979: step 4492, loss 0.000175616, acc 1\n",
            "2019-01-22T06:49:14.703912: step 4493, loss 0.0984674, acc 0.984375\n",
            "2019-01-22T06:49:15.378222: step 4494, loss 0.000521077, acc 1\n",
            "2019-01-22T06:49:16.052880: step 4495, loss 0.000151505, acc 1\n",
            "2019-01-22T06:49:16.729440: step 4496, loss 1.76945e-06, acc 1\n",
            "2019-01-22T06:49:17.406447: step 4497, loss 9.70268e-06, acc 1\n",
            "2019-01-22T06:49:18.089509: step 4498, loss 3.97442e-05, acc 1\n",
            "2019-01-22T06:49:18.764282: step 4499, loss 3.39e-07, acc 1\n",
            "2019-01-22T06:49:19.441218: step 4500, loss 1.83329e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:49:19.802084: step 4500, loss 0.000107899, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4500\n",
            "\n",
            "2019-01-22T06:49:20.620127: step 4501, loss 1.20897e-05, acc 1\n",
            "2019-01-22T06:49:21.306210: step 4502, loss 1.84163e-05, acc 1\n",
            "2019-01-22T06:49:21.983702: step 4503, loss 1.03478e-05, acc 1\n",
            "2019-01-22T06:49:22.666451: step 4504, loss 0.000233223, acc 1\n",
            "2019-01-22T06:49:23.357467: step 4505, loss 0.00115381, acc 1\n",
            "2019-01-22T06:49:24.034993: step 4506, loss 8.47803e-06, acc 1\n",
            "2019-01-22T06:49:24.717775: step 4507, loss 0.00156795, acc 1\n",
            "2019-01-22T06:49:25.408663: step 4508, loss 0.000196867, acc 1\n",
            "2019-01-22T06:49:26.087596: step 4509, loss 0.0161617, acc 0.984375\n",
            "2019-01-22T06:49:26.767247: step 4510, loss 3.13844e-06, acc 1\n",
            "2019-01-22T06:49:27.454412: step 4511, loss 0.0100227, acc 1\n",
            "2019-01-22T06:49:28.128630: step 4512, loss 0.000651117, acc 1\n",
            "2019-01-22T06:49:28.808938: step 4513, loss 6.26936e-06, acc 1\n",
            "2019-01-22T06:49:29.487692: step 4514, loss 0.000576999, acc 1\n",
            "2019-01-22T06:49:30.163843: step 4515, loss 0.0178009, acc 0.984375\n",
            "2019-01-22T06:49:30.844885: step 4516, loss 0.00533673, acc 1\n",
            "2019-01-22T06:49:31.521430: step 4517, loss 0.000221783, acc 1\n",
            "2019-01-22T06:49:32.212952: step 4518, loss 7.9993e-06, acc 1\n",
            "2019-01-22T06:49:32.886135: step 4519, loss 0.00976353, acc 1\n",
            "2019-01-22T06:49:33.567411: step 4520, loss 0.00848353, acc 1\n",
            "2019-01-22T06:49:33.982747: step 4521, loss 0.00027627, acc 1\n",
            "2019-01-22T06:49:34.667286: step 4522, loss 9.66983e-06, acc 1\n",
            "2019-01-22T06:49:35.348263: step 4523, loss 6.33055e-06, acc 1\n",
            "2019-01-22T06:49:36.021304: step 4524, loss 0.000168337, acc 1\n",
            "2019-01-22T06:49:36.705816: step 4525, loss 4.5219e-06, acc 1\n",
            "2019-01-22T06:49:37.382804: step 4526, loss 0.000117156, acc 1\n",
            "2019-01-22T06:49:38.060622: step 4527, loss 0.000227777, acc 1\n",
            "2019-01-22T06:49:38.737662: step 4528, loss 0.0038796, acc 1\n",
            "2019-01-22T06:49:39.416787: step 4529, loss 5.07177e-06, acc 1\n",
            "2019-01-22T06:49:40.094003: step 4530, loss 1.99558e-05, acc 1\n",
            "2019-01-22T06:49:40.774871: step 4531, loss 1.78423e-05, acc 1\n",
            "2019-01-22T06:49:41.454897: step 4532, loss 3.72112e-05, acc 1\n",
            "2019-01-22T06:49:42.132659: step 4533, loss 2.81621e-06, acc 1\n",
            "2019-01-22T06:49:42.810222: step 4534, loss 6.60988e-06, acc 1\n",
            "2019-01-22T06:49:43.494448: step 4535, loss 6.88007e-05, acc 1\n",
            "2019-01-22T06:49:44.173958: step 4536, loss 2.38406e-06, acc 1\n",
            "2019-01-22T06:49:44.851051: step 4537, loss 0.00952157, acc 1\n",
            "2019-01-22T06:49:45.529746: step 4538, loss 0.0650085, acc 0.984375\n",
            "2019-01-22T06:49:46.211151: step 4539, loss 0.041372, acc 0.984375\n",
            "2019-01-22T06:49:46.889669: step 4540, loss 1.74589e-05, acc 1\n",
            "2019-01-22T06:49:47.570211: step 4541, loss 0.00083145, acc 1\n",
            "2019-01-22T06:49:48.246950: step 4542, loss 4.08835e-05, acc 1\n",
            "2019-01-22T06:49:48.925313: step 4543, loss 0.000852226, acc 1\n",
            "2019-01-22T06:49:49.602044: step 4544, loss 1.3767e-05, acc 1\n",
            "2019-01-22T06:49:50.279902: step 4545, loss 4.33029e-05, acc 1\n",
            "2019-01-22T06:49:50.958994: step 4546, loss 0.000343766, acc 1\n",
            "2019-01-22T06:49:51.640137: step 4547, loss 0.0051703, acc 1\n",
            "2019-01-22T06:49:52.316677: step 4548, loss 0.0100172, acc 1\n",
            "2019-01-22T06:49:53.000951: step 4549, loss 0.0035308, acc 1\n",
            "2019-01-22T06:49:53.682874: step 4550, loss 0.0104054, acc 1\n",
            "2019-01-22T06:49:54.359754: step 4551, loss 0.000146865, acc 1\n",
            "2019-01-22T06:49:55.042655: step 4552, loss 0.00402356, acc 1\n",
            "2019-01-22T06:49:55.719246: step 4553, loss 0.000562237, acc 1\n",
            "2019-01-22T06:49:56.131675: step 4554, loss 8.82442e-06, acc 1\n",
            "2019-01-22T06:49:56.812066: step 4555, loss 2.19599e-06, acc 1\n",
            "2019-01-22T06:49:57.491064: step 4556, loss 0.00209054, acc 1\n",
            "2019-01-22T06:49:58.168227: step 4557, loss 3.96138e-05, acc 1\n",
            "2019-01-22T06:49:58.847987: step 4558, loss 2.56476e-06, acc 1\n",
            "2019-01-22T06:49:59.527035: step 4559, loss 0.0279864, acc 0.984375\n",
            "2019-01-22T06:50:00.201311: step 4560, loss 1.52535e-05, acc 1\n",
            "2019-01-22T06:50:00.875469: step 4561, loss 0.0407528, acc 0.984375\n",
            "2019-01-22T06:50:01.554469: step 4562, loss 1.69651e-05, acc 1\n",
            "2019-01-22T06:50:02.231747: step 4563, loss 0.00266482, acc 1\n",
            "2019-01-22T06:50:02.906963: step 4564, loss 1.15411e-05, acc 1\n",
            "2019-01-22T06:50:03.589514: step 4565, loss 0.00523516, acc 1\n",
            "2019-01-22T06:50:04.266942: step 4566, loss 0.00207774, acc 1\n",
            "2019-01-22T06:50:04.941734: step 4567, loss 9.93668e-06, acc 1\n",
            "2019-01-22T06:50:05.626744: step 4568, loss 3.17307e-05, acc 1\n",
            "2019-01-22T06:50:06.303132: step 4569, loss 2.0107e-05, acc 1\n",
            "2019-01-22T06:50:06.985311: step 4570, loss 2.14943e-06, acc 1\n",
            "2019-01-22T06:50:07.669030: step 4571, loss 1.70317e-05, acc 1\n",
            "2019-01-22T06:50:08.345002: step 4572, loss 0.000271279, acc 1\n",
            "2019-01-22T06:50:09.024386: step 4573, loss 1.93456e-05, acc 1\n",
            "2019-01-22T06:50:09.715483: step 4574, loss 1.55704e-05, acc 1\n",
            "2019-01-22T06:50:10.391982: step 4575, loss 7.33421e-05, acc 1\n",
            "2019-01-22T06:50:11.067291: step 4576, loss 4.11225e-06, acc 1\n",
            "2019-01-22T06:50:11.753440: step 4577, loss 7.71524e-05, acc 1\n",
            "2019-01-22T06:50:12.429508: step 4578, loss 9.19047e-05, acc 1\n",
            "2019-01-22T06:50:13.106064: step 4579, loss 5.62675e-06, acc 1\n",
            "2019-01-22T06:50:13.786891: step 4580, loss 0.0013371, acc 1\n",
            "2019-01-22T06:50:14.465474: step 4581, loss 4.79597e-06, acc 1\n",
            "2019-01-22T06:50:15.140826: step 4582, loss 0.00285498, acc 1\n",
            "2019-01-22T06:50:15.820777: step 4583, loss 0.000536387, acc 1\n",
            "2019-01-22T06:50:16.495494: step 4584, loss 5.49923e-05, acc 1\n",
            "2019-01-22T06:50:17.172864: step 4585, loss 0.0506498, acc 0.984375\n",
            "2019-01-22T06:50:17.844200: step 4586, loss 1.11942e-06, acc 1\n",
            "2019-01-22T06:50:18.262915: step 4587, loss 7.24946e-05, acc 1\n",
            "2019-01-22T06:50:18.947487: step 4588, loss 3.72137e-06, acc 1\n",
            "2019-01-22T06:50:19.625727: step 4589, loss 8.95926e-07, acc 1\n",
            "2019-01-22T06:50:20.306476: step 4590, loss 0.000127919, acc 1\n",
            "2019-01-22T06:50:20.987909: step 4591, loss 1.53852e-06, acc 1\n",
            "2019-01-22T06:50:21.666641: step 4592, loss 4.01004e-06, acc 1\n",
            "2019-01-22T06:50:22.345980: step 4593, loss 5.3175e-05, acc 1\n",
            "2019-01-22T06:50:23.023824: step 4594, loss 0.000498272, acc 1\n",
            "2019-01-22T06:50:23.705751: step 4595, loss 0.000379111, acc 1\n",
            "2019-01-22T06:50:24.384965: step 4596, loss 2.96523e-05, acc 1\n",
            "2019-01-22T06:50:25.066857: step 4597, loss 4.3591e-05, acc 1\n",
            "2019-01-22T06:50:25.744692: step 4598, loss 0.0011595, acc 1\n",
            "2019-01-22T06:50:26.423543: step 4599, loss 0.000168223, acc 1\n",
            "2019-01-22T06:50:27.102881: step 4600, loss 3.90171e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:50:27.458066: step 4600, loss 0.000912685, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4600\n",
            "\n",
            "2019-01-22T06:50:28.297795: step 4601, loss 0.0198336, acc 0.984375\n",
            "2019-01-22T06:50:28.982627: step 4602, loss 0.000283901, acc 1\n",
            "2019-01-22T06:50:29.657870: step 4603, loss 2.24708e-05, acc 1\n",
            "2019-01-22T06:50:30.334805: step 4604, loss 0.0211478, acc 0.984375\n",
            "2019-01-22T06:50:31.014302: step 4605, loss 6.35629e-05, acc 1\n",
            "2019-01-22T06:50:31.696519: step 4606, loss 5.25809e-06, acc 1\n",
            "2019-01-22T06:50:32.377698: step 4607, loss 2.64837e-05, acc 1\n",
            "2019-01-22T06:50:33.053081: step 4608, loss 6.19687e-05, acc 1\n",
            "2019-01-22T06:50:33.727467: step 4609, loss 0.000114209, acc 1\n",
            "2019-01-22T06:50:34.411141: step 4610, loss 1.94268e-06, acc 1\n",
            "2019-01-22T06:50:35.085583: step 4611, loss 0.0150474, acc 0.984375\n",
            "2019-01-22T06:50:35.761969: step 4612, loss 8.5631e-06, acc 1\n",
            "2019-01-22T06:50:36.449330: step 4613, loss 5.33661e-05, acc 1\n",
            "2019-01-22T06:50:37.123886: step 4614, loss 0.000135055, acc 1\n",
            "2019-01-22T06:50:37.798826: step 4615, loss 3.27824e-07, acc 1\n",
            "2019-01-22T06:50:38.484935: step 4616, loss 0.00407771, acc 1\n",
            "2019-01-22T06:50:39.159574: step 4617, loss 7.45058e-08, acc 1\n",
            "2019-01-22T06:50:39.835306: step 4618, loss 1.93831e-05, acc 1\n",
            "2019-01-22T06:50:40.511236: step 4619, loss 0.000320869, acc 1\n",
            "2019-01-22T06:50:40.928479: step 4620, loss 1.2704e-06, acc 1\n",
            "2019-01-22T06:50:41.614704: step 4621, loss 2.64495e-07, acc 1\n",
            "2019-01-22T06:50:42.290719: step 4622, loss 3.32548e-05, acc 1\n",
            "2019-01-22T06:50:42.970397: step 4623, loss 0.000153355, acc 1\n",
            "2019-01-22T06:50:43.655934: step 4624, loss 0.0011702, acc 1\n",
            "2019-01-22T06:50:44.332681: step 4625, loss 6.57273e-05, acc 1\n",
            "2019-01-22T06:50:45.008017: step 4626, loss 6.10904e-06, acc 1\n",
            "2019-01-22T06:50:45.690979: step 4627, loss 2.80646e-05, acc 1\n",
            "2019-01-22T06:50:46.367723: step 4628, loss 0.0514042, acc 0.96875\n",
            "2019-01-22T06:50:47.050566: step 4629, loss 0.0719532, acc 0.984375\n",
            "2019-01-22T06:50:47.735063: step 4630, loss 0.00389511, acc 1\n",
            "2019-01-22T06:50:48.410767: step 4631, loss 0.000131301, acc 1\n",
            "2019-01-22T06:50:49.088444: step 4632, loss 0.0284698, acc 0.984375\n",
            "2019-01-22T06:50:49.768752: step 4633, loss 8.6507e-06, acc 1\n",
            "2019-01-22T06:50:50.449207: step 4634, loss 3.95235e-06, acc 1\n",
            "2019-01-22T06:50:51.126885: step 4635, loss 0.00469023, acc 1\n",
            "2019-01-22T06:50:51.804177: step 4636, loss 0.000101256, acc 1\n",
            "2019-01-22T06:50:52.485785: step 4637, loss 0.000384657, acc 1\n",
            "2019-01-22T06:50:53.163339: step 4638, loss 7.80181e-05, acc 1\n",
            "2019-01-22T06:50:53.840214: step 4639, loss 7.97792e-05, acc 1\n",
            "2019-01-22T06:50:54.517288: step 4640, loss 4.02574e-05, acc 1\n",
            "2019-01-22T06:50:55.198354: step 4641, loss 0.00136135, acc 1\n",
            "2019-01-22T06:50:55.872861: step 4642, loss 0.000165263, acc 1\n",
            "2019-01-22T06:50:56.550041: step 4643, loss 1.9777e-05, acc 1\n",
            "2019-01-22T06:50:57.228215: step 4644, loss 0.000231889, acc 1\n",
            "2019-01-22T06:50:57.909349: step 4645, loss 0.000130575, acc 1\n",
            "2019-01-22T06:50:58.589253: step 4646, loss 3.45222e-05, acc 1\n",
            "2019-01-22T06:50:59.268037: step 4647, loss 0.00322153, acc 1\n",
            "2019-01-22T06:50:59.942018: step 4648, loss 7.37299e-06, acc 1\n",
            "2019-01-22T06:51:00.621997: step 4649, loss 3.01748e-07, acc 1\n",
            "2019-01-22T06:51:01.299809: step 4650, loss 0.00140021, acc 1\n",
            "2019-01-22T06:51:01.980433: step 4651, loss 4.14202e-06, acc 1\n",
            "2019-01-22T06:51:02.659004: step 4652, loss 3.89796e-05, acc 1\n",
            "2019-01-22T06:51:03.068852: step 4653, loss 0.000421594, acc 1\n",
            "2019-01-22T06:51:03.749502: step 4654, loss 0.00720629, acc 1\n",
            "2019-01-22T06:51:04.429019: step 4655, loss 1.72478e-06, acc 1\n",
            "2019-01-22T06:51:05.105631: step 4656, loss 0.000164631, acc 1\n",
            "2019-01-22T06:51:05.782733: step 4657, loss 0.000156233, acc 1\n",
            "2019-01-22T06:51:06.458406: step 4658, loss 0.00151529, acc 1\n",
            "2019-01-22T06:51:07.137529: step 4659, loss 1.43e-05, acc 1\n",
            "2019-01-22T06:51:07.816581: step 4660, loss 4.69019e-05, acc 1\n",
            "2019-01-22T06:51:08.492191: step 4661, loss 5.35828e-05, acc 1\n",
            "2019-01-22T06:51:09.168930: step 4662, loss 4.25593e-05, acc 1\n",
            "2019-01-22T06:51:09.847661: step 4663, loss 0.0114423, acc 0.984375\n",
            "2019-01-22T06:51:10.521839: step 4664, loss 3.304e-06, acc 1\n",
            "2019-01-22T06:51:11.194939: step 4665, loss 0.000205489, acc 1\n",
            "2019-01-22T06:51:11.868594: step 4666, loss 0.00515198, acc 1\n",
            "2019-01-22T06:51:12.673532: step 4667, loss 5.73451e-06, acc 1\n",
            "2019-01-22T06:51:13.517630: step 4668, loss 2.70572e-05, acc 1\n",
            "2019-01-22T06:51:14.315179: step 4669, loss 3.16648e-07, acc 1\n",
            "2019-01-22T06:51:15.010861: step 4670, loss 1.28002e-05, acc 1\n",
            "2019-01-22T06:51:15.688504: step 4671, loss 2.01165e-07, acc 1\n",
            "2019-01-22T06:51:16.366818: step 4672, loss 8.55471e-05, acc 1\n",
            "2019-01-22T06:51:17.044097: step 4673, loss 1.1902e-06, acc 1\n",
            "2019-01-22T06:51:17.727122: step 4674, loss 0.0297616, acc 0.984375\n",
            "2019-01-22T06:51:18.405907: step 4675, loss 3.29903e-05, acc 1\n",
            "2019-01-22T06:51:19.086349: step 4676, loss 0.00389876, acc 1\n",
            "2019-01-22T06:51:19.771074: step 4677, loss 1.32246e-06, acc 1\n",
            "2019-01-22T06:51:20.455006: step 4678, loss 2.9346e-05, acc 1\n",
            "2019-01-22T06:51:21.134681: step 4679, loss 6.78703e-06, acc 1\n",
            "2019-01-22T06:51:21.816756: step 4680, loss 0.000958988, acc 1\n",
            "2019-01-22T06:51:22.493614: step 4681, loss 1.35404e-05, acc 1\n",
            "2019-01-22T06:51:23.169628: step 4682, loss 5.87242e-06, acc 1\n",
            "2019-01-22T06:51:23.856749: step 4683, loss 4.87057e-06, acc 1\n",
            "2019-01-22T06:51:24.530444: step 4684, loss 1.0766e-06, acc 1\n",
            "2019-01-22T06:51:25.209878: step 4685, loss 6.61327e-06, acc 1\n",
            "2019-01-22T06:51:25.621941: step 4686, loss 3.55567e-06, acc 1\n",
            "2019-01-22T06:51:26.296934: step 4687, loss 0.0148636, acc 1\n",
            "2019-01-22T06:51:26.984292: step 4688, loss 7.22014e-06, acc 1\n",
            "2019-01-22T06:51:27.660699: step 4689, loss 9.55519e-07, acc 1\n",
            "2019-01-22T06:51:28.338037: step 4690, loss 3.16078e-06, acc 1\n",
            "2019-01-22T06:51:29.023571: step 4691, loss 1.23428e-05, acc 1\n",
            "2019-01-22T06:51:29.698798: step 4692, loss 3.74378e-06, acc 1\n",
            "2019-01-22T06:51:30.374535: step 4693, loss 2.42273e-05, acc 1\n",
            "2019-01-22T06:51:31.059320: step 4694, loss 0.000857776, acc 1\n",
            "2019-01-22T06:51:31.739664: step 4695, loss 4.59664e-06, acc 1\n",
            "2019-01-22T06:51:32.421036: step 4696, loss 3.32281e-06, acc 1\n",
            "2019-01-22T06:51:33.107189: step 4697, loss 4.25427e-05, acc 1\n",
            "2019-01-22T06:51:33.786888: step 4698, loss 0.0019265, acc 1\n",
            "2019-01-22T06:51:34.463628: step 4699, loss 3.94704e-05, acc 1\n",
            "2019-01-22T06:51:35.144504: step 4700, loss 2.7045e-06, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:51:35.501866: step 4700, loss 8.06917e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4700\n",
            "\n",
            "2019-01-22T06:51:36.324046: step 4701, loss 5.07149e-06, acc 1\n",
            "2019-01-22T06:51:37.000111: step 4702, loss 3.63371e-05, acc 1\n",
            "2019-01-22T06:51:37.675812: step 4703, loss 7.87892e-07, acc 1\n",
            "2019-01-22T06:51:38.350762: step 4704, loss 0.00129988, acc 1\n",
            "2019-01-22T06:51:39.024934: step 4705, loss 1.10638e-06, acc 1\n",
            "2019-01-22T06:51:39.700655: step 4706, loss 5.79113e-05, acc 1\n",
            "2019-01-22T06:51:40.382858: step 4707, loss 5.78667e-06, acc 1\n",
            "2019-01-22T06:51:41.056423: step 4708, loss 0.000145846, acc 1\n",
            "2019-01-22T06:51:41.733214: step 4709, loss 8.02806e-06, acc 1\n",
            "2019-01-22T06:51:42.418249: step 4710, loss 0.000265986, acc 1\n",
            "2019-01-22T06:51:43.098728: step 4711, loss 0.0285422, acc 0.984375\n",
            "2019-01-22T06:51:43.773573: step 4712, loss 0.000806594, acc 1\n",
            "2019-01-22T06:51:44.460639: step 4713, loss 0.000131023, acc 1\n",
            "2019-01-22T06:51:45.141439: step 4714, loss 1.77665e-05, acc 1\n",
            "2019-01-22T06:51:45.811899: step 4715, loss 1.4373e-05, acc 1\n",
            "2019-01-22T06:51:46.496404: step 4716, loss 0.0227509, acc 0.984375\n",
            "2019-01-22T06:51:47.172438: step 4717, loss 0.00100606, acc 1\n",
            "2019-01-22T06:51:47.848639: step 4718, loss 1.01996e-05, acc 1\n",
            "2019-01-22T06:51:48.258985: step 4719, loss 5.75608e-07, acc 1\n",
            "2019-01-22T06:51:48.938016: step 4720, loss 0.000104113, acc 1\n",
            "2019-01-22T06:51:49.625344: step 4721, loss 0.000196202, acc 1\n",
            "2019-01-22T06:51:50.301807: step 4722, loss 1.3094e-06, acc 1\n",
            "2019-01-22T06:51:50.982632: step 4723, loss 0.000513842, acc 1\n",
            "2019-01-22T06:51:51.671380: step 4724, loss 0.0142027, acc 0.984375\n",
            "2019-01-22T06:51:52.353475: step 4725, loss 4.52612e-05, acc 1\n",
            "2019-01-22T06:51:53.034009: step 4726, loss 1.27548e-05, acc 1\n",
            "2019-01-22T06:51:53.718990: step 4727, loss 0.000946288, acc 1\n",
            "2019-01-22T06:51:54.398921: step 4728, loss 1.90887e-05, acc 1\n",
            "2019-01-22T06:51:55.074805: step 4729, loss 3.16649e-07, acc 1\n",
            "2019-01-22T06:51:55.760097: step 4730, loss 1.74316e-05, acc 1\n",
            "2019-01-22T06:51:56.440976: step 4731, loss 3.01557e-06, acc 1\n",
            "2019-01-22T06:51:57.117090: step 4732, loss 4.80879e-06, acc 1\n",
            "2019-01-22T06:51:57.802605: step 4733, loss 0.000200699, acc 1\n",
            "2019-01-22T06:51:58.481243: step 4734, loss 0.000384743, acc 1\n",
            "2019-01-22T06:51:59.164194: step 4735, loss 1.36902e-06, acc 1\n",
            "2019-01-22T06:51:59.846211: step 4736, loss 1.00209e-06, acc 1\n",
            "2019-01-22T06:52:00.522968: step 4737, loss 8.48136e-05, acc 1\n",
            "2019-01-22T06:52:01.200878: step 4738, loss 3.87287e-05, acc 1\n",
            "2019-01-22T06:52:01.885354: step 4739, loss 8.87857e-05, acc 1\n",
            "2019-01-22T06:52:02.573392: step 4740, loss 0.00168581, acc 1\n",
            "2019-01-22T06:52:03.248118: step 4741, loss 7.41758e-06, acc 1\n",
            "2019-01-22T06:52:03.926612: step 4742, loss 7.18765e-05, acc 1\n",
            "2019-01-22T06:52:04.604254: step 4743, loss 2.11778e-06, acc 1\n",
            "2019-01-22T06:52:05.287142: step 4744, loss 1.7482e-05, acc 1\n",
            "2019-01-22T06:52:05.963353: step 4745, loss 8.3943e-06, acc 1\n",
            "2019-01-22T06:52:06.638365: step 4746, loss 6.03494e-07, acc 1\n",
            "2019-01-22T06:52:07.312038: step 4747, loss 2.04783e-05, acc 1\n",
            "2019-01-22T06:52:07.994560: step 4748, loss 0.00107113, acc 1\n",
            "2019-01-22T06:52:08.669851: step 4749, loss 6.52899e-05, acc 1\n",
            "2019-01-22T06:52:09.344298: step 4750, loss 1.7866e-05, acc 1\n",
            "2019-01-22T06:52:10.019140: step 4751, loss 3.56685e-06, acc 1\n",
            "2019-01-22T06:52:10.432343: step 4752, loss 2.41138e-06, acc 1\n",
            "2019-01-22T06:52:11.109482: step 4753, loss 8.79002e-05, acc 1\n",
            "2019-01-22T06:52:11.784339: step 4754, loss 2.57938e-05, acc 1\n",
            "2019-01-22T06:52:12.463549: step 4755, loss 5.49308e-05, acc 1\n",
            "2019-01-22T06:52:13.138799: step 4756, loss 2.54635e-05, acc 1\n",
            "2019-01-22T06:52:13.818074: step 4757, loss 0.00231778, acc 1\n",
            "2019-01-22T06:52:14.495348: step 4758, loss 7.73931e-05, acc 1\n",
            "2019-01-22T06:52:15.170929: step 4759, loss 0.000790287, acc 1\n",
            "2019-01-22T06:52:15.847395: step 4760, loss 0.00101088, acc 1\n",
            "2019-01-22T06:52:16.527709: step 4761, loss 0.000184175, acc 1\n",
            "2019-01-22T06:52:17.204546: step 4762, loss 6.90929e-05, acc 1\n",
            "2019-01-22T06:52:17.879089: step 4763, loss 2.68976e-05, acc 1\n",
            "2019-01-22T06:52:18.556561: step 4764, loss 3.97265e-06, acc 1\n",
            "2019-01-22T06:52:19.233705: step 4765, loss 9.2584e-05, acc 1\n",
            "2019-01-22T06:52:19.910468: step 4766, loss 0.00016298, acc 1\n",
            "2019-01-22T06:52:20.588236: step 4767, loss 0.00513976, acc 1\n",
            "2019-01-22T06:52:21.265806: step 4768, loss 2.69173e-05, acc 1\n",
            "2019-01-22T06:52:21.945631: step 4769, loss 0.000198585, acc 1\n",
            "2019-01-22T06:52:22.620845: step 4770, loss 3.62706e-05, acc 1\n",
            "2019-01-22T06:52:23.293862: step 4771, loss 3.27305e-05, acc 1\n",
            "2019-01-22T06:52:23.972870: step 4772, loss 0.000211793, acc 1\n",
            "2019-01-22T06:52:24.649467: step 4773, loss 1.65397e-06, acc 1\n",
            "2019-01-22T06:52:25.326123: step 4774, loss 1.05238e-06, acc 1\n",
            "2019-01-22T06:52:26.004144: step 4775, loss 8.85755e-06, acc 1\n",
            "2019-01-22T06:52:26.685145: step 4776, loss 5.5658e-05, acc 1\n",
            "2019-01-22T06:52:27.360366: step 4777, loss 5.05119e-06, acc 1\n",
            "2019-01-22T06:52:28.038977: step 4778, loss 0.000152489, acc 1\n",
            "2019-01-22T06:52:28.721769: step 4779, loss 0.00325014, acc 1\n",
            "2019-01-22T06:52:29.397328: step 4780, loss 3.43595e-05, acc 1\n",
            "2019-01-22T06:52:30.078372: step 4781, loss 0.000313553, acc 1\n",
            "2019-01-22T06:52:30.762696: step 4782, loss 0.00255615, acc 1\n",
            "2019-01-22T06:52:31.440791: step 4783, loss 2.29064e-05, acc 1\n",
            "2019-01-22T06:52:32.118305: step 4784, loss 2.14204e-07, acc 1\n",
            "2019-01-22T06:52:32.530993: step 4785, loss 7.95434e-05, acc 1\n",
            "2019-01-22T06:52:33.207193: step 4786, loss 1.139e-05, acc 1\n",
            "2019-01-22T06:52:33.885636: step 4787, loss 0.00153816, acc 1\n",
            "2019-01-22T06:52:34.562779: step 4788, loss 8.88466e-07, acc 1\n",
            "2019-01-22T06:52:35.242807: step 4789, loss 0.00033743, acc 1\n",
            "2019-01-22T06:52:35.925821: step 4790, loss 0.00181686, acc 1\n",
            "2019-01-22T06:52:36.605879: step 4791, loss 3.61463e-05, acc 1\n",
            "2019-01-22T06:52:37.283492: step 4792, loss 0.000216248, acc 1\n",
            "2019-01-22T06:52:37.966383: step 4793, loss 0.00142036, acc 1\n",
            "2019-01-22T06:52:38.642931: step 4794, loss 0.0267897, acc 0.984375\n",
            "2019-01-22T06:52:39.321606: step 4795, loss 8.85857e-05, acc 1\n",
            "2019-01-22T06:52:40.007580: step 4796, loss 0.000303216, acc 1\n",
            "2019-01-22T06:52:40.684754: step 4797, loss 1.15484e-07, acc 1\n",
            "2019-01-22T06:52:41.367373: step 4798, loss 0.0015479, acc 1\n",
            "2019-01-22T06:52:42.051865: step 4799, loss 6.46855e-06, acc 1\n",
            "2019-01-22T06:52:42.733295: step 4800, loss 4.00466e-07, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:52:43.093949: step 4800, loss 0.000772783, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4800\n",
            "\n",
            "2019-01-22T06:52:43.919720: step 4801, loss 1.26201e-05, acc 1\n",
            "2019-01-22T06:52:44.596682: step 4802, loss 2.20967e-05, acc 1\n",
            "2019-01-22T06:52:45.274033: step 4803, loss 0.0130826, acc 1\n",
            "2019-01-22T06:52:45.956309: step 4804, loss 0.000324514, acc 1\n",
            "2019-01-22T06:52:46.632463: step 4805, loss 1.18167e-05, acc 1\n",
            "2019-01-22T06:52:47.308492: step 4806, loss 0.000171012, acc 1\n",
            "2019-01-22T06:52:47.984401: step 4807, loss 7.42198e-06, acc 1\n",
            "2019-01-22T06:52:48.663970: step 4808, loss 0.000389926, acc 1\n",
            "2019-01-22T06:52:49.344239: step 4809, loss 0.000163571, acc 1\n",
            "2019-01-22T06:52:50.021992: step 4810, loss 4.36239e-05, acc 1\n",
            "2019-01-22T06:52:50.700263: step 4811, loss 6.215e-06, acc 1\n",
            "2019-01-22T06:52:51.388583: step 4812, loss 0.000233065, acc 1\n",
            "2019-01-22T06:52:52.061335: step 4813, loss 2.09726e-06, acc 1\n",
            "2019-01-22T06:52:52.739398: step 4814, loss 0.0003449, acc 1\n",
            "2019-01-22T06:52:53.423444: step 4815, loss 0.000188279, acc 1\n",
            "2019-01-22T06:52:54.099615: step 4816, loss 0.00229121, acc 1\n",
            "2019-01-22T06:52:54.779378: step 4817, loss 0.000107764, acc 1\n",
            "2019-01-22T06:52:55.191332: step 4818, loss 1.29427e-07, acc 1\n",
            "2019-01-22T06:52:55.868902: step 4819, loss 0.00105202, acc 1\n",
            "2019-01-22T06:52:56.551273: step 4820, loss 0.00031165, acc 1\n",
            "2019-01-22T06:52:57.227561: step 4821, loss 6.8383e-06, acc 1\n",
            "2019-01-22T06:52:57.910237: step 4822, loss 1.89664e-05, acc 1\n",
            "2019-01-22T06:52:58.594312: step 4823, loss 1.19335e-05, acc 1\n",
            "2019-01-22T06:52:59.271791: step 4824, loss 0.0151685, acc 0.984375\n",
            "2019-01-22T06:52:59.946995: step 4825, loss 7.43309e-06, acc 1\n",
            "2019-01-22T06:53:00.633480: step 4826, loss 1.5143e-06, acc 1\n",
            "2019-01-22T06:53:01.311239: step 4827, loss 5.27298e-05, acc 1\n",
            "2019-01-22T06:53:01.989235: step 4828, loss 2.01165e-07, acc 1\n",
            "2019-01-22T06:53:02.675913: step 4829, loss 0.00049251, acc 1\n",
            "2019-01-22T06:53:03.353540: step 4830, loss 0.0201055, acc 0.984375\n",
            "2019-01-22T06:53:04.035337: step 4831, loss 0.00393487, acc 1\n",
            "2019-01-22T06:53:04.721544: step 4832, loss 2.22573e-06, acc 1\n",
            "2019-01-22T06:53:05.396696: step 4833, loss 0.000109133, acc 1\n",
            "2019-01-22T06:53:06.075544: step 4834, loss 0.117113, acc 0.984375\n",
            "2019-01-22T06:53:06.759592: step 4835, loss 6.08826e-06, acc 1\n",
            "2019-01-22T06:53:07.443886: step 4836, loss 0.000262342, acc 1\n",
            "2019-01-22T06:53:08.120672: step 4837, loss 1.61859e-06, acc 1\n",
            "2019-01-22T06:53:08.796612: step 4838, loss 8.40751e-05, acc 1\n",
            "2019-01-22T06:53:09.472994: step 4839, loss 0.0339036, acc 0.984375\n",
            "2019-01-22T06:53:10.154571: step 4840, loss 0.000208906, acc 1\n",
            "2019-01-22T06:53:10.831413: step 4841, loss 1.52825e-05, acc 1\n",
            "2019-01-22T06:53:11.506255: step 4842, loss 0.000105559, acc 1\n",
            "2019-01-22T06:53:12.178184: step 4843, loss 3.33238e-05, acc 1\n",
            "2019-01-22T06:53:12.856380: step 4844, loss 6.76633e-06, acc 1\n",
            "2019-01-22T06:53:13.529197: step 4845, loss 2.99133e-06, acc 1\n",
            "2019-01-22T06:53:14.203101: step 4846, loss 3.78285e-06, acc 1\n",
            "2019-01-22T06:53:14.880373: step 4847, loss 0.0015992, acc 1\n",
            "2019-01-22T06:53:15.557344: step 4848, loss 1.55525e-05, acc 1\n",
            "2019-01-22T06:53:16.231560: step 4849, loss 1.72289e-06, acc 1\n",
            "2019-01-22T06:53:16.909423: step 4850, loss 8.26999e-07, acc 1\n",
            "2019-01-22T06:53:17.323254: step 4851, loss 0.00377689, acc 1\n",
            "2019-01-22T06:53:17.999630: step 4852, loss 0.00049044, acc 1\n",
            "2019-01-22T06:53:18.681550: step 4853, loss 0.00421252, acc 1\n",
            "2019-01-22T06:53:19.355991: step 4854, loss 5.66021e-06, acc 1\n",
            "2019-01-22T06:53:20.033658: step 4855, loss 0.00114014, acc 1\n",
            "2019-01-22T06:53:20.708182: step 4856, loss 8.99897e-06, acc 1\n",
            "2019-01-22T06:53:21.384805: step 4857, loss 1.45284e-06, acc 1\n",
            "2019-01-22T06:53:22.060713: step 4858, loss 1.50685e-06, acc 1\n",
            "2019-01-22T06:53:22.737204: step 4859, loss 0.00427257, acc 1\n",
            "2019-01-22T06:53:23.416454: step 4860, loss 2.64791e-05, acc 1\n",
            "2019-01-22T06:53:24.093096: step 4861, loss 0.00117147, acc 1\n",
            "2019-01-22T06:53:24.768368: step 4862, loss 1.11197e-06, acc 1\n",
            "2019-01-22T06:53:25.441749: step 4863, loss 9.21262e-06, acc 1\n",
            "2019-01-22T06:53:26.120122: step 4864, loss 2.10313e-05, acc 1\n",
            "2019-01-22T06:53:26.803004: step 4865, loss 0.000927472, acc 1\n",
            "2019-01-22T06:53:27.477123: step 4866, loss 0.00143112, acc 1\n",
            "2019-01-22T06:53:28.147273: step 4867, loss 2.40451e-05, acc 1\n",
            "2019-01-22T06:53:28.826101: step 4868, loss 9.70519e-05, acc 1\n",
            "2019-01-22T06:53:29.499536: step 4869, loss 3.80764e-05, acc 1\n",
            "2019-01-22T06:53:30.170617: step 4870, loss 1.16413e-05, acc 1\n",
            "2019-01-22T06:53:30.851025: step 4871, loss 1.83769e-05, acc 1\n",
            "2019-01-22T06:53:31.526357: step 4872, loss 4.90857e-05, acc 1\n",
            "2019-01-22T06:53:32.204533: step 4873, loss 0.000320085, acc 1\n",
            "2019-01-22T06:53:32.881759: step 4874, loss 0.0178572, acc 0.984375\n",
            "2019-01-22T06:53:33.567659: step 4875, loss 7.66877e-06, acc 1\n",
            "2019-01-22T06:53:34.246775: step 4876, loss 0.000309108, acc 1\n",
            "2019-01-22T06:53:34.925461: step 4877, loss 1.3818e-05, acc 1\n",
            "2019-01-22T06:53:35.608531: step 4878, loss 0.000190755, acc 1\n",
            "2019-01-22T06:53:36.281319: step 4879, loss 0.000202106, acc 1\n",
            "2019-01-22T06:53:36.960005: step 4880, loss 2.08798e-06, acc 1\n",
            "2019-01-22T06:53:37.640984: step 4881, loss 7.06269e-05, acc 1\n",
            "2019-01-22T06:53:38.318231: step 4882, loss 7.63327e-06, acc 1\n",
            "2019-01-22T06:53:38.994174: step 4883, loss 0.00205755, acc 1\n",
            "2019-01-22T06:53:39.409453: step 4884, loss 3.86214e-06, acc 1\n",
            "2019-01-22T06:53:40.083353: step 4885, loss 2.60761e-06, acc 1\n",
            "2019-01-22T06:53:40.765888: step 4886, loss 8.99849e-05, acc 1\n",
            "2019-01-22T06:53:41.446852: step 4887, loss 1.36199e-05, acc 1\n",
            "2019-01-22T06:53:42.123119: step 4888, loss 8.0429e-06, acc 1\n",
            "2019-01-22T06:53:42.807415: step 4889, loss 5.99496e-05, acc 1\n",
            "2019-01-22T06:53:43.489331: step 4890, loss 1.06234e-05, acc 1\n",
            "2019-01-22T06:53:44.163956: step 4891, loss 0.00930596, acc 1\n",
            "2019-01-22T06:53:44.846880: step 4892, loss 3.94416e-05, acc 1\n",
            "2019-01-22T06:53:45.521884: step 4893, loss 1.51157e-05, acc 1\n",
            "2019-01-22T06:53:46.203893: step 4894, loss 0.00568433, acc 1\n",
            "2019-01-22T06:53:46.887343: step 4895, loss 1.41337e-05, acc 1\n",
            "2019-01-22T06:53:47.562339: step 4896, loss 4.19262e-06, acc 1\n",
            "2019-01-22T06:53:48.235104: step 4897, loss 1.64355e-05, acc 1\n",
            "2019-01-22T06:53:48.915320: step 4898, loss 7.09185e-06, acc 1\n",
            "2019-01-22T06:53:49.592442: step 4899, loss 1.02815e-06, acc 1\n",
            "2019-01-22T06:53:50.271021: step 4900, loss 2.12341e-07, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:53:50.628732: step 4900, loss 0.000104111, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-4900\n",
            "\n",
            "2019-01-22T06:53:51.447401: step 4901, loss 1.37773e-05, acc 1\n",
            "2019-01-22T06:53:52.122573: step 4902, loss 9.23234e-06, acc 1\n",
            "2019-01-22T06:53:52.799394: step 4903, loss 8.46835e-06, acc 1\n",
            "2019-01-22T06:53:53.474006: step 4904, loss 6.12802e-07, acc 1\n",
            "2019-01-22T06:53:54.150237: step 4905, loss 0.000154567, acc 1\n",
            "2019-01-22T06:53:54.825248: step 4906, loss 0.000370316, acc 1\n",
            "2019-01-22T06:53:55.501637: step 4907, loss 5.41985e-06, acc 1\n",
            "2019-01-22T06:53:56.186552: step 4908, loss 0.000115676, acc 1\n",
            "2019-01-22T06:53:56.862891: step 4909, loss 3.9158e-05, acc 1\n",
            "2019-01-22T06:53:57.538003: step 4910, loss 2.52701e-05, acc 1\n",
            "2019-01-22T06:53:58.224608: step 4911, loss 1.00579e-05, acc 1\n",
            "2019-01-22T06:53:58.905817: step 4912, loss 5.41403e-06, acc 1\n",
            "2019-01-22T06:53:59.581574: step 4913, loss 4.61345e-06, acc 1\n",
            "2019-01-22T06:54:00.269513: step 4914, loss 1.48261e-06, acc 1\n",
            "2019-01-22T06:54:00.946971: step 4915, loss 9.27592e-07, acc 1\n",
            "2019-01-22T06:54:01.621315: step 4916, loss 0.0456995, acc 0.984375\n",
            "2019-01-22T06:54:02.034561: step 4917, loss 3.81462e-06, acc 1\n",
            "2019-01-22T06:54:02.712225: step 4918, loss 0.00486361, acc 1\n",
            "2019-01-22T06:54:03.394391: step 4919, loss 1.88531e-05, acc 1\n",
            "2019-01-22T06:54:04.075588: step 4920, loss 4.64041e-05, acc 1\n",
            "2019-01-22T06:54:04.753428: step 4921, loss 0.000208993, acc 1\n",
            "2019-01-22T06:54:05.439923: step 4922, loss 0.00541603, acc 1\n",
            "2019-01-22T06:54:06.120907: step 4923, loss 9.49054e-05, acc 1\n",
            "2019-01-22T06:54:06.797043: step 4924, loss 3.00041e-05, acc 1\n",
            "2019-01-22T06:54:07.480089: step 4925, loss 1.52201e-05, acc 1\n",
            "2019-01-22T06:54:08.157258: step 4926, loss 8.65808e-06, acc 1\n",
            "2019-01-22T06:54:08.832540: step 4927, loss 4.76521e-05, acc 1\n",
            "2019-01-22T06:54:09.523684: step 4928, loss 0.0158034, acc 0.984375\n",
            "2019-01-22T06:54:10.204355: step 4929, loss 1.16429e-05, acc 1\n",
            "2019-01-22T06:54:10.876222: step 4930, loss 0.00179805, acc 1\n",
            "2019-01-22T06:54:11.555534: step 4931, loss 6.23772e-05, acc 1\n",
            "2019-01-22T06:54:12.230506: step 4932, loss 0.0385606, acc 0.984375\n",
            "2019-01-22T06:54:12.906973: step 4933, loss 2.26863e-06, acc 1\n",
            "2019-01-22T06:54:13.582746: step 4934, loss 6.36787e-06, acc 1\n",
            "2019-01-22T06:54:14.260517: step 4935, loss 0.00356154, acc 1\n",
            "2019-01-22T06:54:14.935883: step 4936, loss 3.01349e-06, acc 1\n",
            "2019-01-22T06:54:15.609920: step 4937, loss 0.00110479, acc 1\n",
            "2019-01-22T06:54:16.285672: step 4938, loss 0.000104578, acc 1\n",
            "2019-01-22T06:54:16.961933: step 4939, loss 1.70274e-05, acc 1\n",
            "2019-01-22T06:54:17.646049: step 4940, loss 0.00182526, acc 1\n",
            "2019-01-22T06:54:18.321703: step 4941, loss 1.93959e-05, acc 1\n",
            "2019-01-22T06:54:18.992253: step 4942, loss 0.000229094, acc 1\n",
            "2019-01-22T06:54:19.669652: step 4943, loss 2.6549e-05, acc 1\n",
            "2019-01-22T06:54:20.350425: step 4944, loss 0.00061032, acc 1\n",
            "2019-01-22T06:54:21.023049: step 4945, loss 9.52578e-06, acc 1\n",
            "2019-01-22T06:54:21.699429: step 4946, loss 0.0288216, acc 0.984375\n",
            "2019-01-22T06:54:22.373791: step 4947, loss 1.5847e-05, acc 1\n",
            "2019-01-22T06:54:23.053836: step 4948, loss 0.00610512, acc 1\n",
            "2019-01-22T06:54:23.729269: step 4949, loss 6.59271e-05, acc 1\n",
            "2019-01-22T06:54:24.139278: step 4950, loss 5.67287e-05, acc 1\n",
            "2019-01-22T06:54:24.817477: step 4951, loss 0.014899, acc 0.984375\n",
            "2019-01-22T06:54:25.494654: step 4952, loss 0.000494854, acc 1\n",
            "2019-01-22T06:54:26.175581: step 4953, loss 0.000287965, acc 1\n",
            "2019-01-22T06:54:26.849457: step 4954, loss 4.8056e-07, acc 1\n",
            "2019-01-22T06:54:27.525646: step 4955, loss 0.000115388, acc 1\n",
            "2019-01-22T06:54:28.201960: step 4956, loss 0.0502335, acc 0.984375\n",
            "2019-01-22T06:54:28.882720: step 4957, loss 6.92438e-06, acc 1\n",
            "2019-01-22T06:54:29.559628: step 4958, loss 1.6421e-05, acc 1\n",
            "2019-01-22T06:54:30.230686: step 4959, loss 2.34804e-05, acc 1\n",
            "2019-01-22T06:54:30.906275: step 4960, loss 7.02248e-05, acc 1\n",
            "2019-01-22T06:54:31.582568: step 4961, loss 1.9091e-06, acc 1\n",
            "2019-01-22T06:54:32.258138: step 4962, loss 2.20342e-06, acc 1\n",
            "2019-01-22T06:54:32.932051: step 4963, loss 0.000158852, acc 1\n",
            "2019-01-22T06:54:33.615580: step 4964, loss 5.8631e-05, acc 1\n",
            "2019-01-22T06:54:34.287601: step 4965, loss 2.80154e-05, acc 1\n",
            "2019-01-22T06:54:34.961777: step 4966, loss 0.000892741, acc 1\n",
            "2019-01-22T06:54:35.637593: step 4967, loss 3.54525e-05, acc 1\n",
            "2019-01-22T06:54:36.323743: step 4968, loss 4.865e-06, acc 1\n",
            "2019-01-22T06:54:37.002642: step 4969, loss 9.22972e-06, acc 1\n",
            "2019-01-22T06:54:37.679359: step 4970, loss 4.33419e-06, acc 1\n",
            "2019-01-22T06:54:38.368823: step 4971, loss 0.0795487, acc 0.96875\n",
            "2019-01-22T06:54:39.046688: step 4972, loss 2.84778e-06, acc 1\n",
            "2019-01-22T06:54:39.723186: step 4973, loss 0.109118, acc 0.984375\n",
            "2019-01-22T06:54:40.406663: step 4974, loss 0.00123241, acc 1\n",
            "2019-01-22T06:54:41.082077: step 4975, loss 0.024364, acc 0.984375\n",
            "2019-01-22T06:54:41.755455: step 4976, loss 1.06914e-06, acc 1\n",
            "2019-01-22T06:54:42.436678: step 4977, loss 0.00357542, acc 1\n",
            "2019-01-22T06:54:43.113534: step 4978, loss 8.54356e-06, acc 1\n",
            "2019-01-22T06:54:43.791063: step 4979, loss 3.26358e-05, acc 1\n",
            "2019-01-22T06:54:44.468923: step 4980, loss 1.1029e-05, acc 1\n",
            "2019-01-22T06:54:45.149430: step 4981, loss 2.06309e-05, acc 1\n",
            "2019-01-22T06:54:45.822352: step 4982, loss 2.38688e-05, acc 1\n",
            "2019-01-22T06:54:46.228919: step 4983, loss 0.0247457, acc 0.971429\n",
            "2019-01-22T06:54:46.907548: step 4984, loss 0.019057, acc 0.984375\n",
            "2019-01-22T06:54:47.591220: step 4985, loss 0.0259166, acc 0.984375\n",
            "2019-01-22T06:54:48.271198: step 4986, loss 2.92435e-07, acc 1\n",
            "2019-01-22T06:54:48.943614: step 4987, loss 1.47331e-06, acc 1\n",
            "2019-01-22T06:54:49.624883: step 4988, loss 6.3952e-06, acc 1\n",
            "2019-01-22T06:54:50.300667: step 4989, loss 1.05071e-05, acc 1\n",
            "2019-01-22T06:54:50.980390: step 4990, loss 3.49975e-06, acc 1\n",
            "2019-01-22T06:54:51.653767: step 4991, loss 5.1595e-07, acc 1\n",
            "2019-01-22T06:54:52.330146: step 4992, loss 4.17023e-05, acc 1\n",
            "2019-01-22T06:54:53.007115: step 4993, loss 6.50688e-06, acc 1\n",
            "2019-01-22T06:54:53.687067: step 4994, loss 0.000101125, acc 1\n",
            "2019-01-22T06:54:54.362304: step 4995, loss 0.000130388, acc 1\n",
            "2019-01-22T06:54:55.039301: step 4996, loss 0.00018273, acc 1\n",
            "2019-01-22T06:54:55.717498: step 4997, loss 0.000163252, acc 1\n",
            "2019-01-22T06:54:56.401972: step 4998, loss 0.000403226, acc 1\n",
            "2019-01-22T06:54:57.079188: step 4999, loss 0.000172346, acc 1\n",
            "2019-01-22T06:54:57.762062: step 5000, loss 0.000320331, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:54:58.123079: step 5000, loss 0.00207178, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5000\n",
            "\n",
            "2019-01-22T06:54:58.960209: step 5001, loss 0.00173839, acc 1\n",
            "2019-01-22T06:54:59.637548: step 5002, loss 0.00914861, acc 1\n",
            "2019-01-22T06:55:00.339675: step 5003, loss 0.121434, acc 0.96875\n",
            "2019-01-22T06:55:01.022523: step 5004, loss 0.00303753, acc 1\n",
            "2019-01-22T06:55:01.699767: step 5005, loss 1.47579e-05, acc 1\n",
            "2019-01-22T06:55:02.378841: step 5006, loss 3.16438e-06, acc 1\n",
            "2019-01-22T06:55:03.061870: step 5007, loss 0.000186532, acc 1\n",
            "2019-01-22T06:55:03.739136: step 5008, loss 0.000561722, acc 1\n",
            "2019-01-22T06:55:04.412819: step 5009, loss 0.000394751, acc 1\n",
            "2019-01-22T06:55:05.096265: step 5010, loss 0.000132156, acc 1\n",
            "2019-01-22T06:55:05.770920: step 5011, loss 9.66702e-07, acc 1\n",
            "2019-01-22T06:55:06.445270: step 5012, loss 9.03347e-06, acc 1\n",
            "2019-01-22T06:55:07.126367: step 5013, loss 0.000203625, acc 1\n",
            "2019-01-22T06:55:07.799046: step 5014, loss 0.00567928, acc 1\n",
            "2019-01-22T06:55:08.478642: step 5015, loss 0.0449783, acc 0.984375\n",
            "2019-01-22T06:55:08.890687: step 5016, loss 1.17845e-06, acc 1\n",
            "2019-01-22T06:55:09.565978: step 5017, loss 0.000403177, acc 1\n",
            "2019-01-22T06:55:10.249210: step 5018, loss 6.06113e-05, acc 1\n",
            "2019-01-22T06:55:10.923378: step 5019, loss 0.00329099, acc 1\n",
            "2019-01-22T06:55:11.595051: step 5020, loss 9.53655e-07, acc 1\n",
            "2019-01-22T06:55:12.281810: step 5021, loss 6.60618e-06, acc 1\n",
            "2019-01-22T06:55:12.966984: step 5022, loss 8.82731e-06, acc 1\n",
            "2019-01-22T06:55:13.650840: step 5023, loss 1.69497e-06, acc 1\n",
            "2019-01-22T06:55:14.331084: step 5024, loss 4.75879e-06, acc 1\n",
            "2019-01-22T06:55:15.005660: step 5025, loss 8.79908e-05, acc 1\n",
            "2019-01-22T06:55:15.687445: step 5026, loss 2.73408e-05, acc 1\n",
            "2019-01-22T06:55:16.367905: step 5027, loss 1.52921e-06, acc 1\n",
            "2019-01-22T06:55:17.043292: step 5028, loss 8.30316e-06, acc 1\n",
            "2019-01-22T06:55:17.719128: step 5029, loss 1.19075e-05, acc 1\n",
            "2019-01-22T06:55:18.397323: step 5030, loss 2.06188e-06, acc 1\n",
            "2019-01-22T06:55:19.071830: step 5031, loss 2.8535e-06, acc 1\n",
            "2019-01-22T06:55:19.746535: step 5032, loss 2.19721e-05, acc 1\n",
            "2019-01-22T06:55:20.423608: step 5033, loss 7.47348e-06, acc 1\n",
            "2019-01-22T06:55:21.101788: step 5034, loss 0.000133381, acc 1\n",
            "2019-01-22T06:55:21.777975: step 5035, loss 4.23725e-06, acc 1\n",
            "2019-01-22T06:55:22.456037: step 5036, loss 0.000184124, acc 1\n",
            "2019-01-22T06:55:23.133758: step 5037, loss 2.7883e-06, acc 1\n",
            "2019-01-22T06:55:23.814477: step 5038, loss 0.000166833, acc 1\n",
            "2019-01-22T06:55:24.489287: step 5039, loss 2.37851e-06, acc 1\n",
            "2019-01-22T06:55:25.167938: step 5040, loss 0.000942, acc 1\n",
            "2019-01-22T06:55:25.841450: step 5041, loss 0.0263392, acc 0.984375\n",
            "2019-01-22T06:55:26.522726: step 5042, loss 1.35973e-07, acc 1\n",
            "2019-01-22T06:55:27.196470: step 5043, loss 1.63445e-05, acc 1\n",
            "2019-01-22T06:55:27.870537: step 5044, loss 0.00017202, acc 1\n",
            "2019-01-22T06:55:28.547140: step 5045, loss 0.000127551, acc 1\n",
            "2019-01-22T06:55:29.218141: step 5046, loss 0.0414942, acc 0.984375\n",
            "2019-01-22T06:55:29.891818: step 5047, loss 3.07399e-05, acc 1\n",
            "2019-01-22T06:55:30.569039: step 5048, loss 9.83415e-05, acc 1\n",
            "2019-01-22T06:55:30.982542: step 5049, loss 5.7284e-06, acc 1\n",
            "2019-01-22T06:55:31.657062: step 5050, loss 0.000102207, acc 1\n",
            "2019-01-22T06:55:32.334017: step 5051, loss 2.5629e-06, acc 1\n",
            "2019-01-22T06:55:33.008297: step 5052, loss 0.000118175, acc 1\n",
            "2019-01-22T06:55:33.684931: step 5053, loss 3.57237e-06, acc 1\n",
            "2019-01-22T06:55:34.364328: step 5054, loss 4.71552e-06, acc 1\n",
            "2019-01-22T06:55:35.039725: step 5055, loss 0.00635362, acc 1\n",
            "2019-01-22T06:55:35.717985: step 5056, loss 0.000165178, acc 1\n",
            "2019-01-22T06:55:36.393135: step 5057, loss 6.01631e-07, acc 1\n",
            "2019-01-22T06:55:37.072937: step 5058, loss 9.27699e-06, acc 1\n",
            "2019-01-22T06:55:37.746838: step 5059, loss 5.63352e-05, acc 1\n",
            "2019-01-22T06:55:38.424686: step 5060, loss 2.7904e-05, acc 1\n",
            "2019-01-22T06:55:39.103983: step 5061, loss 0.000227927, acc 1\n",
            "2019-01-22T06:55:39.784039: step 5062, loss 0.000114999, acc 1\n",
            "2019-01-22T06:55:40.461279: step 5063, loss 0.000274678, acc 1\n",
            "2019-01-22T06:55:41.142826: step 5064, loss 0.0038186, acc 1\n",
            "2019-01-22T06:55:41.818804: step 5065, loss 3.15142e-06, acc 1\n",
            "2019-01-22T06:55:42.498057: step 5066, loss 0.0255506, acc 0.984375\n",
            "2019-01-22T06:55:43.181063: step 5067, loss 0.00107712, acc 1\n",
            "2019-01-22T06:55:43.858531: step 5068, loss 0.00120871, acc 1\n",
            "2019-01-22T06:55:44.537214: step 5069, loss 0.00282356, acc 1\n",
            "2019-01-22T06:55:45.223834: step 5070, loss 0.00183161, acc 1\n",
            "2019-01-22T06:55:45.903352: step 5071, loss 0.00012807, acc 1\n",
            "2019-01-22T06:55:46.584805: step 5072, loss 2.06948e-05, acc 1\n",
            "2019-01-22T06:55:47.270591: step 5073, loss 0.000866749, acc 1\n",
            "2019-01-22T06:55:47.948423: step 5074, loss 0.000126344, acc 1\n",
            "2019-01-22T06:55:48.632748: step 5075, loss 0.000104692, acc 1\n",
            "2019-01-22T06:55:49.311077: step 5076, loss 0.0033803, acc 1\n",
            "2019-01-22T06:55:49.988414: step 5077, loss 0.000281144, acc 1\n",
            "2019-01-22T06:55:50.664430: step 5078, loss 0.00330946, acc 1\n",
            "2019-01-22T06:55:51.348276: step 5079, loss 0.000709631, acc 1\n",
            "2019-01-22T06:55:52.030094: step 5080, loss 0.000433547, acc 1\n",
            "2019-01-22T06:55:52.710769: step 5081, loss 0.000861996, acc 1\n",
            "2019-01-22T06:55:53.125440: step 5082, loss 0.0300476, acc 0.971429\n",
            "2019-01-22T06:55:53.802390: step 5083, loss 0.12286, acc 0.984375\n",
            "2019-01-22T06:55:54.486800: step 5084, loss 6.94763e-07, acc 1\n",
            "2019-01-22T06:55:55.161100: step 5085, loss 2.84599e-06, acc 1\n",
            "2019-01-22T06:55:55.840432: step 5086, loss 0.0330689, acc 0.984375\n",
            "2019-01-22T06:55:56.518346: step 5087, loss 3.22223e-06, acc 1\n",
            "2019-01-22T06:55:57.193918: step 5088, loss 7.88937e-06, acc 1\n",
            "2019-01-22T06:55:57.867918: step 5089, loss 0.000217259, acc 1\n",
            "2019-01-22T06:55:58.540329: step 5090, loss 6.929e-07, acc 1\n",
            "2019-01-22T06:55:59.218536: step 5091, loss 2.87875e-05, acc 1\n",
            "2019-01-22T06:55:59.892649: step 5092, loss 5.65824e-06, acc 1\n",
            "2019-01-22T06:56:00.572871: step 5093, loss 3.45306e-06, acc 1\n",
            "2019-01-22T06:56:01.247428: step 5094, loss 7.08573e-05, acc 1\n",
            "2019-01-22T06:56:01.924033: step 5095, loss 0.000154223, acc 1\n",
            "2019-01-22T06:56:02.598375: step 5096, loss 7.45593e-05, acc 1\n",
            "2019-01-22T06:56:03.279911: step 5097, loss 0.000426526, acc 1\n",
            "2019-01-22T06:56:03.956584: step 5098, loss 1.72289e-06, acc 1\n",
            "2019-01-22T06:56:04.636138: step 5099, loss 9.72149e-06, acc 1\n",
            "2019-01-22T06:56:05.310058: step 5100, loss 7.48467e-06, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:56:05.669635: step 5100, loss 0.00013083, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5100\n",
            "\n",
            "2019-01-22T06:56:06.493996: step 5101, loss 0.00177737, acc 1\n",
            "2019-01-22T06:56:07.169506: step 5102, loss 0.000209638, acc 1\n",
            "2019-01-22T06:56:07.850143: step 5103, loss 7.21506e-05, acc 1\n",
            "2019-01-22T06:56:08.522925: step 5104, loss 0.000134417, acc 1\n",
            "2019-01-22T06:56:09.196605: step 5105, loss 5.88592e-07, acc 1\n",
            "2019-01-22T06:56:09.869804: step 5106, loss 7.00851e-06, acc 1\n",
            "2019-01-22T06:56:10.542606: step 5107, loss 2.4698e-06, acc 1\n",
            "2019-01-22T06:56:11.222391: step 5108, loss 6.76576e-05, acc 1\n",
            "2019-01-22T06:56:11.897651: step 5109, loss 0.000577022, acc 1\n",
            "2019-01-22T06:56:12.577777: step 5110, loss 8.81158e-05, acc 1\n",
            "2019-01-22T06:56:13.252614: step 5111, loss 0.105994, acc 0.984375\n",
            "2019-01-22T06:56:13.932833: step 5112, loss 3.88513e-05, acc 1\n",
            "2019-01-22T06:56:14.608960: step 5113, loss 4.01449e-05, acc 1\n",
            "2019-01-22T06:56:15.283419: step 5114, loss 4.70859e-06, acc 1\n",
            "2019-01-22T06:56:15.696526: step 5115, loss 1.09899e-05, acc 1\n",
            "2019-01-22T06:56:16.372085: step 5116, loss 3.44703e-05, acc 1\n",
            "2019-01-22T06:56:17.046996: step 5117, loss 6.09076e-07, acc 1\n",
            "2019-01-22T06:56:17.723641: step 5118, loss 2.5834e-06, acc 1\n",
            "2019-01-22T06:56:18.399743: step 5119, loss 0.000272797, acc 1\n",
            "2019-01-22T06:56:19.072344: step 5120, loss 1.00582e-06, acc 1\n",
            "2019-01-22T06:56:19.756363: step 5121, loss 3.14765e-06, acc 1\n",
            "2019-01-22T06:56:20.430140: step 5122, loss 0.000480307, acc 1\n",
            "2019-01-22T06:56:21.104128: step 5123, loss 0.00116826, acc 1\n",
            "2019-01-22T06:56:21.779544: step 5124, loss 0.0804717, acc 0.984375\n",
            "2019-01-22T06:56:22.457225: step 5125, loss 0.0001075, acc 1\n",
            "2019-01-22T06:56:23.129078: step 5126, loss 1.24795e-06, acc 1\n",
            "2019-01-22T06:56:23.804408: step 5127, loss 2.09907e-06, acc 1\n",
            "2019-01-22T06:56:24.479229: step 5128, loss 0.000324503, acc 1\n",
            "2019-01-22T06:56:25.151823: step 5129, loss 3.97518e-05, acc 1\n",
            "2019-01-22T06:56:25.826012: step 5130, loss 0.00456355, acc 1\n",
            "2019-01-22T06:56:26.501045: step 5131, loss 5.22193e-05, acc 1\n",
            "2019-01-22T06:56:27.179420: step 5132, loss 3.41287e-05, acc 1\n",
            "2019-01-22T06:56:27.850732: step 5133, loss 2.1289e-06, acc 1\n",
            "2019-01-22T06:56:28.530390: step 5134, loss 0.00734458, acc 1\n",
            "2019-01-22T06:56:29.204913: step 5135, loss 0.000521623, acc 1\n",
            "2019-01-22T06:56:29.876529: step 5136, loss 4.84285e-07, acc 1\n",
            "2019-01-22T06:56:30.558442: step 5137, loss 0.00209094, acc 1\n",
            "2019-01-22T06:56:31.232849: step 5138, loss 0.000235214, acc 1\n",
            "2019-01-22T06:56:31.908540: step 5139, loss 0.000196569, acc 1\n",
            "2019-01-22T06:56:32.581850: step 5140, loss 2.28587e-05, acc 1\n",
            "2019-01-22T06:56:33.254048: step 5141, loss 1.75729e-05, acc 1\n",
            "2019-01-22T06:56:33.929612: step 5142, loss 4.27683e-05, acc 1\n",
            "2019-01-22T06:56:34.616624: step 5143, loss 0.0333753, acc 0.984375\n",
            "2019-01-22T06:56:35.291477: step 5144, loss 1.0505e-06, acc 1\n",
            "2019-01-22T06:56:35.965886: step 5145, loss 0.0162216, acc 0.984375\n",
            "2019-01-22T06:56:36.647997: step 5146, loss 1.51409e-05, acc 1\n",
            "2019-01-22T06:56:37.329092: step 5147, loss 8.34922e-05, acc 1\n",
            "2019-01-22T06:56:37.742367: step 5148, loss 1.79153e-06, acc 1\n",
            "2019-01-22T06:56:38.418462: step 5149, loss 0.0130757, acc 0.984375\n",
            "2019-01-22T06:56:39.094542: step 5150, loss 0.0733628, acc 0.984375\n",
            "2019-01-22T06:56:39.779907: step 5151, loss 4.56346e-07, acc 1\n",
            "2019-01-22T06:56:40.456708: step 5152, loss 0.0017764, acc 1\n",
            "2019-01-22T06:56:41.133740: step 5153, loss 0.000533621, acc 1\n",
            "2019-01-22T06:56:41.813772: step 5154, loss 1.95047e-05, acc 1\n",
            "2019-01-22T06:56:42.487536: step 5155, loss 4.91735e-07, acc 1\n",
            "2019-01-22T06:56:43.158771: step 5156, loss 0.00427541, acc 1\n",
            "2019-01-22T06:56:43.843868: step 5157, loss 0.000633896, acc 1\n",
            "2019-01-22T06:56:44.519257: step 5158, loss 2.19222e-06, acc 1\n",
            "2019-01-22T06:56:45.190673: step 5159, loss 1.61725e-05, acc 1\n",
            "2019-01-22T06:56:45.872142: step 5160, loss 0.000396029, acc 1\n",
            "2019-01-22T06:56:46.554349: step 5161, loss 4.79829e-05, acc 1\n",
            "2019-01-22T06:56:47.228354: step 5162, loss 2.89804e-06, acc 1\n",
            "2019-01-22T06:56:47.899465: step 5163, loss 0.000585488, acc 1\n",
            "2019-01-22T06:56:48.582774: step 5164, loss 2.09725e-06, acc 1\n",
            "2019-01-22T06:56:49.257803: step 5165, loss 4.49645e-05, acc 1\n",
            "2019-01-22T06:56:49.929757: step 5166, loss 8.26206e-06, acc 1\n",
            "2019-01-22T06:56:50.607270: step 5167, loss 1.81329e-05, acc 1\n",
            "2019-01-22T06:56:51.286972: step 5168, loss 8.8513e-05, acc 1\n",
            "2019-01-22T06:56:51.956227: step 5169, loss 7.4862e-05, acc 1\n",
            "2019-01-22T06:56:52.631297: step 5170, loss 3.03095e-05, acc 1\n",
            "2019-01-22T06:56:53.307232: step 5171, loss 6.50313e-06, acc 1\n",
            "2019-01-22T06:56:53.985474: step 5172, loss 0.000259479, acc 1\n",
            "2019-01-22T06:56:54.658540: step 5173, loss 1.60187e-07, acc 1\n",
            "2019-01-22T06:56:55.331617: step 5174, loss 4.50695e-06, acc 1\n",
            "2019-01-22T06:56:56.013631: step 5175, loss 0.000582102, acc 1\n",
            "2019-01-22T06:56:56.687700: step 5176, loss 1.57031e-05, acc 1\n",
            "2019-01-22T06:56:57.362353: step 5177, loss 5.99877e-05, acc 1\n",
            "2019-01-22T06:56:58.037816: step 5178, loss 0.00204733, acc 1\n",
            "2019-01-22T06:56:58.714602: step 5179, loss 1.58324e-07, acc 1\n",
            "2019-01-22T06:56:59.390012: step 5180, loss 3.02222e-05, acc 1\n",
            "2019-01-22T06:56:59.800085: step 5181, loss 0.00680501, acc 1\n",
            "2019-01-22T06:57:00.475274: step 5182, loss 0.00206617, acc 1\n",
            "2019-01-22T06:57:01.149451: step 5183, loss 2.3283e-07, acc 1\n",
            "2019-01-22T06:57:01.830392: step 5184, loss 2.51456e-07, acc 1\n",
            "2019-01-22T06:57:02.500943: step 5185, loss 1.73364e-05, acc 1\n",
            "2019-01-22T06:57:03.176600: step 5186, loss 5.77419e-08, acc 1\n",
            "2019-01-22T06:57:03.850382: step 5187, loss 7.59154e-05, acc 1\n",
            "2019-01-22T06:57:04.525090: step 5188, loss 0.00111517, acc 1\n",
            "2019-01-22T06:57:05.197726: step 5189, loss 5.299e-06, acc 1\n",
            "2019-01-22T06:57:05.870528: step 5190, loss 3.3059e-06, acc 1\n",
            "2019-01-22T06:57:06.553710: step 5191, loss 2.74542e-06, acc 1\n",
            "2019-01-22T06:57:07.226626: step 5192, loss 2.03028e-07, acc 1\n",
            "2019-01-22T06:57:07.898877: step 5193, loss 3.97717e-05, acc 1\n",
            "2019-01-22T06:57:08.578973: step 5194, loss 3.1067e-06, acc 1\n",
            "2019-01-22T06:57:09.264326: step 5195, loss 8.93885e-06, acc 1\n",
            "2019-01-22T06:57:09.940940: step 5196, loss 2.02611e-05, acc 1\n",
            "2019-01-22T06:57:10.625485: step 5197, loss 0.000110781, acc 1\n",
            "2019-01-22T06:57:11.305193: step 5198, loss 3.33413e-07, acc 1\n",
            "2019-01-22T06:57:11.982270: step 5199, loss 7.37593e-07, acc 1\n",
            "2019-01-22T06:57:12.665944: step 5200, loss 7.5466e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:57:13.026747: step 5200, loss 3.63817e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5200\n",
            "\n",
            "2019-01-22T06:57:13.845108: step 5201, loss 1.80006e-05, acc 1\n",
            "2019-01-22T06:57:14.521922: step 5202, loss 4.83131e-06, acc 1\n",
            "2019-01-22T06:57:15.196833: step 5203, loss 7.26188e-06, acc 1\n",
            "2019-01-22T06:57:15.865878: step 5204, loss 1.27959e-06, acc 1\n",
            "2019-01-22T06:57:16.546585: step 5205, loss 3.50176e-07, acc 1\n",
            "2019-01-22T06:57:17.218757: step 5206, loss 7.22691e-07, acc 1\n",
            "2019-01-22T06:57:17.897617: step 5207, loss 1.67638e-07, acc 1\n",
            "2019-01-22T06:57:18.574112: step 5208, loss 4.92297e-05, acc 1\n",
            "2019-01-22T06:57:19.252526: step 5209, loss 1.65026e-06, acc 1\n",
            "2019-01-22T06:57:19.926728: step 5210, loss 4.43294e-05, acc 1\n",
            "2019-01-22T06:57:20.605882: step 5211, loss 2.28353e-06, acc 1\n",
            "2019-01-22T06:57:21.283010: step 5212, loss 3.69416e-05, acc 1\n",
            "2019-01-22T06:57:21.968033: step 5213, loss 0.086587, acc 0.984375\n",
            "2019-01-22T06:57:22.376744: step 5214, loss 3.06538e-08, acc 1\n",
            "2019-01-22T06:57:23.052039: step 5215, loss 0.000815481, acc 1\n",
            "2019-01-22T06:57:23.731798: step 5216, loss 0.0765259, acc 0.984375\n",
            "2019-01-22T06:57:24.407099: step 5217, loss 0.016423, acc 0.984375\n",
            "2019-01-22T06:57:25.084412: step 5218, loss 0.00130948, acc 1\n",
            "2019-01-22T06:57:25.761134: step 5219, loss 6.21698e-06, acc 1\n",
            "2019-01-22T06:57:26.438587: step 5220, loss 4.82422e-07, acc 1\n",
            "2019-01-22T06:57:27.121225: step 5221, loss 1.7889e-05, acc 1\n",
            "2019-01-22T06:57:27.795940: step 5222, loss 5.2355e-06, acc 1\n",
            "2019-01-22T06:57:28.470099: step 5223, loss 1.92287e-05, acc 1\n",
            "2019-01-22T06:57:29.160952: step 5224, loss 2.50887e-06, acc 1\n",
            "2019-01-22T06:57:29.832888: step 5225, loss 4.19092e-07, acc 1\n",
            "2019-01-22T06:57:30.506027: step 5226, loss 3.63214e-07, acc 1\n",
            "2019-01-22T06:57:31.187754: step 5227, loss 5.01049e-07, acc 1\n",
            "2019-01-22T06:57:31.864925: step 5228, loss 0.000733919, acc 1\n",
            "2019-01-22T06:57:32.542680: step 5229, loss 0.000438324, acc 1\n",
            "2019-01-22T06:57:33.219711: step 5230, loss 1.32674e-05, acc 1\n",
            "2019-01-22T06:57:33.898888: step 5231, loss 4.51938e-05, acc 1\n",
            "2019-01-22T06:57:34.577150: step 5232, loss 0.000304388, acc 1\n",
            "2019-01-22T06:57:35.248450: step 5233, loss 0.000188467, acc 1\n",
            "2019-01-22T06:57:35.917993: step 5234, loss 4.61879e-06, acc 1\n",
            "2019-01-22T06:57:36.590533: step 5235, loss 4.01307e-05, acc 1\n",
            "2019-01-22T06:57:37.265888: step 5236, loss 4.84251e-06, acc 1\n",
            "2019-01-22T06:57:37.938424: step 5237, loss 0.00828028, acc 1\n",
            "2019-01-22T06:57:38.613421: step 5238, loss 0.00105016, acc 1\n",
            "2019-01-22T06:57:39.293490: step 5239, loss 1.40255e-06, acc 1\n",
            "2019-01-22T06:57:39.970654: step 5240, loss 0.0183943, acc 0.984375\n",
            "2019-01-22T06:57:40.647337: step 5241, loss 6.12719e-06, acc 1\n",
            "2019-01-22T06:57:41.318498: step 5242, loss 0.000418999, acc 1\n",
            "2019-01-22T06:57:41.995869: step 5243, loss 3.74289e-05, acc 1\n",
            "2019-01-22T06:57:42.673261: step 5244, loss 0.000104418, acc 1\n",
            "2019-01-22T06:57:43.350789: step 5245, loss 4.31215e-05, acc 1\n",
            "2019-01-22T06:57:44.027446: step 5246, loss 8.33673e-05, acc 1\n",
            "2019-01-22T06:57:44.435957: step 5247, loss 1.84598e-06, acc 1\n",
            "2019-01-22T06:57:45.112495: step 5248, loss 0.000452756, acc 1\n",
            "2019-01-22T06:57:45.786910: step 5249, loss 2.42448e-05, acc 1\n",
            "2019-01-22T06:57:46.461108: step 5250, loss 1.13621e-07, acc 1\n",
            "2019-01-22T06:57:47.134653: step 5251, loss 9.19761e-05, acc 1\n",
            "2019-01-22T06:57:47.812255: step 5252, loss 5.33951e-06, acc 1\n",
            "2019-01-22T06:57:48.488572: step 5253, loss 8.50886e-05, acc 1\n",
            "2019-01-22T06:57:49.162389: step 5254, loss 0.000144431, acc 1\n",
            "2019-01-22T06:57:49.843381: step 5255, loss 0.000287237, acc 1\n",
            "2019-01-22T06:57:50.519248: step 5256, loss 3.22514e-05, acc 1\n",
            "2019-01-22T06:57:51.195544: step 5257, loss 0.000365993, acc 1\n",
            "2019-01-22T06:57:51.869743: step 5258, loss 8.34528e-06, acc 1\n",
            "2019-01-22T06:57:52.554434: step 5259, loss 9.90078e-06, acc 1\n",
            "2019-01-22T06:57:53.229554: step 5260, loss 0.000655702, acc 1\n",
            "2019-01-22T06:57:53.908050: step 5261, loss 0.00975473, acc 1\n",
            "2019-01-22T06:57:54.584889: step 5262, loss 0.000140741, acc 1\n",
            "2019-01-22T06:57:55.264546: step 5263, loss 0.000391379, acc 1\n",
            "2019-01-22T06:57:55.940181: step 5264, loss 1.05863e-05, acc 1\n",
            "2019-01-22T06:57:56.614100: step 5265, loss 4.45168e-07, acc 1\n",
            "2019-01-22T06:57:57.291005: step 5266, loss 1.27273e-05, acc 1\n",
            "2019-01-22T06:57:57.976086: step 5267, loss 6.0273e-05, acc 1\n",
            "2019-01-22T06:57:58.650627: step 5268, loss 2.36735e-06, acc 1\n",
            "2019-01-22T06:57:59.324248: step 5269, loss 7.86019e-07, acc 1\n",
            "2019-01-22T06:58:00.007654: step 5270, loss 0.00105353, acc 1\n",
            "2019-01-22T06:58:00.686508: step 5271, loss 4.25e-06, acc 1\n",
            "2019-01-22T06:58:01.361434: step 5272, loss 1.48568e-05, acc 1\n",
            "2019-01-22T06:58:02.047604: step 5273, loss 2.85206e-05, acc 1\n",
            "2019-01-22T06:58:02.724911: step 5274, loss 1.03331e-05, acc 1\n",
            "2019-01-22T06:58:03.399954: step 5275, loss 2.71185e-06, acc 1\n",
            "2019-01-22T06:58:04.080664: step 5276, loss 7.69268e-07, acc 1\n",
            "2019-01-22T06:58:04.763232: step 5277, loss 0.000158011, acc 1\n",
            "2019-01-22T06:58:05.439887: step 5278, loss 2.56927e-05, acc 1\n",
            "2019-01-22T06:58:06.111592: step 5279, loss 2.61449e-05, acc 1\n",
            "2019-01-22T06:58:06.519584: step 5280, loss 4.48497e-05, acc 1\n",
            "2019-01-22T06:58:07.200732: step 5281, loss 9.98587e-05, acc 1\n",
            "2019-01-22T06:58:07.875367: step 5282, loss 1.14364e-06, acc 1\n",
            "2019-01-22T06:58:08.550322: step 5283, loss 1.14768e-05, acc 1\n",
            "2019-01-22T06:58:09.228307: step 5284, loss 3.1924e-06, acc 1\n",
            "2019-01-22T06:58:09.905980: step 5285, loss 2.63032e-05, acc 1\n",
            "2019-01-22T06:58:10.580943: step 5286, loss 0.000125321, acc 1\n",
            "2019-01-22T06:58:11.254238: step 5287, loss 1.86926e-05, acc 1\n",
            "2019-01-22T06:58:11.931896: step 5288, loss 5.43999e-06, acc 1\n",
            "2019-01-22T06:58:12.607069: step 5289, loss 1.15667e-05, acc 1\n",
            "2019-01-22T06:58:13.280242: step 5290, loss 1.35412e-06, acc 1\n",
            "2019-01-22T06:58:13.953839: step 5291, loss 0.000323477, acc 1\n",
            "2019-01-22T06:58:14.632283: step 5292, loss 6.00791e-06, acc 1\n",
            "2019-01-22T06:58:15.305910: step 5293, loss 3.00966e-05, acc 1\n",
            "2019-01-22T06:58:15.979554: step 5294, loss 7.80431e-07, acc 1\n",
            "2019-01-22T06:58:16.657574: step 5295, loss 1.87375e-06, acc 1\n",
            "2019-01-22T06:58:17.330932: step 5296, loss 5.17033e-06, acc 1\n",
            "2019-01-22T06:58:18.008898: step 5297, loss 4.52776e-05, acc 1\n",
            "2019-01-22T06:58:18.679623: step 5298, loss 7.96478e-05, acc 1\n",
            "2019-01-22T06:58:19.363454: step 5299, loss 3.03052e-05, acc 1\n",
            "2019-01-22T06:58:20.038592: step 5300, loss 0.000225983, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:58:20.401433: step 5300, loss 6.75549e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5300\n",
            "\n",
            "2019-01-22T06:58:21.228718: step 5301, loss 5.9045e-07, acc 1\n",
            "2019-01-22T06:58:21.899696: step 5302, loss 2.25376e-06, acc 1\n",
            "2019-01-22T06:58:22.586086: step 5303, loss 0.000773155, acc 1\n",
            "2019-01-22T06:58:23.267173: step 5304, loss 2.2779e-06, acc 1\n",
            "2019-01-22T06:58:23.941149: step 5305, loss 2.74911e-06, acc 1\n",
            "2019-01-22T06:58:24.616406: step 5306, loss 2.2514e-05, acc 1\n",
            "2019-01-22T06:58:25.289712: step 5307, loss 2.64494e-07, acc 1\n",
            "2019-01-22T06:58:25.968686: step 5308, loss 0.000300418, acc 1\n",
            "2019-01-22T06:58:26.641264: step 5309, loss 2.81258e-07, acc 1\n",
            "2019-01-22T06:58:27.315414: step 5310, loss 1.22934e-07, acc 1\n",
            "2019-01-22T06:58:27.996074: step 5311, loss 0.000182745, acc 1\n",
            "2019-01-22T06:58:28.667416: step 5312, loss 6.76201e-05, acc 1\n",
            "2019-01-22T06:58:29.075923: step 5313, loss 1.31951e-05, acc 1\n",
            "2019-01-22T06:58:29.749916: step 5314, loss 8.1769e-07, acc 1\n",
            "2019-01-22T06:58:30.422467: step 5315, loss 7.54363e-07, acc 1\n",
            "2019-01-22T06:58:31.102924: step 5316, loss 5.1437e-05, acc 1\n",
            "2019-01-22T06:58:31.779888: step 5317, loss 0.00649786, acc 1\n",
            "2019-01-22T06:58:32.457748: step 5318, loss 7.76364e-06, acc 1\n",
            "2019-01-22T06:58:33.131526: step 5319, loss 4.80873e-06, acc 1\n",
            "2019-01-22T06:58:33.806568: step 5320, loss 2.22942e-05, acc 1\n",
            "2019-01-22T06:58:34.481727: step 5321, loss 3.41252e-05, acc 1\n",
            "2019-01-22T06:58:35.153772: step 5322, loss 6.49419e-06, acc 1\n",
            "2019-01-22T06:58:35.835676: step 5323, loss 4.75019e-05, acc 1\n",
            "2019-01-22T06:58:36.509400: step 5324, loss 1.03593e-05, acc 1\n",
            "2019-01-22T06:58:37.183612: step 5325, loss 0.000167718, acc 1\n",
            "2019-01-22T06:58:37.856992: step 5326, loss 1.61211e-05, acc 1\n",
            "2019-01-22T06:58:38.537256: step 5327, loss 2.41472e-05, acc 1\n",
            "2019-01-22T06:58:39.212121: step 5328, loss 0.000135578, acc 1\n",
            "2019-01-22T06:58:39.887505: step 5329, loss 3.4255e-05, acc 1\n",
            "2019-01-22T06:58:40.561686: step 5330, loss 0.000250209, acc 1\n",
            "2019-01-22T06:58:41.241090: step 5331, loss 4.56211e-05, acc 1\n",
            "2019-01-22T06:58:41.916334: step 5332, loss 0.000103684, acc 1\n",
            "2019-01-22T06:58:42.593910: step 5333, loss 0.000255814, acc 1\n",
            "2019-01-22T06:58:43.271178: step 5334, loss 0.000121623, acc 1\n",
            "2019-01-22T06:58:43.952901: step 5335, loss 9.31322e-09, acc 1\n",
            "2019-01-22T06:58:44.626592: step 5336, loss 8.5681e-07, acc 1\n",
            "2019-01-22T06:58:45.305846: step 5337, loss 2.46598e-06, acc 1\n",
            "2019-01-22T06:58:45.986055: step 5338, loss 5.36309e-05, acc 1\n",
            "2019-01-22T06:58:46.671322: step 5339, loss 1.2678e-05, acc 1\n",
            "2019-01-22T06:58:47.352587: step 5340, loss 2.30209e-06, acc 1\n",
            "2019-01-22T06:58:48.029969: step 5341, loss 1.18316e-05, acc 1\n",
            "2019-01-22T06:58:48.706656: step 5342, loss 0.00188701, acc 1\n",
            "2019-01-22T06:58:49.393767: step 5343, loss 8.38337e-05, acc 1\n",
            "2019-01-22T06:58:50.067936: step 5344, loss 1.14924e-06, acc 1\n",
            "2019-01-22T06:58:50.745063: step 5345, loss 8.26149e-05, acc 1\n",
            "2019-01-22T06:58:51.156364: step 5346, loss 5.89342e-05, acc 1\n",
            "2019-01-22T06:58:51.833556: step 5347, loss 0.000118279, acc 1\n",
            "2019-01-22T06:58:52.512932: step 5348, loss 0.0625773, acc 0.984375\n",
            "2019-01-22T06:58:53.192915: step 5349, loss 2.20566e-05, acc 1\n",
            "2019-01-22T06:58:53.868213: step 5350, loss 4.47739e-06, acc 1\n",
            "2019-01-22T06:58:54.550870: step 5351, loss 1.5923e-05, acc 1\n",
            "2019-01-22T06:58:55.227267: step 5352, loss 0.000139275, acc 1\n",
            "2019-01-22T06:58:55.909533: step 5353, loss 0.000129249, acc 1\n",
            "2019-01-22T06:58:56.594379: step 5354, loss 2.82743e-05, acc 1\n",
            "2019-01-22T06:58:57.270095: step 5355, loss 0.0169034, acc 0.984375\n",
            "2019-01-22T06:58:57.950134: step 5356, loss 0.000259938, acc 1\n",
            "2019-01-22T06:58:58.634640: step 5357, loss 0.000289874, acc 1\n",
            "2019-01-22T06:58:59.311912: step 5358, loss 0.00608805, acc 1\n",
            "2019-01-22T06:58:59.988942: step 5359, loss 0.00306241, acc 1\n",
            "2019-01-22T06:59:00.679723: step 5360, loss 9.55031e-05, acc 1\n",
            "2019-01-22T06:59:01.357906: step 5361, loss 5.06399e-05, acc 1\n",
            "2019-01-22T06:59:02.034346: step 5362, loss 0.000390936, acc 1\n",
            "2019-01-22T06:59:02.719348: step 5363, loss 0.00103478, acc 1\n",
            "2019-01-22T06:59:03.395932: step 5364, loss 2.07424e-05, acc 1\n",
            "2019-01-22T06:59:04.071430: step 5365, loss 9.44589e-05, acc 1\n",
            "2019-01-22T06:59:04.758547: step 5366, loss 0.000451867, acc 1\n",
            "2019-01-22T06:59:05.438811: step 5367, loss 0.163021, acc 0.96875\n",
            "2019-01-22T06:59:06.118033: step 5368, loss 3.61838e-05, acc 1\n",
            "2019-01-22T06:59:06.794992: step 5369, loss 0.000136963, acc 1\n",
            "2019-01-22T06:59:07.472664: step 5370, loss 0.000128753, acc 1\n",
            "2019-01-22T06:59:08.149416: step 5371, loss 0.000106216, acc 1\n",
            "2019-01-22T06:59:08.828139: step 5372, loss 0.00651012, acc 1\n",
            "2019-01-22T06:59:09.508853: step 5373, loss 0.00173344, acc 1\n",
            "2019-01-22T06:59:10.188442: step 5374, loss 0.0279291, acc 0.984375\n",
            "2019-01-22T06:59:10.865197: step 5375, loss 3.3539e-05, acc 1\n",
            "2019-01-22T06:59:11.543588: step 5376, loss 3.87798e-05, acc 1\n",
            "2019-01-22T06:59:12.220737: step 5377, loss 0.00012998, acc 1\n",
            "2019-01-22T06:59:12.893669: step 5378, loss 8.71709e-07, acc 1\n",
            "2019-01-22T06:59:13.303601: step 5379, loss 5.28983e-05, acc 1\n",
            "2019-01-22T06:59:13.981404: step 5380, loss 3.67275e-05, acc 1\n",
            "2019-01-22T06:59:14.655600: step 5381, loss 1.20869e-05, acc 1\n",
            "2019-01-22T06:59:15.331943: step 5382, loss 0.0715517, acc 0.984375\n",
            "2019-01-22T06:59:16.012789: step 5383, loss 0.000343945, acc 1\n",
            "2019-01-22T06:59:16.689430: step 5384, loss 6.09758e-05, acc 1\n",
            "2019-01-22T06:59:17.363711: step 5385, loss 2.32663e-05, acc 1\n",
            "2019-01-22T06:59:18.041409: step 5386, loss 0.000277255, acc 1\n",
            "2019-01-22T06:59:18.728094: step 5387, loss 0.0140287, acc 0.984375\n",
            "2019-01-22T06:59:19.406375: step 5388, loss 4.41958e-06, acc 1\n",
            "2019-01-22T06:59:20.084319: step 5389, loss 1.7819e-05, acc 1\n",
            "2019-01-22T06:59:20.759771: step 5390, loss 0.000637646, acc 1\n",
            "2019-01-22T06:59:21.435435: step 5391, loss 8.9839e-06, acc 1\n",
            "2019-01-22T06:59:22.111767: step 5392, loss 7.5069e-05, acc 1\n",
            "2019-01-22T06:59:22.788821: step 5393, loss 0.075596, acc 0.984375\n",
            "2019-01-22T06:59:23.462007: step 5394, loss 1.83833e-06, acc 1\n",
            "2019-01-22T06:59:24.140064: step 5395, loss 0.00526907, acc 1\n",
            "2019-01-22T06:59:24.817576: step 5396, loss 3.26495e-06, acc 1\n",
            "2019-01-22T06:59:25.495667: step 5397, loss 1.27819e-05, acc 1\n",
            "2019-01-22T06:59:26.172465: step 5398, loss 0.000223467, acc 1\n",
            "2019-01-22T06:59:26.852298: step 5399, loss 0.00011586, acc 1\n",
            "2019-01-22T06:59:27.525321: step 5400, loss 5.28355e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T06:59:27.887303: step 5400, loss 0.000126963, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5400\n",
            "\n",
            "2019-01-22T06:59:28.709738: step 5401, loss 2.06375e-06, acc 1\n",
            "2019-01-22T06:59:29.383799: step 5402, loss 1.9744e-07, acc 1\n",
            "2019-01-22T06:59:30.065452: step 5403, loss 7.74234e-06, acc 1\n",
            "2019-01-22T06:59:30.739387: step 5404, loss 0.0528541, acc 0.984375\n",
            "2019-01-22T06:59:31.413807: step 5405, loss 8.20025e-05, acc 1\n",
            "2019-01-22T06:59:32.093883: step 5406, loss 0.00043285, acc 1\n",
            "2019-01-22T06:59:32.774109: step 5407, loss 5.41791e-05, acc 1\n",
            "2019-01-22T06:59:33.451785: step 5408, loss 5.56528e-06, acc 1\n",
            "2019-01-22T06:59:34.126263: step 5409, loss 3.99336e-05, acc 1\n",
            "2019-01-22T06:59:34.801410: step 5410, loss 4.74199e-06, acc 1\n",
            "2019-01-22T06:59:35.478842: step 5411, loss 7.2059e-06, acc 1\n",
            "2019-01-22T06:59:35.893216: step 5412, loss 2.76637e-05, acc 1\n",
            "2019-01-22T06:59:36.571090: step 5413, loss 0.00701819, acc 1\n",
            "2019-01-22T06:59:37.245335: step 5414, loss 0.0167057, acc 0.984375\n",
            "2019-01-22T06:59:37.919888: step 5415, loss 2.00347e-05, acc 1\n",
            "2019-01-22T06:59:38.597068: step 5416, loss 0.000169648, acc 1\n",
            "2019-01-22T06:59:39.271518: step 5417, loss 0.00153734, acc 1\n",
            "2019-01-22T06:59:39.948207: step 5418, loss 0.0831377, acc 0.984375\n",
            "2019-01-22T06:59:40.626485: step 5419, loss 0.00145983, acc 1\n",
            "2019-01-22T06:59:41.309059: step 5420, loss 0.0107838, acc 1\n",
            "2019-01-22T06:59:41.987560: step 5421, loss 2.57633e-05, acc 1\n",
            "2019-01-22T06:59:42.662631: step 5422, loss 3.27348e-05, acc 1\n",
            "2019-01-22T06:59:43.341035: step 5423, loss 7.53486e-06, acc 1\n",
            "2019-01-22T06:59:44.017069: step 5424, loss 0.020061, acc 0.984375\n",
            "2019-01-22T06:59:44.695532: step 5425, loss 5.92073e-06, acc 1\n",
            "2019-01-22T06:59:45.369825: step 5426, loss 4.41875e-05, acc 1\n",
            "2019-01-22T06:59:46.047116: step 5427, loss 0.00411599, acc 1\n",
            "2019-01-22T06:59:46.729068: step 5428, loss 2.0883e-05, acc 1\n",
            "2019-01-22T06:59:47.402775: step 5429, loss 0.00047145, acc 1\n",
            "2019-01-22T06:59:48.075287: step 5430, loss 4.76829e-05, acc 1\n",
            "2019-01-22T06:59:48.751361: step 5431, loss 1.51961e-05, acc 1\n",
            "2019-01-22T06:59:49.426623: step 5432, loss 4.95402e-05, acc 1\n",
            "2019-01-22T06:59:50.107396: step 5433, loss 3.45111e-05, acc 1\n",
            "2019-01-22T06:59:50.780813: step 5434, loss 7.26673e-05, acc 1\n",
            "2019-01-22T06:59:51.455881: step 5435, loss 4.17773e-06, acc 1\n",
            "2019-01-22T06:59:52.138894: step 5436, loss 2.51457e-07, acc 1\n",
            "2019-01-22T06:59:52.813470: step 5437, loss 1.28467e-05, acc 1\n",
            "2019-01-22T06:59:53.484031: step 5438, loss 1.03561e-06, acc 1\n",
            "2019-01-22T06:59:54.169274: step 5439, loss 1.26011e-05, acc 1\n",
            "2019-01-22T06:59:54.844110: step 5440, loss 1.3635e-05, acc 1\n",
            "2019-01-22T06:59:55.516338: step 5441, loss 0.00152162, acc 1\n",
            "2019-01-22T06:59:56.191455: step 5442, loss 5.32822e-06, acc 1\n",
            "2019-01-22T06:59:56.871039: step 5443, loss 1.28522e-07, acc 1\n",
            "2019-01-22T06:59:57.545112: step 5444, loss 9.64841e-07, acc 1\n",
            "2019-01-22T06:59:57.955924: step 5445, loss 7.80601e-06, acc 1\n",
            "2019-01-22T06:59:58.632998: step 5446, loss 0.000212233, acc 1\n",
            "2019-01-22T06:59:59.314085: step 5447, loss 0.000319409, acc 1\n",
            "2019-01-22T06:59:59.989466: step 5448, loss 0.000132336, acc 1\n",
            "2019-01-22T07:00:00.664048: step 5449, loss 0.0493748, acc 0.984375\n",
            "2019-01-22T07:00:01.341354: step 5450, loss 1.08032e-06, acc 1\n",
            "2019-01-22T07:00:02.016303: step 5451, loss 5.36399e-05, acc 1\n",
            "2019-01-22T07:00:02.696177: step 5452, loss 4.91185e-05, acc 1\n",
            "2019-01-22T07:00:03.368086: step 5453, loss 9.87735e-05, acc 1\n",
            "2019-01-22T07:00:04.045126: step 5454, loss 1.78436e-06, acc 1\n",
            "2019-01-22T07:00:04.716519: step 5455, loss 0.00311924, acc 1\n",
            "2019-01-22T07:00:05.395012: step 5456, loss 0.00362297, acc 1\n",
            "2019-01-22T07:00:06.068088: step 5457, loss 0.000854342, acc 1\n",
            "2019-01-22T07:00:06.745201: step 5458, loss 0.000168246, acc 1\n",
            "2019-01-22T07:00:07.417966: step 5459, loss 0.00608544, acc 1\n",
            "2019-01-22T07:00:08.093627: step 5460, loss 1.51624e-05, acc 1\n",
            "2019-01-22T07:00:08.765448: step 5461, loss 7.10277e-06, acc 1\n",
            "2019-01-22T07:00:09.445063: step 5462, loss 2.32908e-05, acc 1\n",
            "2019-01-22T07:00:10.127575: step 5463, loss 0.0023109, acc 1\n",
            "2019-01-22T07:00:10.801339: step 5464, loss 4.16808e-05, acc 1\n",
            "2019-01-22T07:00:11.475947: step 5465, loss 0.000506694, acc 1\n",
            "2019-01-22T07:00:12.147924: step 5466, loss 4.81268e-06, acc 1\n",
            "2019-01-22T07:00:12.823782: step 5467, loss 1.30942e-06, acc 1\n",
            "2019-01-22T07:00:13.496813: step 5468, loss 0.000310708, acc 1\n",
            "2019-01-22T07:00:14.170788: step 5469, loss 4.98094e-05, acc 1\n",
            "2019-01-22T07:00:14.853631: step 5470, loss 1.41637e-05, acc 1\n",
            "2019-01-22T07:00:15.527664: step 5471, loss 0.00364383, acc 1\n",
            "2019-01-22T07:00:16.205080: step 5472, loss 0.00404726, acc 1\n",
            "2019-01-22T07:00:16.883008: step 5473, loss 0.00269293, acc 1\n",
            "2019-01-22T07:00:17.563550: step 5474, loss 3.79531e-05, acc 1\n",
            "2019-01-22T07:00:18.235498: step 5475, loss 0.000340683, acc 1\n",
            "2019-01-22T07:00:18.921770: step 5476, loss 1.9077e-05, acc 1\n",
            "2019-01-22T07:00:19.599070: step 5477, loss 0.000567152, acc 1\n",
            "2019-01-22T07:00:20.012143: step 5478, loss 1.64848e-06, acc 1\n",
            "2019-01-22T07:00:20.688089: step 5479, loss 7.38532e-05, acc 1\n",
            "2019-01-22T07:00:21.373290: step 5480, loss 0.000660447, acc 1\n",
            "2019-01-22T07:00:22.048036: step 5481, loss 7.95026e-06, acc 1\n",
            "2019-01-22T07:00:22.722591: step 5482, loss 4.07128e-06, acc 1\n",
            "2019-01-22T07:00:23.402679: step 5483, loss 2.77525e-06, acc 1\n",
            "2019-01-22T07:00:24.088028: step 5484, loss 2.30591e-05, acc 1\n",
            "2019-01-22T07:00:24.762325: step 5485, loss 3.54065e-06, acc 1\n",
            "2019-01-22T07:00:25.439198: step 5486, loss 1.25103e-05, acc 1\n",
            "2019-01-22T07:00:26.123141: step 5487, loss 7.84943e-05, acc 1\n",
            "2019-01-22T07:00:26.800060: step 5488, loss 1.67231e-05, acc 1\n",
            "2019-01-22T07:00:27.477393: step 5489, loss 0.000195853, acc 1\n",
            "2019-01-22T07:00:28.160574: step 5490, loss 4.67522e-07, acc 1\n",
            "2019-01-22T07:00:28.833669: step 5491, loss 0.00131409, acc 1\n",
            "2019-01-22T07:00:29.510713: step 5492, loss 0.0021274, acc 1\n",
            "2019-01-22T07:00:30.189356: step 5493, loss 1.505e-06, acc 1\n",
            "2019-01-22T07:00:30.866870: step 5494, loss 9.57371e-07, acc 1\n",
            "2019-01-22T07:00:31.542058: step 5495, loss 3.45874e-06, acc 1\n",
            "2019-01-22T07:00:32.226127: step 5496, loss 1.66683e-05, acc 1\n",
            "2019-01-22T07:00:32.901361: step 5497, loss 2.32033e-05, acc 1\n",
            "2019-01-22T07:00:33.576575: step 5498, loss 3.57626e-07, acc 1\n",
            "2019-01-22T07:00:34.252221: step 5499, loss 0.000379876, acc 1\n",
            "2019-01-22T07:00:34.930568: step 5500, loss 3.37113e-06, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:00:35.294730: step 5500, loss 0.000120176, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5500\n",
            "\n",
            "2019-01-22T07:00:36.132794: step 5501, loss 7.37604e-07, acc 1\n",
            "2019-01-22T07:00:36.805375: step 5502, loss 1.08471e-05, acc 1\n",
            "2019-01-22T07:00:37.477833: step 5503, loss 8.72354e-06, acc 1\n",
            "2019-01-22T07:00:38.152573: step 5504, loss 2.51832e-05, acc 1\n",
            "2019-01-22T07:00:38.828462: step 5505, loss 0.000147791, acc 1\n",
            "2019-01-22T07:00:39.504867: step 5506, loss 8.79013e-06, acc 1\n",
            "2019-01-22T07:00:40.179961: step 5507, loss 0.00122072, acc 1\n",
            "2019-01-22T07:00:40.862079: step 5508, loss 8.79149e-06, acc 1\n",
            "2019-01-22T07:00:41.540981: step 5509, loss 6.88878e-06, acc 1\n",
            "2019-01-22T07:00:42.215750: step 5510, loss 0.00102246, acc 1\n",
            "2019-01-22T07:00:42.627655: step 5511, loss 0.000225129, acc 1\n",
            "2019-01-22T07:00:43.303872: step 5512, loss 1.84314e-05, acc 1\n",
            "2019-01-22T07:00:43.979712: step 5513, loss 0.000449893, acc 1\n",
            "2019-01-22T07:00:44.659123: step 5514, loss 1.32248e-07, acc 1\n",
            "2019-01-22T07:00:45.336370: step 5515, loss 6.35672e-06, acc 1\n",
            "2019-01-22T07:00:46.009681: step 5516, loss 0.0412885, acc 0.984375\n",
            "2019-01-22T07:00:46.688546: step 5517, loss 6.33955e-06, acc 1\n",
            "2019-01-22T07:00:47.361095: step 5518, loss 5.62248e-06, acc 1\n",
            "2019-01-22T07:00:48.034399: step 5519, loss 7.38412e-06, acc 1\n",
            "2019-01-22T07:00:48.719250: step 5520, loss 0.000481385, acc 1\n",
            "2019-01-22T07:00:49.396702: step 5521, loss 8.10037e-05, acc 1\n",
            "2019-01-22T07:00:50.073502: step 5522, loss 0.00549098, acc 1\n",
            "2019-01-22T07:00:50.754576: step 5523, loss 0.00020592, acc 1\n",
            "2019-01-22T07:00:51.430005: step 5524, loss 3.83848e-06, acc 1\n",
            "2019-01-22T07:00:52.107121: step 5525, loss 3.63191e-06, acc 1\n",
            "2019-01-22T07:00:52.780979: step 5526, loss 1.5013e-05, acc 1\n",
            "2019-01-22T07:00:53.451578: step 5527, loss 4.61169e-06, acc 1\n",
            "2019-01-22T07:00:54.122607: step 5528, loss 0.0281848, acc 0.984375\n",
            "2019-01-22T07:00:54.801950: step 5529, loss 1.38466e-05, acc 1\n",
            "2019-01-22T07:00:55.473545: step 5530, loss 2.48394e-05, acc 1\n",
            "2019-01-22T07:00:56.146594: step 5531, loss 2.52569e-05, acc 1\n",
            "2019-01-22T07:00:56.832352: step 5532, loss 1.02817e-06, acc 1\n",
            "2019-01-22T07:00:57.505375: step 5533, loss 2.79395e-07, acc 1\n",
            "2019-01-22T07:00:58.178707: step 5534, loss 6.75291e-06, acc 1\n",
            "2019-01-22T07:00:58.852645: step 5535, loss 5.82167e-06, acc 1\n",
            "2019-01-22T07:00:59.532909: step 5536, loss 6.69493e-06, acc 1\n",
            "2019-01-22T07:01:00.206337: step 5537, loss 8.932e-05, acc 1\n",
            "2019-01-22T07:01:00.878120: step 5538, loss 7.52218e-05, acc 1\n",
            "2019-01-22T07:01:01.551492: step 5539, loss 3.1079e-05, acc 1\n",
            "2019-01-22T07:01:02.224227: step 5540, loss 0.000360524, acc 1\n",
            "2019-01-22T07:01:02.900309: step 5541, loss 8.91778e-06, acc 1\n",
            "2019-01-22T07:01:03.574676: step 5542, loss 8.60594e-05, acc 1\n",
            "2019-01-22T07:01:04.257972: step 5543, loss 2.74537e-06, acc 1\n",
            "2019-01-22T07:01:04.668774: step 5544, loss 0.000471356, acc 1\n",
            "2019-01-22T07:01:05.343144: step 5545, loss 3.68442e-05, acc 1\n",
            "2019-01-22T07:01:06.014992: step 5546, loss 0.000136218, acc 1\n",
            "2019-01-22T07:01:06.685377: step 5547, loss 0.00220727, acc 1\n",
            "2019-01-22T07:01:07.367918: step 5548, loss 9.20134e-07, acc 1\n",
            "2019-01-22T07:01:08.044234: step 5549, loss 5.74022e-06, acc 1\n",
            "2019-01-22T07:01:08.717312: step 5550, loss 2.61193e-05, acc 1\n",
            "2019-01-22T07:01:09.390242: step 5551, loss 4.75811e-05, acc 1\n",
            "2019-01-22T07:01:10.069378: step 5552, loss 3.34329e-06, acc 1\n",
            "2019-01-22T07:01:10.742287: step 5553, loss 3.42776e-05, acc 1\n",
            "2019-01-22T07:01:11.423316: step 5554, loss 0.0357792, acc 0.984375\n",
            "2019-01-22T07:01:12.093240: step 5555, loss 0.000132939, acc 1\n",
            "2019-01-22T07:01:12.765329: step 5556, loss 5.87215e-06, acc 1\n",
            "2019-01-22T07:01:13.451367: step 5557, loss 0.0143114, acc 0.984375\n",
            "2019-01-22T07:01:14.131301: step 5558, loss 4.50826e-05, acc 1\n",
            "2019-01-22T07:01:14.805992: step 5559, loss 1.86264e-08, acc 1\n",
            "2019-01-22T07:01:15.486877: step 5560, loss 6.22113e-07, acc 1\n",
            "2019-01-22T07:01:16.162532: step 5561, loss 0.00859704, acc 1\n",
            "2019-01-22T07:01:16.836330: step 5562, loss 1.15108e-06, acc 1\n",
            "2019-01-22T07:01:17.514699: step 5563, loss 4.47034e-08, acc 1\n",
            "2019-01-22T07:01:18.185294: step 5564, loss 6.33287e-07, acc 1\n",
            "2019-01-22T07:01:18.859818: step 5565, loss 0.0277431, acc 0.984375\n",
            "2019-01-22T07:01:19.538803: step 5566, loss 0.000252495, acc 1\n",
            "2019-01-22T07:01:20.214504: step 5567, loss 0.00529794, acc 1\n",
            "2019-01-22T07:01:20.887391: step 5568, loss 1.80676e-07, acc 1\n",
            "2019-01-22T07:01:21.561002: step 5569, loss 2.63545e-06, acc 1\n",
            "2019-01-22T07:01:22.231493: step 5570, loss 2.38779e-06, acc 1\n",
            "2019-01-22T07:01:22.906971: step 5571, loss 1.2666e-07, acc 1\n",
            "2019-01-22T07:01:23.579980: step 5572, loss 6.10943e-07, acc 1\n",
            "2019-01-22T07:01:24.258067: step 5573, loss 7.83535e-06, acc 1\n",
            "2019-01-22T07:01:24.937412: step 5574, loss 0.000166367, acc 1\n",
            "2019-01-22T07:01:25.614332: step 5575, loss 5.70591e-05, acc 1\n",
            "2019-01-22T07:01:26.285663: step 5576, loss 0.00122711, acc 1\n",
            "2019-01-22T07:01:26.695738: step 5577, loss 3.90299e-05, acc 1\n",
            "2019-01-22T07:01:27.373127: step 5578, loss 3.02105e-06, acc 1\n",
            "2019-01-22T07:01:28.052915: step 5579, loss 0.000787306, acc 1\n",
            "2019-01-22T07:01:28.724424: step 5580, loss 1.7782e-05, acc 1\n",
            "2019-01-22T07:01:29.393278: step 5581, loss 3.73775e-05, acc 1\n",
            "2019-01-22T07:01:30.069498: step 5582, loss 3.29687e-07, acc 1\n",
            "2019-01-22T07:01:30.740943: step 5583, loss 6.2911e-06, acc 1\n",
            "2019-01-22T07:01:31.414384: step 5584, loss 4.26711e-06, acc 1\n",
            "2019-01-22T07:01:32.089886: step 5585, loss 1.12876e-05, acc 1\n",
            "2019-01-22T07:01:32.764560: step 5586, loss 0.00030235, acc 1\n",
            "2019-01-22T07:01:33.437820: step 5587, loss 1.22883e-05, acc 1\n",
            "2019-01-22T07:01:34.116518: step 5588, loss 0.0365756, acc 0.984375\n",
            "2019-01-22T07:01:34.793376: step 5589, loss 0.000132021, acc 1\n",
            "2019-01-22T07:01:35.465922: step 5590, loss 0.0015887, acc 1\n",
            "2019-01-22T07:01:36.142782: step 5591, loss 3.93017e-07, acc 1\n",
            "2019-01-22T07:01:36.817586: step 5592, loss 4.11987e-06, acc 1\n",
            "2019-01-22T07:01:37.498655: step 5593, loss 5.08755e-05, acc 1\n",
            "2019-01-22T07:01:38.187047: step 5594, loss 3.05134e-05, acc 1\n",
            "2019-01-22T07:01:38.860461: step 5595, loss 4.12958e-05, acc 1\n",
            "2019-01-22T07:01:39.537013: step 5596, loss 5.04904e-06, acc 1\n",
            "2019-01-22T07:01:40.220479: step 5597, loss 0.0213887, acc 0.984375\n",
            "2019-01-22T07:01:40.896439: step 5598, loss 8.6463e-05, acc 1\n",
            "2019-01-22T07:01:41.573475: step 5599, loss 1.34501e-05, acc 1\n",
            "2019-01-22T07:01:42.256475: step 5600, loss 0.00202323, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:01:42.615595: step 5600, loss 0.000175818, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5600\n",
            "\n",
            "2019-01-22T07:01:43.431046: step 5601, loss 1.08405e-06, acc 1\n",
            "2019-01-22T07:01:44.110345: step 5602, loss 1.68749e-06, acc 1\n",
            "2019-01-22T07:01:44.787793: step 5603, loss 0.00389839, acc 1\n",
            "2019-01-22T07:01:45.460362: step 5604, loss 1.91104e-06, acc 1\n",
            "2019-01-22T07:01:46.136621: step 5605, loss 1.12502e-06, acc 1\n",
            "2019-01-22T07:01:46.814336: step 5606, loss 1.90731e-06, acc 1\n",
            "2019-01-22T07:01:47.491335: step 5607, loss 9.16396e-07, acc 1\n",
            "2019-01-22T07:01:48.163371: step 5608, loss 3.21706e-05, acc 1\n",
            "2019-01-22T07:01:48.843607: step 5609, loss 4.24681e-07, acc 1\n",
            "2019-01-22T07:01:49.253227: step 5610, loss 2.45903e-06, acc 1\n",
            "2019-01-22T07:01:49.926682: step 5611, loss 7.00839e-05, acc 1\n",
            "2019-01-22T07:01:50.601832: step 5612, loss 3.22236e-07, acc 1\n",
            "2019-01-22T07:01:51.283894: step 5613, loss 4.47014e-05, acc 1\n",
            "2019-01-22T07:01:51.964775: step 5614, loss 3.81813e-06, acc 1\n",
            "2019-01-22T07:01:52.642486: step 5615, loss 0.000117208, acc 1\n",
            "2019-01-22T07:01:53.316421: step 5616, loss 9.57383e-07, acc 1\n",
            "2019-01-22T07:01:53.993592: step 5617, loss 5.35449e-06, acc 1\n",
            "2019-01-22T07:01:54.678576: step 5618, loss 9.10821e-07, acc 1\n",
            "2019-01-22T07:01:55.352329: step 5619, loss 4.09781e-08, acc 1\n",
            "2019-01-22T07:01:56.026703: step 5620, loss 0.000142375, acc 1\n",
            "2019-01-22T07:01:56.705121: step 5621, loss 5.17183e-06, acc 1\n",
            "2019-01-22T07:01:57.378717: step 5622, loss 0.000102881, acc 1\n",
            "2019-01-22T07:01:58.051871: step 5623, loss 1.86265e-09, acc 1\n",
            "2019-01-22T07:01:58.731745: step 5624, loss 1.59067e-06, acc 1\n",
            "2019-01-22T07:01:59.404019: step 5625, loss 4.0181e-05, acc 1\n",
            "2019-01-22T07:02:00.082841: step 5626, loss 0.000707756, acc 1\n",
            "2019-01-22T07:02:00.760705: step 5627, loss 0.000696266, acc 1\n",
            "2019-01-22T07:02:01.436247: step 5628, loss 1.00609e-05, acc 1\n",
            "2019-01-22T07:02:02.114121: step 5629, loss 0.000340166, acc 1\n",
            "2019-01-22T07:02:02.790111: step 5630, loss 6.82758e-05, acc 1\n",
            "2019-01-22T07:02:03.465083: step 5631, loss 6.61235e-07, acc 1\n",
            "2019-01-22T07:02:04.140381: step 5632, loss 5.84861e-07, acc 1\n",
            "2019-01-22T07:02:04.818982: step 5633, loss 7.31637e-05, acc 1\n",
            "2019-01-22T07:02:05.490003: step 5634, loss 1.89235e-06, acc 1\n",
            "2019-01-22T07:02:06.165343: step 5635, loss 2.38176e-05, acc 1\n",
            "2019-01-22T07:02:06.837633: step 5636, loss 2.26483e-05, acc 1\n",
            "2019-01-22T07:02:07.513715: step 5637, loss 0.000984972, acc 1\n",
            "2019-01-22T07:02:08.189884: step 5638, loss 1.15618e-05, acc 1\n",
            "2019-01-22T07:02:08.865013: step 5639, loss 4.85523e-06, acc 1\n",
            "2019-01-22T07:02:09.539316: step 5640, loss 0.000124153, acc 1\n",
            "2019-01-22T07:02:10.214115: step 5641, loss 0.00694721, acc 1\n",
            "2019-01-22T07:02:10.887095: step 5642, loss 3.0695e-06, acc 1\n",
            "2019-01-22T07:02:11.300405: step 5643, loss 0.0153067, acc 1\n",
            "2019-01-22T07:02:11.974601: step 5644, loss 0.00547371, acc 1\n",
            "2019-01-22T07:02:12.651877: step 5645, loss 5.91659e-06, acc 1\n",
            "2019-01-22T07:02:13.325372: step 5646, loss 0.000635434, acc 1\n",
            "2019-01-22T07:02:14.001322: step 5647, loss 2.75089e-06, acc 1\n",
            "2019-01-22T07:02:14.676088: step 5648, loss 0.00389464, acc 1\n",
            "2019-01-22T07:02:15.354204: step 5649, loss 3.30421e-06, acc 1\n",
            "2019-01-22T07:02:16.030036: step 5650, loss 4.99131e-06, acc 1\n",
            "2019-01-22T07:02:16.703359: step 5651, loss 9.88468e-06, acc 1\n",
            "2019-01-22T07:02:17.375376: step 5652, loss 2.44006e-07, acc 1\n",
            "2019-01-22T07:02:18.053377: step 5653, loss 6.74706e-06, acc 1\n",
            "2019-01-22T07:02:18.724771: step 5654, loss 1.6864e-05, acc 1\n",
            "2019-01-22T07:02:19.404310: step 5655, loss 6.16528e-07, acc 1\n",
            "2019-01-22T07:02:20.080102: step 5656, loss 1.92034e-06, acc 1\n",
            "2019-01-22T07:02:20.753995: step 5657, loss 2.19791e-07, acc 1\n",
            "2019-01-22T07:02:21.439132: step 5658, loss 9.58975e-06, acc 1\n",
            "2019-01-22T07:02:22.123913: step 5659, loss 7.37593e-07, acc 1\n",
            "2019-01-22T07:02:22.795004: step 5660, loss 2.24481e-05, acc 1\n",
            "2019-01-22T07:02:23.481367: step 5661, loss 5.27231e-06, acc 1\n",
            "2019-01-22T07:02:24.156933: step 5662, loss 1.16348e-05, acc 1\n",
            "2019-01-22T07:02:24.832063: step 5663, loss 8.01307e-06, acc 1\n",
            "2019-01-22T07:02:25.509741: step 5664, loss 6.54786e-05, acc 1\n",
            "2019-01-22T07:02:26.181683: step 5665, loss 1.89989e-07, acc 1\n",
            "2019-01-22T07:02:26.856359: step 5666, loss 0.00623724, acc 1\n",
            "2019-01-22T07:02:27.529208: step 5667, loss 6.2025e-07, acc 1\n",
            "2019-01-22T07:02:28.208814: step 5668, loss 2.02092e-06, acc 1\n",
            "2019-01-22T07:02:28.884414: step 5669, loss 7.1711e-07, acc 1\n",
            "2019-01-22T07:02:29.560194: step 5670, loss 0.000235919, acc 1\n",
            "2019-01-22T07:02:30.233667: step 5671, loss 2.93175e-06, acc 1\n",
            "2019-01-22T07:02:30.910479: step 5672, loss 3.03924e-05, acc 1\n",
            "2019-01-22T07:02:31.587684: step 5673, loss 0.00137097, acc 1\n",
            "2019-01-22T07:02:32.270739: step 5674, loss 1.81604e-06, acc 1\n",
            "2019-01-22T07:02:32.946412: step 5675, loss 0.00136484, acc 1\n",
            "2019-01-22T07:02:33.359007: step 5676, loss 0.000364745, acc 1\n",
            "2019-01-22T07:02:34.035675: step 5677, loss 0.015453, acc 0.984375\n",
            "2019-01-22T07:02:34.706803: step 5678, loss 4.28248e-05, acc 1\n",
            "2019-01-22T07:02:35.379224: step 5679, loss 3.24362e-05, acc 1\n",
            "2019-01-22T07:02:36.056909: step 5680, loss 2.56252e-05, acc 1\n",
            "2019-01-22T07:02:36.734707: step 5681, loss 2.69332e-06, acc 1\n",
            "2019-01-22T07:02:37.413109: step 5682, loss 8.1403e-05, acc 1\n",
            "2019-01-22T07:02:38.087031: step 5683, loss 9.13307e-06, acc 1\n",
            "2019-01-22T07:02:38.764373: step 5684, loss 0.00242385, acc 1\n",
            "2019-01-22T07:02:39.442082: step 5685, loss 6.98581e-05, acc 1\n",
            "2019-01-22T07:02:40.120858: step 5686, loss 1.18651e-05, acc 1\n",
            "2019-01-22T07:02:40.798008: step 5687, loss 0.0041032, acc 1\n",
            "2019-01-22T07:02:41.473195: step 5688, loss 7.98985e-06, acc 1\n",
            "2019-01-22T07:02:42.146758: step 5689, loss 0.000128958, acc 1\n",
            "2019-01-22T07:02:42.823297: step 5690, loss 7.39967e-05, acc 1\n",
            "2019-01-22T07:02:43.498962: step 5691, loss 0.0455471, acc 0.984375\n",
            "2019-01-22T07:02:44.180018: step 5692, loss 0.000484532, acc 1\n",
            "2019-01-22T07:02:44.855597: step 5693, loss 1.20431e-05, acc 1\n",
            "2019-01-22T07:02:45.529124: step 5694, loss 1.31498e-06, acc 1\n",
            "2019-01-22T07:02:46.206427: step 5695, loss 5.12944e-06, acc 1\n",
            "2019-01-22T07:02:46.888605: step 5696, loss 0.000300813, acc 1\n",
            "2019-01-22T07:02:47.562505: step 5697, loss 1.08514e-05, acc 1\n",
            "2019-01-22T07:02:48.244268: step 5698, loss 3.7511e-06, acc 1\n",
            "2019-01-22T07:02:48.918040: step 5699, loss 0.000834455, acc 1\n",
            "2019-01-22T07:02:49.600349: step 5700, loss 1.45839e-06, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:02:49.963914: step 5700, loss 0.000113312, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5700\n",
            "\n",
            "2019-01-22T07:02:50.790185: step 5701, loss 1.09459e-05, acc 1\n",
            "2019-01-22T07:02:51.466631: step 5702, loss 0.000332961, acc 1\n",
            "2019-01-22T07:02:52.136685: step 5703, loss 1.11571e-06, acc 1\n",
            "2019-01-22T07:02:52.818191: step 5704, loss 3.3786e-06, acc 1\n",
            "2019-01-22T07:02:53.489011: step 5705, loss 6.88486e-05, acc 1\n",
            "2019-01-22T07:02:54.162613: step 5706, loss 3.03263e-05, acc 1\n",
            "2019-01-22T07:02:54.837271: step 5707, loss 3.48119e-06, acc 1\n",
            "2019-01-22T07:02:55.514317: step 5708, loss 4.06787e-06, acc 1\n",
            "2019-01-22T07:02:55.925985: step 5709, loss 0.00120636, acc 1\n",
            "2019-01-22T07:02:56.604350: step 5710, loss 0.0012568, acc 1\n",
            "2019-01-22T07:02:57.279530: step 5711, loss 1.76757e-06, acc 1\n",
            "2019-01-22T07:02:57.955019: step 5712, loss 4.48836e-06, acc 1\n",
            "2019-01-22T07:02:58.637641: step 5713, loss 8.12669e-05, acc 1\n",
            "2019-01-22T07:02:59.310148: step 5714, loss 8.32198e-06, acc 1\n",
            "2019-01-22T07:02:59.984009: step 5715, loss 5.23354e-06, acc 1\n",
            "2019-01-22T07:03:00.659687: step 5716, loss 1.12358e-05, acc 1\n",
            "2019-01-22T07:03:01.339379: step 5717, loss 0.000317469, acc 1\n",
            "2019-01-22T07:03:02.015636: step 5718, loss 4.10856e-06, acc 1\n",
            "2019-01-22T07:03:02.689663: step 5719, loss 4.67519e-07, acc 1\n",
            "2019-01-22T07:03:03.364221: step 5720, loss 0.000253831, acc 1\n",
            "2019-01-22T07:03:04.034840: step 5721, loss 2.34692e-07, acc 1\n",
            "2019-01-22T07:03:04.716093: step 5722, loss 0.000150164, acc 1\n",
            "2019-01-22T07:03:05.389726: step 5723, loss 0.000118504, acc 1\n",
            "2019-01-22T07:03:06.069810: step 5724, loss 5.41237e-06, acc 1\n",
            "2019-01-22T07:03:06.753113: step 5725, loss 0.000156754, acc 1\n",
            "2019-01-22T07:03:07.431264: step 5726, loss 8.08645e-05, acc 1\n",
            "2019-01-22T07:03:08.103719: step 5727, loss 0.000591821, acc 1\n",
            "2019-01-22T07:03:08.796479: step 5728, loss 3.8929e-07, acc 1\n",
            "2019-01-22T07:03:09.471744: step 5729, loss 0.000638016, acc 1\n",
            "2019-01-22T07:03:10.144943: step 5730, loss 1.07471e-06, acc 1\n",
            "2019-01-22T07:03:10.823026: step 5731, loss 0.00469828, acc 1\n",
            "2019-01-22T07:03:11.498070: step 5732, loss 6.66823e-07, acc 1\n",
            "2019-01-22T07:03:12.173972: step 5733, loss 6.40742e-07, acc 1\n",
            "2019-01-22T07:03:12.845831: step 5734, loss 0.000632127, acc 1\n",
            "2019-01-22T07:03:13.520628: step 5735, loss 1.99688e-05, acc 1\n",
            "2019-01-22T07:03:14.198981: step 5736, loss 7.56456e-06, acc 1\n",
            "2019-01-22T07:03:14.877443: step 5737, loss 0.00357072, acc 1\n",
            "2019-01-22T07:03:15.550356: step 5738, loss 8.82036e-05, acc 1\n",
            "2019-01-22T07:03:16.224142: step 5739, loss 7.46911e-07, acc 1\n",
            "2019-01-22T07:03:16.900463: step 5740, loss 2.37108e-06, acc 1\n",
            "2019-01-22T07:03:17.575151: step 5741, loss 6.53781e-07, acc 1\n",
            "2019-01-22T07:03:17.984198: step 5742, loss 7.95581e-06, acc 1\n",
            "2019-01-22T07:03:18.659189: step 5743, loss 3.96739e-07, acc 1\n",
            "2019-01-22T07:03:19.334354: step 5744, loss 1.37145e-05, acc 1\n",
            "2019-01-22T07:03:20.009221: step 5745, loss 2.98023e-08, acc 1\n",
            "2019-01-22T07:03:20.686461: step 5746, loss 2.3283e-07, acc 1\n",
            "2019-01-22T07:03:21.363458: step 5747, loss 8.24883e-06, acc 1\n",
            "2019-01-22T07:03:22.040613: step 5748, loss 2.36355e-05, acc 1\n",
            "2019-01-22T07:03:22.719110: step 5749, loss 2.19647e-05, acc 1\n",
            "2019-01-22T07:03:23.394019: step 5750, loss 1.6205e-07, acc 1\n",
            "2019-01-22T07:03:24.069150: step 5751, loss 0.00204575, acc 1\n",
            "2019-01-22T07:03:24.749777: step 5752, loss 0.000150518, acc 1\n",
            "2019-01-22T07:03:25.421556: step 5753, loss 5.68103e-07, acc 1\n",
            "2019-01-22T07:03:26.095348: step 5754, loss 0.00721596, acc 1\n",
            "2019-01-22T07:03:26.770122: step 5755, loss 1.88308e-06, acc 1\n",
            "2019-01-22T07:03:27.446609: step 5756, loss 1.30385e-08, acc 1\n",
            "2019-01-22T07:03:28.122384: step 5757, loss 0.000836924, acc 1\n",
            "2019-01-22T07:03:28.795474: step 5758, loss 7.12635e-05, acc 1\n",
            "2019-01-22T07:03:29.474680: step 5759, loss 4.96154e-05, acc 1\n",
            "2019-01-22T07:03:30.154360: step 5760, loss 7.11937e-05, acc 1\n",
            "2019-01-22T07:03:30.829640: step 5761, loss 9.70584e-06, acc 1\n",
            "2019-01-22T07:03:31.509807: step 5762, loss 3.71017e-06, acc 1\n",
            "2019-01-22T07:03:32.183766: step 5763, loss 0.000224708, acc 1\n",
            "2019-01-22T07:03:32.857014: step 5764, loss 0.000488085, acc 1\n",
            "2019-01-22T07:03:33.537128: step 5765, loss 5.36771e-06, acc 1\n",
            "2019-01-22T07:03:34.212632: step 5766, loss 1.24793e-06, acc 1\n",
            "2019-01-22T07:03:34.899886: step 5767, loss 0.000309327, acc 1\n",
            "2019-01-22T07:03:35.583018: step 5768, loss 5.69965e-07, acc 1\n",
            "2019-01-22T07:03:36.251636: step 5769, loss 2.45869e-07, acc 1\n",
            "2019-01-22T07:03:36.928658: step 5770, loss 8.16059e-06, acc 1\n",
            "2019-01-22T07:03:37.618907: step 5771, loss 1.70621e-05, acc 1\n",
            "2019-01-22T07:03:38.294465: step 5772, loss 4.80688e-06, acc 1\n",
            "2019-01-22T07:03:38.966822: step 5773, loss 1.08517e-05, acc 1\n",
            "2019-01-22T07:03:39.652550: step 5774, loss 6.43085e-05, acc 1\n",
            "2019-01-22T07:03:40.060400: step 5775, loss 2.51012e-06, acc 1\n",
            "2019-01-22T07:03:40.735383: step 5776, loss 1.02257e-06, acc 1\n",
            "2019-01-22T07:03:41.418459: step 5777, loss 1.18833e-05, acc 1\n",
            "2019-01-22T07:03:42.090730: step 5778, loss 1.47889e-06, acc 1\n",
            "2019-01-22T07:03:42.770895: step 5779, loss 8.5867e-07, acc 1\n",
            "2019-01-22T07:03:43.446805: step 5780, loss 2.77533e-07, acc 1\n",
            "2019-01-22T07:03:44.127531: step 5781, loss 1.37284e-05, acc 1\n",
            "2019-01-22T07:03:44.809741: step 5782, loss 5.30849e-07, acc 1\n",
            "2019-01-22T07:03:45.489717: step 5783, loss 4.13505e-07, acc 1\n",
            "2019-01-22T07:03:46.163056: step 5784, loss 0.000382474, acc 1\n",
            "2019-01-22T07:03:46.856807: step 5785, loss 4.76819e-06, acc 1\n",
            "2019-01-22T07:03:47.530715: step 5786, loss 0.00942082, acc 1\n",
            "2019-01-22T07:03:48.205760: step 5787, loss 8.37504e-06, acc 1\n",
            "2019-01-22T07:03:48.878401: step 5788, loss 8.24333e-05, acc 1\n",
            "2019-01-22T07:03:49.558359: step 5789, loss 4.29705e-05, acc 1\n",
            "2019-01-22T07:03:50.235227: step 5790, loss 6.17718e-05, acc 1\n",
            "2019-01-22T07:03:50.909112: step 5791, loss 2.32829e-07, acc 1\n",
            "2019-01-22T07:03:51.580718: step 5792, loss 1.32616e-06, acc 1\n",
            "2019-01-22T07:03:52.255464: step 5793, loss 3.45693e-06, acc 1\n",
            "2019-01-22T07:03:52.931617: step 5794, loss 2.36555e-07, acc 1\n",
            "2019-01-22T07:03:53.607565: step 5795, loss 1.54324e-05, acc 1\n",
            "2019-01-22T07:03:54.281318: step 5796, loss 7.82232e-06, acc 1\n",
            "2019-01-22T07:03:54.955499: step 5797, loss 0.00101348, acc 1\n",
            "2019-01-22T07:03:55.627948: step 5798, loss 7.01889e-05, acc 1\n",
            "2019-01-22T07:03:56.305207: step 5799, loss 2.58154e-06, acc 1\n",
            "2019-01-22T07:03:56.981915: step 5800, loss 1.26659e-07, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:03:57.346964: step 5800, loss 0.00012764, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5800\n",
            "\n",
            "2019-01-22T07:03:58.192958: step 5801, loss 9.78065e-05, acc 1\n",
            "2019-01-22T07:03:58.868332: step 5802, loss 7.22089e-06, acc 1\n",
            "2019-01-22T07:03:59.541966: step 5803, loss 6.55089e-05, acc 1\n",
            "2019-01-22T07:04:00.222750: step 5804, loss 4.36021e-06, acc 1\n",
            "2019-01-22T07:04:00.902095: step 5805, loss 5.37118e-06, acc 1\n",
            "2019-01-22T07:04:01.577862: step 5806, loss 9.97307e-06, acc 1\n",
            "2019-01-22T07:04:02.254878: step 5807, loss 3.21915e-05, acc 1\n",
            "2019-01-22T07:04:02.666499: step 5808, loss 1.05398e-05, acc 1\n",
            "2019-01-22T07:04:03.347698: step 5809, loss 2.83103e-06, acc 1\n",
            "2019-01-22T07:04:04.024953: step 5810, loss 0.00059352, acc 1\n",
            "2019-01-22T07:04:04.701860: step 5811, loss 4.56345e-07, acc 1\n",
            "2019-01-22T07:04:05.380655: step 5812, loss 6.21403e-05, acc 1\n",
            "2019-01-22T07:04:06.059683: step 5813, loss 2.61433e-05, acc 1\n",
            "2019-01-22T07:04:06.732440: step 5814, loss 0.00141762, acc 1\n",
            "2019-01-22T07:04:07.413648: step 5815, loss 4.14675e-05, acc 1\n",
            "2019-01-22T07:04:08.085596: step 5816, loss 1.2349e-06, acc 1\n",
            "2019-01-22T07:04:08.757887: step 5817, loss 1.86817e-06, acc 1\n",
            "2019-01-22T07:04:09.450523: step 5818, loss 0.00395623, acc 1\n",
            "2019-01-22T07:04:10.135463: step 5819, loss 0.00802685, acc 1\n",
            "2019-01-22T07:04:10.811985: step 5820, loss 2.28353e-06, acc 1\n",
            "2019-01-22T07:04:11.487325: step 5821, loss 8.60217e-05, acc 1\n",
            "2019-01-22T07:04:12.161448: step 5822, loss 3.27823e-07, acc 1\n",
            "2019-01-22T07:04:12.840385: step 5823, loss 9.89337e-05, acc 1\n",
            "2019-01-22T07:04:13.517315: step 5824, loss 0.000540004, acc 1\n",
            "2019-01-22T07:04:14.191599: step 5825, loss 0.000645472, acc 1\n",
            "2019-01-22T07:04:14.866541: step 5826, loss 2.56847e-05, acc 1\n",
            "2019-01-22T07:04:15.541437: step 5827, loss 2.55072e-05, acc 1\n",
            "2019-01-22T07:04:16.216929: step 5828, loss 0.000343725, acc 1\n",
            "2019-01-22T07:04:16.893329: step 5829, loss 0.000322774, acc 1\n",
            "2019-01-22T07:04:17.574895: step 5830, loss 1.90279e-05, acc 1\n",
            "2019-01-22T07:04:18.247350: step 5831, loss 8.64495e-05, acc 1\n",
            "2019-01-22T07:04:18.921937: step 5832, loss 3.20343e-06, acc 1\n",
            "2019-01-22T07:04:19.598246: step 5833, loss 0.000287967, acc 1\n",
            "2019-01-22T07:04:20.274070: step 5834, loss 0.00164458, acc 1\n",
            "2019-01-22T07:04:20.953812: step 5835, loss 3.12622e-05, acc 1\n",
            "2019-01-22T07:04:21.628991: step 5836, loss 0.00247292, acc 1\n",
            "2019-01-22T07:04:22.306064: step 5837, loss 1.14735e-06, acc 1\n",
            "2019-01-22T07:04:22.982449: step 5838, loss 0.00328819, acc 1\n",
            "2019-01-22T07:04:23.657615: step 5839, loss 0.000216737, acc 1\n",
            "2019-01-22T07:04:24.332907: step 5840, loss 0.000258867, acc 1\n",
            "2019-01-22T07:04:24.741862: step 5841, loss 1.30787e-06, acc 1\n",
            "2019-01-22T07:04:25.416611: step 5842, loss 6.70775e-05, acc 1\n",
            "2019-01-22T07:04:26.097287: step 5843, loss 1.41091e-05, acc 1\n",
            "2019-01-22T07:04:26.770583: step 5844, loss 1.73591e-06, acc 1\n",
            "2019-01-22T07:04:27.446993: step 5845, loss 2.66901e-06, acc 1\n",
            "2019-01-22T07:04:28.120358: step 5846, loss 2.70323e-05, acc 1\n",
            "2019-01-22T07:04:28.794938: step 5847, loss 4.8987e-07, acc 1\n",
            "2019-01-22T07:04:29.473831: step 5848, loss 8.49146e-06, acc 1\n",
            "2019-01-22T07:04:30.147993: step 5849, loss 0.00491474, acc 1\n",
            "2019-01-22T07:04:30.829293: step 5850, loss 9.19879e-05, acc 1\n",
            "2019-01-22T07:04:31.506031: step 5851, loss 2.04692e-06, acc 1\n",
            "2019-01-22T07:04:32.189380: step 5852, loss 7.63696e-06, acc 1\n",
            "2019-01-22T07:04:32.861722: step 5853, loss 4.65887e-05, acc 1\n",
            "2019-01-22T07:04:33.534225: step 5854, loss 2.55181e-07, acc 1\n",
            "2019-01-22T07:04:34.218407: step 5855, loss 3.33158e-05, acc 1\n",
            "2019-01-22T07:04:34.896074: step 5856, loss 0.0012106, acc 1\n",
            "2019-01-22T07:04:35.567409: step 5857, loss 7.56229e-07, acc 1\n",
            "2019-01-22T07:04:36.256043: step 5858, loss 2.2195e-05, acc 1\n",
            "2019-01-22T07:04:36.932765: step 5859, loss 0.000325037, acc 1\n",
            "2019-01-22T07:04:37.612488: step 5860, loss 3.17379e-06, acc 1\n",
            "2019-01-22T07:04:38.289555: step 5861, loss 8.30724e-07, acc 1\n",
            "2019-01-22T07:04:38.970681: step 5862, loss 0.000323473, acc 1\n",
            "2019-01-22T07:04:39.648485: step 5863, loss 2.51645e-05, acc 1\n",
            "2019-01-22T07:04:40.325362: step 5864, loss 0.00172125, acc 1\n",
            "2019-01-22T07:04:40.997614: step 5865, loss 2.96158e-07, acc 1\n",
            "2019-01-22T07:04:41.678008: step 5866, loss 1.0803e-06, acc 1\n",
            "2019-01-22T07:04:42.352280: step 5867, loss 0.0025123, acc 1\n",
            "2019-01-22T07:04:43.026683: step 5868, loss 8.43306e-06, acc 1\n",
            "2019-01-22T07:04:43.703300: step 5869, loss 2.54446e-05, acc 1\n",
            "2019-01-22T07:04:44.383321: step 5870, loss 7.46908e-07, acc 1\n",
            "2019-01-22T07:04:45.058960: step 5871, loss 3.53902e-08, acc 1\n",
            "2019-01-22T07:04:45.737537: step 5872, loss 3.38123e-05, acc 1\n",
            "2019-01-22T07:04:46.415797: step 5873, loss 2.70756e-05, acc 1\n",
            "2019-01-22T07:04:46.825345: step 5874, loss 2.72477e-07, acc 1\n",
            "2019-01-22T07:04:47.499566: step 5875, loss 2.52874e-05, acc 1\n",
            "2019-01-22T07:04:48.176450: step 5876, loss 1.02258e-06, acc 1\n",
            "2019-01-22T07:04:48.851930: step 5877, loss 0.000163939, acc 1\n",
            "2019-01-22T07:04:49.526950: step 5878, loss 8.22836e-05, acc 1\n",
            "2019-01-22T07:04:50.207383: step 5879, loss 1.01843e-05, acc 1\n",
            "2019-01-22T07:04:50.881323: step 5880, loss 2.22943e-06, acc 1\n",
            "2019-01-22T07:04:51.553038: step 5881, loss 1.29825e-06, acc 1\n",
            "2019-01-22T07:04:52.227021: step 5882, loss 1.23119e-06, acc 1\n",
            "2019-01-22T07:04:52.904737: step 5883, loss 0.00136465, acc 1\n",
            "2019-01-22T07:04:53.580007: step 5884, loss 1.45281e-06, acc 1\n",
            "2019-01-22T07:04:54.252717: step 5885, loss 0.000172402, acc 1\n",
            "2019-01-22T07:04:54.933244: step 5886, loss 4.26515e-06, acc 1\n",
            "2019-01-22T07:04:55.607244: step 5887, loss 3.52038e-07, acc 1\n",
            "2019-01-22T07:04:56.283645: step 5888, loss 0.0132319, acc 0.984375\n",
            "2019-01-22T07:04:56.960879: step 5889, loss 1.19691e-05, acc 1\n",
            "2019-01-22T07:04:57.641515: step 5890, loss 1.09896e-07, acc 1\n",
            "2019-01-22T07:04:58.316877: step 5891, loss 1.12234e-05, acc 1\n",
            "2019-01-22T07:04:58.992990: step 5892, loss 0.00024989, acc 1\n",
            "2019-01-22T07:04:59.672075: step 5893, loss 4.69887e-06, acc 1\n",
            "2019-01-22T07:05:00.347537: step 5894, loss 4.45032e-05, acc 1\n",
            "2019-01-22T07:05:01.023268: step 5895, loss 0.000247914, acc 1\n",
            "2019-01-22T07:05:01.696833: step 5896, loss 2.15833e-05, acc 1\n",
            "2019-01-22T07:05:02.370822: step 5897, loss 5.77419e-08, acc 1\n",
            "2019-01-22T07:05:03.052676: step 5898, loss 7.69319e-05, acc 1\n",
            "2019-01-22T07:05:03.726879: step 5899, loss 1.27031e-06, acc 1\n",
            "2019-01-22T07:05:04.401661: step 5900, loss 2.774e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:05:04.760041: step 5900, loss 0.00022155, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-5900\n",
            "\n",
            "2019-01-22T07:05:05.618402: step 5901, loss 1.80112e-06, acc 1\n",
            "2019-01-22T07:05:06.293142: step 5902, loss 1.19299e-05, acc 1\n",
            "2019-01-22T07:05:06.970856: step 5903, loss 0.000247513, acc 1\n",
            "2019-01-22T07:05:07.646657: step 5904, loss 3.05451e-05, acc 1\n",
            "2019-01-22T07:05:08.322108: step 5905, loss 4.84284e-07, acc 1\n",
            "2019-01-22T07:05:08.994493: step 5906, loss 2.23517e-08, acc 1\n",
            "2019-01-22T07:05:09.407231: step 5907, loss 2.41478e-06, acc 1\n",
            "2019-01-22T07:05:10.083274: step 5908, loss 1.2591e-06, acc 1\n",
            "2019-01-22T07:05:10.758879: step 5909, loss 3.72529e-09, acc 1\n",
            "2019-01-22T07:05:11.435390: step 5910, loss 0.000232083, acc 1\n",
            "2019-01-22T07:05:12.116354: step 5911, loss 0.0210824, acc 0.984375\n",
            "2019-01-22T07:05:12.797639: step 5912, loss 0.000761596, acc 1\n",
            "2019-01-22T07:05:13.469318: step 5913, loss 0.00612486, acc 1\n",
            "2019-01-22T07:05:14.140654: step 5914, loss 1.26844e-06, acc 1\n",
            "2019-01-22T07:05:14.820942: step 5915, loss 0.00222614, acc 1\n",
            "2019-01-22T07:05:15.494500: step 5916, loss 4.47034e-08, acc 1\n",
            "2019-01-22T07:05:16.167259: step 5917, loss 1.41064e-05, acc 1\n",
            "2019-01-22T07:05:16.842726: step 5918, loss 1.43234e-06, acc 1\n",
            "2019-01-22T07:05:17.530571: step 5919, loss 1.41558e-06, acc 1\n",
            "2019-01-22T07:05:18.209912: step 5920, loss 0.000962366, acc 1\n",
            "2019-01-22T07:05:18.886030: step 5921, loss 3.49034e-05, acc 1\n",
            "2019-01-22T07:05:19.575390: step 5922, loss 1.28717e-05, acc 1\n",
            "2019-01-22T07:05:20.253920: step 5923, loss 0.00015569, acc 1\n",
            "2019-01-22T07:05:20.936938: step 5924, loss 0.000254094, acc 1\n",
            "2019-01-22T07:05:21.622232: step 5925, loss 2.38566e-05, acc 1\n",
            "2019-01-22T07:05:22.297961: step 5926, loss 2.50144e-06, acc 1\n",
            "2019-01-22T07:05:22.978070: step 5927, loss 1.56948e-05, acc 1\n",
            "2019-01-22T07:05:23.663920: step 5928, loss 2.8716e-05, acc 1\n",
            "2019-01-22T07:05:24.339442: step 5929, loss 0.000136648, acc 1\n",
            "2019-01-22T07:05:25.013727: step 5930, loss 0.000125872, acc 1\n",
            "2019-01-22T07:05:25.692706: step 5931, loss 6.59434e-05, acc 1\n",
            "2019-01-22T07:05:26.369289: step 5932, loss 4.91519e-05, acc 1\n",
            "2019-01-22T07:05:27.048418: step 5933, loss 0.0147607, acc 0.984375\n",
            "2019-01-22T07:05:27.725902: step 5934, loss 1.35273e-05, acc 1\n",
            "2019-01-22T07:05:28.401811: step 5935, loss 1.25734e-05, acc 1\n",
            "2019-01-22T07:05:29.080208: step 5936, loss 2.04884e-06, acc 1\n",
            "2019-01-22T07:05:29.754085: step 5937, loss 2.67752e-05, acc 1\n",
            "2019-01-22T07:05:30.430534: step 5938, loss 0.00807467, acc 1\n",
            "2019-01-22T07:05:31.104814: step 5939, loss 6.89665e-05, acc 1\n",
            "2019-01-22T07:05:31.514502: step 5940, loss 8.24816e-06, acc 1\n",
            "2019-01-22T07:05:32.190688: step 5941, loss 0.03377, acc 0.984375\n",
            "2019-01-22T07:05:32.866600: step 5942, loss 0.00803799, acc 1\n",
            "2019-01-22T07:05:33.540535: step 5943, loss 1.7055e-05, acc 1\n",
            "2019-01-22T07:05:34.215342: step 5944, loss 0.00111787, acc 1\n",
            "2019-01-22T07:05:34.893513: step 5945, loss 3.25373e-06, acc 1\n",
            "2019-01-22T07:05:35.567724: step 5946, loss 0.000746413, acc 1\n",
            "2019-01-22T07:05:36.242598: step 5947, loss 0.00011266, acc 1\n",
            "2019-01-22T07:05:36.913798: step 5948, loss 9.79736e-07, acc 1\n",
            "2019-01-22T07:05:37.590836: step 5949, loss 7.46133e-05, acc 1\n",
            "2019-01-22T07:05:38.265497: step 5950, loss 0.0261167, acc 0.96875\n",
            "2019-01-22T07:05:38.940092: step 5951, loss 0.000166945, acc 1\n",
            "2019-01-22T07:05:39.620857: step 5952, loss 0.0501076, acc 0.984375\n",
            "2019-01-22T07:05:40.295710: step 5953, loss 2.0637e-06, acc 1\n",
            "2019-01-22T07:05:40.970340: step 5954, loss 5.47103e-05, acc 1\n",
            "2019-01-22T07:05:41.645236: step 5955, loss 0.000148219, acc 1\n",
            "2019-01-22T07:05:42.329660: step 5956, loss 3.29476e-06, acc 1\n",
            "2019-01-22T07:05:43.002969: step 5957, loss 1.5175e-05, acc 1\n",
            "2019-01-22T07:05:43.679034: step 5958, loss 1.695e-07, acc 1\n",
            "2019-01-22T07:05:44.351680: step 5959, loss 1.02975e-05, acc 1\n",
            "2019-01-22T07:05:45.028335: step 5960, loss 2.34693e-07, acc 1\n",
            "2019-01-22T07:05:45.702236: step 5961, loss 0.00104022, acc 1\n",
            "2019-01-22T07:05:46.378511: step 5962, loss 1.19768e-05, acc 1\n",
            "2019-01-22T07:05:47.057214: step 5963, loss 0.00442022, acc 1\n",
            "2019-01-22T07:05:47.733139: step 5964, loss 8.50697e-05, acc 1\n",
            "2019-01-22T07:05:48.419922: step 5965, loss 7.02517e-06, acc 1\n",
            "2019-01-22T07:05:49.096578: step 5966, loss 6.29565e-07, acc 1\n",
            "2019-01-22T07:05:49.774682: step 5967, loss 4.7521e-05, acc 1\n",
            "2019-01-22T07:05:50.465719: step 5968, loss 6.72821e-05, acc 1\n",
            "2019-01-22T07:05:51.143912: step 5969, loss 0.0207662, acc 0.984375\n",
            "2019-01-22T07:05:51.821760: step 5970, loss 0.000199395, acc 1\n",
            "2019-01-22T07:05:52.508210: step 5971, loss 1.86228e-05, acc 1\n",
            "2019-01-22T07:05:53.181947: step 5972, loss 0.0844673, acc 0.984375\n",
            "2019-01-22T07:05:53.591746: step 5973, loss 6.54555e-06, acc 1\n",
            "2019-01-22T07:05:54.268456: step 5974, loss 1.86377e-05, acc 1\n",
            "2019-01-22T07:05:54.943726: step 5975, loss 2.91678e-06, acc 1\n",
            "2019-01-22T07:05:55.626374: step 5976, loss 1.32304e-05, acc 1\n",
            "2019-01-22T07:05:56.307682: step 5977, loss 0.000187924, acc 1\n",
            "2019-01-22T07:05:56.983035: step 5978, loss 1.33966e-05, acc 1\n",
            "2019-01-22T07:05:57.669674: step 5979, loss 8.19548e-07, acc 1\n",
            "2019-01-22T07:05:58.342926: step 5980, loss 7.8416e-07, acc 1\n",
            "2019-01-22T07:05:59.016406: step 5981, loss 3.83702e-07, acc 1\n",
            "2019-01-22T07:05:59.704300: step 5982, loss 2.30966e-07, acc 1\n",
            "2019-01-22T07:06:00.376493: step 5983, loss 3.63547e-06, acc 1\n",
            "2019-01-22T07:06:01.048327: step 5984, loss 0.000139098, acc 1\n",
            "2019-01-22T07:06:01.727521: step 5985, loss 0.000360863, acc 1\n",
            "2019-01-22T07:06:02.406909: step 5986, loss 0.000772755, acc 1\n",
            "2019-01-22T07:06:03.084461: step 5987, loss 0.0391239, acc 0.984375\n",
            "2019-01-22T07:06:03.759565: step 5988, loss 2.63742e-06, acc 1\n",
            "2019-01-22T07:06:04.433705: step 5989, loss 1.45286e-07, acc 1\n",
            "2019-01-22T07:06:05.108723: step 5990, loss 0.00119971, acc 1\n",
            "2019-01-22T07:06:05.781986: step 5991, loss 2.20119e-05, acc 1\n",
            "2019-01-22T07:06:06.454455: step 5992, loss 1.60187e-07, acc 1\n",
            "2019-01-22T07:06:07.130543: step 5993, loss 5.62516e-07, acc 1\n",
            "2019-01-22T07:06:07.805923: step 5994, loss 0.0609837, acc 0.984375\n",
            "2019-01-22T07:06:08.479719: step 5995, loss 1.73225e-07, acc 1\n",
            "2019-01-22T07:06:09.149532: step 5996, loss 1.04308e-07, acc 1\n",
            "2019-01-22T07:06:09.827044: step 5997, loss 3.84801e-06, acc 1\n",
            "2019-01-22T07:06:10.500719: step 5998, loss 1.77321e-06, acc 1\n",
            "2019-01-22T07:06:11.176619: step 5999, loss 5.25028e-06, acc 1\n",
            "2019-01-22T07:06:11.857130: step 6000, loss 2.44736e-06, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:06:12.218279: step 6000, loss 0.00074994, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-6000\n",
            "\n",
            "2019-01-22T07:06:13.041633: step 6001, loss 0, acc 1\n",
            "2019-01-22T07:06:13.719648: step 6002, loss 0.000159016, acc 1\n",
            "2019-01-22T07:06:14.395574: step 6003, loss 4.36352e-05, acc 1\n",
            "2019-01-22T07:06:15.073984: step 6004, loss 0.000363993, acc 1\n",
            "2019-01-22T07:06:15.756921: step 6005, loss 0.000167109, acc 1\n",
            "2019-01-22T07:06:16.173268: step 6006, loss 0.00042619, acc 1\n",
            "2019-01-22T07:06:16.848069: step 6007, loss 8.97754e-06, acc 1\n",
            "2019-01-22T07:06:17.523463: step 6008, loss 0.0827154, acc 0.984375\n",
            "2019-01-22T07:06:18.208056: step 6009, loss 0.031777, acc 0.984375\n",
            "2019-01-22T07:06:18.887377: step 6010, loss 5.11581e-05, acc 1\n",
            "2019-01-22T07:06:19.558505: step 6011, loss 2.90571e-07, acc 1\n",
            "2019-01-22T07:06:20.234542: step 6012, loss 1.63912e-07, acc 1\n",
            "2019-01-22T07:06:20.908625: step 6013, loss 7.45058e-09, acc 1\n",
            "2019-01-22T07:06:21.591057: step 6014, loss 8.51048e-05, acc 1\n",
            "2019-01-22T07:06:22.267755: step 6015, loss 1.52361e-06, acc 1\n",
            "2019-01-22T07:06:22.941493: step 6016, loss 0.000386226, acc 1\n",
            "2019-01-22T07:06:23.616375: step 6017, loss 1.76951e-07, acc 1\n",
            "2019-01-22T07:06:24.293856: step 6018, loss 7.33398e-05, acc 1\n",
            "2019-01-22T07:06:24.969846: step 6019, loss 8.06509e-07, acc 1\n",
            "2019-01-22T07:06:25.643591: step 6020, loss 4.30219e-06, acc 1\n",
            "2019-01-22T07:06:26.320821: step 6021, loss 2.9521e-06, acc 1\n",
            "2019-01-22T07:06:26.999688: step 6022, loss 1.17346e-07, acc 1\n",
            "2019-01-22T07:06:27.676072: step 6023, loss 3.20373e-07, acc 1\n",
            "2019-01-22T07:06:28.354227: step 6024, loss 1.57205e-06, acc 1\n",
            "2019-01-22T07:06:29.030234: step 6025, loss 0.00018091, acc 1\n",
            "2019-01-22T07:06:29.703705: step 6026, loss 1.20748e-05, acc 1\n",
            "2019-01-22T07:06:30.378889: step 6027, loss 0.00402591, acc 1\n",
            "2019-01-22T07:06:31.054269: step 6028, loss 0.000100558, acc 1\n",
            "2019-01-22T07:06:31.729840: step 6029, loss 0.000627204, acc 1\n",
            "2019-01-22T07:06:32.406403: step 6030, loss 1.42717e-05, acc 1\n",
            "2019-01-22T07:06:33.078648: step 6031, loss 0.000387668, acc 1\n",
            "2019-01-22T07:06:33.752220: step 6032, loss 0.0077926, acc 1\n",
            "2019-01-22T07:06:34.426288: step 6033, loss 0.000273006, acc 1\n",
            "2019-01-22T07:06:35.099942: step 6034, loss 0.0271925, acc 0.984375\n",
            "2019-01-22T07:06:35.779113: step 6035, loss 7.63672e-07, acc 1\n",
            "2019-01-22T07:06:36.457893: step 6036, loss 9.31321e-08, acc 1\n",
            "2019-01-22T07:06:37.132267: step 6037, loss 0.0169713, acc 0.984375\n",
            "2019-01-22T07:06:37.815024: step 6038, loss 9.03315e-05, acc 1\n",
            "2019-01-22T07:06:38.223955: step 6039, loss 2.65067e-05, acc 1\n",
            "2019-01-22T07:06:38.898107: step 6040, loss 6.48191e-07, acc 1\n",
            "2019-01-22T07:06:39.573563: step 6041, loss 2.61251e-05, acc 1\n",
            "2019-01-22T07:06:40.248970: step 6042, loss 1.10451e-06, acc 1\n",
            "2019-01-22T07:06:40.928202: step 6043, loss 0.000179373, acc 1\n",
            "2019-01-22T07:06:41.601380: step 6044, loss 9.31969e-05, acc 1\n",
            "2019-01-22T07:06:42.286703: step 6045, loss 3.65713e-05, acc 1\n",
            "2019-01-22T07:06:42.971226: step 6046, loss 0.0397637, acc 0.984375\n",
            "2019-01-22T07:06:43.645263: step 6047, loss 0.00619128, acc 1\n",
            "2019-01-22T07:06:44.323639: step 6048, loss 3.96548e-05, acc 1\n",
            "2019-01-22T07:06:45.008058: step 6049, loss 0.000418015, acc 1\n",
            "2019-01-22T07:06:45.683786: step 6050, loss 3.12923e-07, acc 1\n",
            "2019-01-22T07:06:46.356383: step 6051, loss 2.3943e-05, acc 1\n",
            "2019-01-22T07:06:47.035720: step 6052, loss 0.000113258, acc 1\n",
            "2019-01-22T07:06:47.712547: step 6053, loss 6.70552e-08, acc 1\n",
            "2019-01-22T07:06:48.390184: step 6054, loss 6.94547e-05, acc 1\n",
            "2019-01-22T07:06:49.062398: step 6055, loss 3.72527e-07, acc 1\n",
            "2019-01-22T07:06:49.736651: step 6056, loss 1.41561e-07, acc 1\n",
            "2019-01-22T07:06:50.419855: step 6057, loss 0.000249869, acc 1\n",
            "2019-01-22T07:06:51.095078: step 6058, loss 5.77419e-08, acc 1\n",
            "2019-01-22T07:06:51.769566: step 6059, loss 1.5832e-06, acc 1\n",
            "2019-01-22T07:06:52.443748: step 6060, loss 0.000406748, acc 1\n",
            "2019-01-22T07:06:53.122597: step 6061, loss 2.53177e-05, acc 1\n",
            "2019-01-22T07:06:53.797765: step 6062, loss 0.0135153, acc 0.984375\n",
            "2019-01-22T07:06:54.470438: step 6063, loss 1.0617e-07, acc 1\n",
            "2019-01-22T07:06:55.147400: step 6064, loss 5.25262e-07, acc 1\n",
            "2019-01-22T07:06:55.820725: step 6065, loss 1.71913e-06, acc 1\n",
            "2019-01-22T07:06:56.492856: step 6066, loss 3.20168e-06, acc 1\n",
            "2019-01-22T07:06:57.165750: step 6067, loss 4.2184e-06, acc 1\n",
            "2019-01-22T07:06:57.844679: step 6068, loss 0.0143568, acc 0.984375\n",
            "2019-01-22T07:06:58.514749: step 6069, loss 3.16649e-08, acc 1\n",
            "2019-01-22T07:06:59.188980: step 6070, loss 0.000105187, acc 1\n",
            "2019-01-22T07:06:59.862452: step 6071, loss 1.04864e-06, acc 1\n",
            "2019-01-22T07:07:00.275777: step 6072, loss 0.00061883, acc 1\n",
            "2019-01-22T07:07:00.951455: step 6073, loss 4.23139e-06, acc 1\n",
            "2019-01-22T07:07:01.625920: step 6074, loss 6.80012e-05, acc 1\n",
            "2019-01-22T07:07:02.301312: step 6075, loss 0.000381624, acc 1\n",
            "2019-01-22T07:07:02.981040: step 6076, loss 0.0583953, acc 0.984375\n",
            "2019-01-22T07:07:03.653238: step 6077, loss 0.000174129, acc 1\n",
            "2019-01-22T07:07:04.328596: step 6078, loss 7.15728e-06, acc 1\n",
            "2019-01-22T07:07:05.001683: step 6079, loss 1.33919e-06, acc 1\n",
            "2019-01-22T07:07:05.683070: step 6080, loss 2.51456e-07, acc 1\n",
            "2019-01-22T07:07:06.357012: step 6081, loss 7.15699e-06, acc 1\n",
            "2019-01-22T07:07:07.031441: step 6082, loss 1.55569e-05, acc 1\n",
            "2019-01-22T07:07:07.705636: step 6083, loss 1.15855e-06, acc 1\n",
            "2019-01-22T07:07:08.381973: step 6084, loss 4.40516e-05, acc 1\n",
            "2019-01-22T07:07:09.054171: step 6085, loss 0.000413692, acc 1\n",
            "2019-01-22T07:07:09.739648: step 6086, loss 0.00100833, acc 1\n",
            "2019-01-22T07:07:10.416339: step 6087, loss 3.72442e-05, acc 1\n",
            "2019-01-22T07:07:11.091428: step 6088, loss 0.000251109, acc 1\n",
            "2019-01-22T07:07:11.773550: step 6089, loss 4.20248e-05, acc 1\n",
            "2019-01-22T07:07:12.452439: step 6090, loss 0.000260296, acc 1\n",
            "2019-01-22T07:07:13.126700: step 6091, loss 0.00141099, acc 1\n",
            "2019-01-22T07:07:13.805773: step 6092, loss 1.64436e-05, acc 1\n",
            "2019-01-22T07:07:14.482504: step 6093, loss 5.11897e-05, acc 1\n",
            "2019-01-22T07:07:15.155219: step 6094, loss 1.41359e-05, acc 1\n",
            "2019-01-22T07:07:15.835576: step 6095, loss 0.0727661, acc 0.984375\n",
            "2019-01-22T07:07:16.511080: step 6096, loss 5.01746e-06, acc 1\n",
            "2019-01-22T07:07:17.188357: step 6097, loss 1.31207e-05, acc 1\n",
            "2019-01-22T07:07:17.857399: step 6098, loss 3.94668e-05, acc 1\n",
            "2019-01-22T07:07:18.539582: step 6099, loss 8.92949e-05, acc 1\n",
            "2019-01-22T07:07:19.211784: step 6100, loss 1.87103e-05, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:07:19.574778: step 6100, loss 0.000569043, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-6100\n",
            "\n",
            "2019-01-22T07:07:20.397761: step 6101, loss 4.07769e-05, acc 1\n",
            "2019-01-22T07:07:21.080790: step 6102, loss 1.11545e-05, acc 1\n",
            "2019-01-22T07:07:21.757974: step 6103, loss 3.39912e-06, acc 1\n",
            "2019-01-22T07:07:22.429495: step 6104, loss 5.24329e-05, acc 1\n",
            "2019-01-22T07:07:22.841911: step 6105, loss 2.14576e-07, acc 1\n",
            "2019-01-22T07:07:23.515250: step 6106, loss 1.85821e-05, acc 1\n",
            "2019-01-22T07:07:24.194066: step 6107, loss 0.0005653, acc 1\n",
            "2019-01-22T07:07:24.873713: step 6108, loss 3.04612e-05, acc 1\n",
            "2019-01-22T07:07:25.545439: step 6109, loss 1.53479e-06, acc 1\n",
            "2019-01-22T07:07:26.229525: step 6110, loss 0.000238287, acc 1\n",
            "2019-01-22T07:07:26.903553: step 6111, loss 8.71697e-07, acc 1\n",
            "2019-01-22T07:07:27.579937: step 6112, loss 7.80432e-07, acc 1\n",
            "2019-01-22T07:07:28.264016: step 6113, loss 2.77533e-07, acc 1\n",
            "2019-01-22T07:07:28.940457: step 6114, loss 2.04891e-08, acc 1\n",
            "2019-01-22T07:07:29.614911: step 6115, loss 1.24062e-05, acc 1\n",
            "2019-01-22T07:07:30.300341: step 6116, loss 3.94032e-05, acc 1\n",
            "2019-01-22T07:07:30.976255: step 6117, loss 1.95196e-05, acc 1\n",
            "2019-01-22T07:07:31.651458: step 6118, loss 3.52933e-06, acc 1\n",
            "2019-01-22T07:07:32.330603: step 6119, loss 0.000133972, acc 1\n",
            "2019-01-22T07:07:33.006389: step 6120, loss 0.00010608, acc 1\n",
            "2019-01-22T07:07:33.679481: step 6121, loss 0.000228541, acc 1\n",
            "2019-01-22T07:07:34.355596: step 6122, loss 1.26656e-06, acc 1\n",
            "2019-01-22T07:07:35.032919: step 6123, loss 5.2918e-05, acc 1\n",
            "2019-01-22T07:07:35.714741: step 6124, loss 7.97964e-06, acc 1\n",
            "2019-01-22T07:07:36.390491: step 6125, loss 0.000860222, acc 1\n",
            "2019-01-22T07:07:37.062254: step 6126, loss 2.59829e-06, acc 1\n",
            "2019-01-22T07:07:37.736432: step 6127, loss 5.73648e-06, acc 1\n",
            "2019-01-22T07:07:38.415486: step 6128, loss 4.54482e-07, acc 1\n",
            "2019-01-22T07:07:39.090344: step 6129, loss 2.59954e-05, acc 1\n",
            "2019-01-22T07:07:39.765699: step 6130, loss 9.18766e-06, acc 1\n",
            "2019-01-22T07:07:40.444134: step 6131, loss 1.37333e-05, acc 1\n",
            "2019-01-22T07:07:41.120721: step 6132, loss 0.000131232, acc 1\n",
            "2019-01-22T07:07:41.796312: step 6133, loss 6.3702e-07, acc 1\n",
            "2019-01-22T07:07:42.471342: step 6134, loss 5.46173e-05, acc 1\n",
            "2019-01-22T07:07:43.151991: step 6135, loss 1.56924e-05, acc 1\n",
            "2019-01-22T07:07:43.826960: step 6136, loss 0.00338489, acc 1\n",
            "2019-01-22T07:07:44.499758: step 6137, loss 2.89253e-06, acc 1\n",
            "2019-01-22T07:07:44.911225: step 6138, loss 4.62334e-05, acc 1\n",
            "2019-01-22T07:07:45.586500: step 6139, loss 1.50003e-05, acc 1\n",
            "2019-01-22T07:07:46.265586: step 6140, loss 0.0769828, acc 0.984375\n",
            "2019-01-22T07:07:46.942337: step 6141, loss 2.6077e-08, acc 1\n",
            "2019-01-22T07:07:47.615627: step 6142, loss 6.03492e-07, acc 1\n",
            "2019-01-22T07:07:48.291075: step 6143, loss 4.90374e-06, acc 1\n",
            "2019-01-22T07:07:48.971524: step 6144, loss 4.62244e-06, acc 1\n",
            "2019-01-22T07:07:49.646437: step 6145, loss 8.09344e-05, acc 1\n",
            "2019-01-22T07:07:50.321076: step 6146, loss 0.0239929, acc 0.984375\n",
            "2019-01-22T07:07:50.993509: step 6147, loss 6.10871e-05, acc 1\n",
            "2019-01-22T07:07:51.672235: step 6148, loss 5.02907e-07, acc 1\n",
            "2019-01-22T07:07:52.344534: step 6149, loss 2.76963e-06, acc 1\n",
            "2019-01-22T07:07:53.018789: step 6150, loss 6.45658e-06, acc 1\n",
            "2019-01-22T07:07:53.697443: step 6151, loss 5.2154e-08, acc 1\n",
            "2019-01-22T07:07:54.373479: step 6152, loss 0.0699696, acc 0.984375\n",
            "2019-01-22T07:07:55.054850: step 6153, loss 0.000299898, acc 1\n",
            "2019-01-22T07:07:55.732601: step 6154, loss 0.0281206, acc 0.984375\n",
            "2019-01-22T07:07:56.410647: step 6155, loss 2.88393e-05, acc 1\n",
            "2019-01-22T07:07:57.095864: step 6156, loss 0.000259138, acc 1\n",
            "2019-01-22T07:07:57.771268: step 6157, loss 6.76272e-06, acc 1\n",
            "2019-01-22T07:07:58.443997: step 6158, loss 9.23252e-05, acc 1\n",
            "2019-01-22T07:07:59.128076: step 6159, loss 0.043891, acc 0.984375\n",
            "2019-01-22T07:07:59.801021: step 6160, loss 2.79397e-08, acc 1\n",
            "2019-01-22T07:08:00.474944: step 6161, loss 8.27004e-07, acc 1\n",
            "2019-01-22T07:08:01.146878: step 6162, loss 1.65775e-07, acc 1\n",
            "2019-01-22T07:08:01.824977: step 6163, loss 0.000687316, acc 1\n",
            "2019-01-22T07:08:02.497003: step 6164, loss 0.00782947, acc 1\n",
            "2019-01-22T07:08:03.169769: step 6165, loss 3.15799e-05, acc 1\n",
            "2019-01-22T07:08:03.843356: step 6166, loss 4.59492e-05, acc 1\n",
            "2019-01-22T07:08:04.518360: step 6167, loss 1.8461e-05, acc 1\n",
            "2019-01-22T07:08:05.194019: step 6168, loss 0.000893325, acc 1\n",
            "2019-01-22T07:08:05.869929: step 6169, loss 0.061523, acc 0.984375\n",
            "2019-01-22T07:08:06.550927: step 6170, loss 0.085194, acc 0.984375\n",
            "2019-01-22T07:08:06.959273: step 6171, loss 9.36151e-06, acc 1\n",
            "2019-01-22T07:08:07.637280: step 6172, loss 0.0013343, acc 1\n",
            "2019-01-22T07:08:08.310486: step 6173, loss 6.84983e-05, acc 1\n",
            "2019-01-22T07:08:08.984368: step 6174, loss 0.000280775, acc 1\n",
            "2019-01-22T07:08:09.666259: step 6175, loss 3.70664e-07, acc 1\n",
            "2019-01-22T07:08:10.337951: step 6176, loss 3.1665e-08, acc 1\n",
            "2019-01-22T07:08:11.013033: step 6177, loss 0.000663923, acc 1\n",
            "2019-01-22T07:08:11.687190: step 6178, loss 1.41561e-07, acc 1\n",
            "2019-01-22T07:08:12.362274: step 6179, loss 2.31258e-05, acc 1\n",
            "2019-01-22T07:08:13.033352: step 6180, loss 1.06249e-05, acc 1\n",
            "2019-01-22T07:08:13.710101: step 6181, loss 0.000297867, acc 1\n",
            "2019-01-22T07:08:14.386152: step 6182, loss 1.12431e-05, acc 1\n",
            "2019-01-22T07:08:15.061535: step 6183, loss 1.73226e-07, acc 1\n",
            "2019-01-22T07:08:15.736982: step 6184, loss 4.02346e-05, acc 1\n",
            "2019-01-22T07:08:16.413592: step 6185, loss 2.19227e-06, acc 1\n",
            "2019-01-22T07:08:17.092278: step 6186, loss 7.00483e-05, acc 1\n",
            "2019-01-22T07:08:17.768734: step 6187, loss 0.00165931, acc 1\n",
            "2019-01-22T07:08:18.441838: step 6188, loss 0.000411327, acc 1\n",
            "2019-01-22T07:08:19.115546: step 6189, loss 5.64198e-05, acc 1\n",
            "2019-01-22T07:08:19.797075: step 6190, loss 3.90749e-06, acc 1\n",
            "2019-01-22T07:08:20.471978: step 6191, loss 9.10249e-05, acc 1\n",
            "2019-01-22T07:08:21.149615: step 6192, loss 0.0013699, acc 1\n",
            "2019-01-22T07:08:21.837976: step 6193, loss 0.00942337, acc 1\n",
            "2019-01-22T07:08:22.515400: step 6194, loss 2.41682e-05, acc 1\n",
            "2019-01-22T07:08:23.192152: step 6195, loss 4.25614e-05, acc 1\n",
            "2019-01-22T07:08:23.873364: step 6196, loss 3.8929e-07, acc 1\n",
            "2019-01-22T07:08:24.547077: step 6197, loss 6.38883e-07, acc 1\n",
            "2019-01-22T07:08:25.227946: step 6198, loss 0.00100156, acc 1\n",
            "2019-01-22T07:08:25.911561: step 6199, loss 6.50445e-05, acc 1\n",
            "2019-01-22T07:08:26.587679: step 6200, loss 0.000409598, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:08:26.952405: step 6200, loss 5.94079e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-6200\n",
            "\n",
            "2019-01-22T07:08:27.776869: step 6201, loss 0.0127493, acc 0.984375\n",
            "2019-01-22T07:08:28.448655: step 6202, loss 6.70551e-08, acc 1\n",
            "2019-01-22T07:08:29.119024: step 6203, loss 8.49357e-07, acc 1\n",
            "2019-01-22T07:08:29.526541: step 6204, loss 1.28062e-06, acc 1\n",
            "2019-01-22T07:08:30.202880: step 6205, loss 0.00118333, acc 1\n",
            "2019-01-22T07:08:30.879121: step 6206, loss 5.96046e-08, acc 1\n",
            "2019-01-22T07:08:31.556488: step 6207, loss 3.27018e-05, acc 1\n",
            "2019-01-22T07:08:32.234462: step 6208, loss 0.000146716, acc 1\n",
            "2019-01-22T07:08:32.905493: step 6209, loss 1.86264e-08, acc 1\n",
            "2019-01-22T07:08:33.579152: step 6210, loss 7.96381e-05, acc 1\n",
            "2019-01-22T07:08:34.258966: step 6211, loss 2.04891e-08, acc 1\n",
            "2019-01-22T07:08:34.936403: step 6212, loss 4.84287e-08, acc 1\n",
            "2019-01-22T07:08:35.609940: step 6213, loss 1.06171e-07, acc 1\n",
            "2019-01-22T07:08:36.286439: step 6214, loss 0.0164767, acc 0.984375\n",
            "2019-01-22T07:08:36.967814: step 6215, loss 2.54318e-05, acc 1\n",
            "2019-01-22T07:08:37.640795: step 6216, loss 1.18276e-06, acc 1\n",
            "2019-01-22T07:08:38.322845: step 6217, loss 1.24422e-06, acc 1\n",
            "2019-01-22T07:08:38.999762: step 6218, loss 2.64495e-07, acc 1\n",
            "2019-01-22T07:08:39.675970: step 6219, loss 2.32829e-07, acc 1\n",
            "2019-01-22T07:08:40.356887: step 6220, loss 2.25558e-06, acc 1\n",
            "2019-01-22T07:08:41.026856: step 6221, loss 9.31322e-09, acc 1\n",
            "2019-01-22T07:08:41.703139: step 6222, loss 5.58794e-09, acc 1\n",
            "2019-01-22T07:08:42.383042: step 6223, loss 2.31403e-05, acc 1\n",
            "2019-01-22T07:08:43.063686: step 6224, loss 6.89178e-08, acc 1\n",
            "2019-01-22T07:08:43.737967: step 6225, loss 0.00676584, acc 1\n",
            "2019-01-22T07:08:44.411363: step 6226, loss 4.55185e-06, acc 1\n",
            "2019-01-22T07:08:45.087266: step 6227, loss 7.02304e-06, acc 1\n",
            "2019-01-22T07:08:45.762739: step 6228, loss 2.63361e-06, acc 1\n",
            "2019-01-22T07:08:46.436644: step 6229, loss 8.6426e-07, acc 1\n",
            "2019-01-22T07:08:47.114271: step 6230, loss 6.90196e-05, acc 1\n",
            "2019-01-22T07:08:47.787132: step 6231, loss 5.36435e-07, acc 1\n",
            "2019-01-22T07:08:48.458336: step 6232, loss 5.03792e-06, acc 1\n",
            "2019-01-22T07:08:49.128715: step 6233, loss 5.30036e-05, acc 1\n",
            "2019-01-22T07:08:49.801049: step 6234, loss 3.14757e-06, acc 1\n",
            "2019-01-22T07:08:50.478874: step 6235, loss 2.10478e-07, acc 1\n",
            "2019-01-22T07:08:51.151221: step 6236, loss 0.000215446, acc 1\n",
            "2019-01-22T07:08:51.558350: step 6237, loss 2.282e-07, acc 1\n",
            "2019-01-22T07:08:52.233079: step 6238, loss 3.11059e-07, acc 1\n",
            "2019-01-22T07:08:52.907063: step 6239, loss 7.68917e-06, acc 1\n",
            "2019-01-22T07:08:53.578598: step 6240, loss 5.77419e-08, acc 1\n",
            "2019-01-22T07:08:54.255036: step 6241, loss 0.00268763, acc 1\n",
            "2019-01-22T07:08:54.931706: step 6242, loss 0.0689185, acc 0.984375\n",
            "2019-01-22T07:08:55.607384: step 6243, loss 0.000248853, acc 1\n",
            "2019-01-22T07:08:56.281398: step 6244, loss 1.35973e-07, acc 1\n",
            "2019-01-22T07:08:56.962583: step 6245, loss 0.0044502, acc 1\n",
            "2019-01-22T07:08:57.638476: step 6246, loss 1.32807e-05, acc 1\n",
            "2019-01-22T07:08:58.311833: step 6247, loss 0.000319644, acc 1\n",
            "2019-01-22T07:08:58.988198: step 6248, loss 4.09779e-07, acc 1\n",
            "2019-01-22T07:08:59.662321: step 6249, loss 0.00166631, acc 1\n",
            "2019-01-22T07:09:00.338929: step 6250, loss 2.04807e-05, acc 1\n",
            "2019-01-22T07:09:01.014881: step 6251, loss 6.84876e-05, acc 1\n",
            "2019-01-22T07:09:01.691330: step 6252, loss 0.000309055, acc 1\n",
            "2019-01-22T07:09:02.368197: step 6253, loss 4.2527e-05, acc 1\n",
            "2019-01-22T07:09:03.048453: step 6254, loss 0.000140765, acc 1\n",
            "2019-01-22T07:09:03.726402: step 6255, loss 3.68952e-06, acc 1\n",
            "2019-01-22T07:09:04.403935: step 6256, loss 9.31628e-05, acc 1\n",
            "2019-01-22T07:09:05.085926: step 6257, loss 8.34394e-05, acc 1\n",
            "2019-01-22T07:09:05.761120: step 6258, loss 0.0470713, acc 0.96875\n",
            "2019-01-22T07:09:06.437304: step 6259, loss 3.0361e-07, acc 1\n",
            "2019-01-22T07:09:07.115719: step 6260, loss 0.000655745, acc 1\n",
            "2019-01-22T07:09:07.790417: step 6261, loss 1.42745e-05, acc 1\n",
            "2019-01-22T07:09:08.466851: step 6262, loss 5.8439e-06, acc 1\n",
            "2019-01-22T07:09:09.142934: step 6263, loss 0.00195433, acc 1\n",
            "2019-01-22T07:09:09.819550: step 6264, loss 1.30385e-07, acc 1\n",
            "2019-01-22T07:09:10.491932: step 6265, loss 2.96158e-07, acc 1\n",
            "2019-01-22T07:09:11.163194: step 6266, loss 1.93715e-07, acc 1\n",
            "2019-01-22T07:09:11.835930: step 6267, loss 1.86265e-09, acc 1\n",
            "2019-01-22T07:09:12.513865: step 6268, loss 2.4028e-07, acc 1\n",
            "2019-01-22T07:09:13.185556: step 6269, loss 1.49011e-07, acc 1\n",
            "2019-01-22T07:09:13.595268: step 6270, loss 1.59737e-06, acc 1\n",
            "2019-01-22T07:09:14.268127: step 6271, loss 4.72639e-05, acc 1\n",
            "2019-01-22T07:09:14.943444: step 6272, loss 6.89178e-08, acc 1\n",
            "2019-01-22T07:09:15.620620: step 6273, loss 6.48276e-06, acc 1\n",
            "2019-01-22T07:09:16.292379: step 6274, loss 2.04891e-08, acc 1\n",
            "2019-01-22T07:09:16.968669: step 6275, loss 0.00227952, acc 1\n",
            "2019-01-22T07:09:17.649052: step 6276, loss 9.31322e-09, acc 1\n",
            "2019-01-22T07:09:18.323649: step 6277, loss 7.45058e-09, acc 1\n",
            "2019-01-22T07:09:19.000075: step 6278, loss 1.67257e-06, acc 1\n",
            "2019-01-22T07:09:19.670042: step 6279, loss 9.06388e-06, acc 1\n",
            "2019-01-22T07:09:20.348671: step 6280, loss 2.13655e-05, acc 1\n",
            "2019-01-22T07:09:21.025736: step 6281, loss 9.14106e-06, acc 1\n",
            "2019-01-22T07:09:21.701879: step 6282, loss 1.99294e-06, acc 1\n",
            "2019-01-22T07:09:22.378965: step 6283, loss 2.33376e-06, acc 1\n",
            "2019-01-22T07:09:23.054364: step 6284, loss 2.91107e-06, acc 1\n",
            "2019-01-22T07:09:23.727147: step 6285, loss 4.93597e-07, acc 1\n",
            "2019-01-22T07:09:24.402977: step 6286, loss 0.0149473, acc 0.984375\n",
            "2019-01-22T07:09:25.083356: step 6287, loss 1.86264e-08, acc 1\n",
            "2019-01-22T07:09:25.755392: step 6288, loss 2.23517e-08, acc 1\n",
            "2019-01-22T07:09:26.428131: step 6289, loss 1.695e-07, acc 1\n",
            "2019-01-22T07:09:27.100240: step 6290, loss 2.23517e-08, acc 1\n",
            "2019-01-22T07:09:27.783791: step 6291, loss 3.79975e-07, acc 1\n",
            "2019-01-22T07:09:28.457340: step 6292, loss 9.31322e-09, acc 1\n",
            "2019-01-22T07:09:29.130253: step 6293, loss 2.68219e-07, acc 1\n",
            "2019-01-22T07:09:29.818624: step 6294, loss 2.37628e-05, acc 1\n",
            "2019-01-22T07:09:30.493405: step 6295, loss 7.07804e-08, acc 1\n",
            "2019-01-22T07:09:31.163349: step 6296, loss 1.49012e-08, acc 1\n",
            "2019-01-22T07:09:31.842825: step 6297, loss 0.000662007, acc 1\n",
            "2019-01-22T07:09:32.520588: step 6298, loss 0.000142837, acc 1\n",
            "2019-01-22T07:09:33.196035: step 6299, loss 8.56816e-08, acc 1\n",
            "2019-01-22T07:09:33.868567: step 6300, loss 2.81258e-07, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:09:34.229353: step 6300, loss 3.31767e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-6300\n",
            "\n",
            "2019-01-22T07:09:35.052593: step 6301, loss 7.59945e-07, acc 1\n",
            "2019-01-22T07:09:35.730071: step 6302, loss 3.59487e-07, acc 1\n",
            "2019-01-22T07:09:36.136666: step 6303, loss 0, acc 1\n",
            "2019-01-22T07:09:36.810434: step 6304, loss 6.38878e-07, acc 1\n",
            "2019-01-22T07:09:37.482707: step 6305, loss 4.28408e-08, acc 1\n",
            "2019-01-22T07:09:38.150403: step 6306, loss 0.000419295, acc 1\n",
            "2019-01-22T07:09:38.827062: step 6307, loss 6.31615e-05, acc 1\n",
            "2019-01-22T07:09:39.496191: step 6308, loss 1.30385e-08, acc 1\n",
            "2019-01-22T07:09:40.169393: step 6309, loss 2.45711e-05, acc 1\n",
            "2019-01-22T07:09:40.851320: step 6310, loss 6.98478e-07, acc 1\n",
            "2019-01-22T07:09:41.524520: step 6311, loss 3.16466e-05, acc 1\n",
            "2019-01-22T07:09:42.207183: step 6312, loss 3.53305e-06, acc 1\n",
            "2019-01-22T07:09:42.882618: step 6313, loss 1.26816e-05, acc 1\n",
            "2019-01-22T07:09:43.563397: step 6314, loss 0.000226855, acc 1\n",
            "2019-01-22T07:09:44.246140: step 6315, loss 3.20548e-05, acc 1\n",
            "2019-01-22T07:09:44.918968: step 6316, loss 1.41751e-05, acc 1\n",
            "2019-01-22T07:09:45.595087: step 6317, loss 2.01748e-05, acc 1\n",
            "2019-01-22T07:09:46.282415: step 6318, loss 1.49012e-08, acc 1\n",
            "2019-01-22T07:09:46.956508: step 6319, loss 5.78618e-06, acc 1\n",
            "2019-01-22T07:09:47.629970: step 6320, loss 5.14083e-07, acc 1\n",
            "2019-01-22T07:09:48.311736: step 6321, loss 0.00503707, acc 1\n",
            "2019-01-22T07:09:48.986943: step 6322, loss 1.47517e-06, acc 1\n",
            "2019-01-22T07:09:49.658830: step 6323, loss 1.11759e-08, acc 1\n",
            "2019-01-22T07:09:50.330642: step 6324, loss 6.32803e-06, acc 1\n",
            "2019-01-22T07:09:51.006671: step 6325, loss 1.6465e-06, acc 1\n",
            "2019-01-22T07:09:51.686023: step 6326, loss 1.56462e-07, acc 1\n",
            "2019-01-22T07:09:52.361027: step 6327, loss 0.00030771, acc 1\n",
            "2019-01-22T07:09:53.034582: step 6328, loss 1.11759e-08, acc 1\n",
            "2019-01-22T07:09:53.706880: step 6329, loss 7.26424e-07, acc 1\n",
            "2019-01-22T07:09:54.381576: step 6330, loss 0.000277748, acc 1\n",
            "2019-01-22T07:09:55.057325: step 6331, loss 6.01628e-07, acc 1\n",
            "2019-01-22T07:09:55.729974: step 6332, loss 6.27706e-07, acc 1\n",
            "2019-01-22T07:09:56.407997: step 6333, loss 1.5776e-06, acc 1\n",
            "2019-01-22T07:09:57.083308: step 6334, loss 1.86264e-08, acc 1\n",
            "2019-01-22T07:09:57.755599: step 6335, loss 7.13883e-05, acc 1\n",
            "2019-01-22T07:09:58.161589: step 6336, loss 4.63525e-06, acc 1\n",
            "2019-01-22T07:09:58.833509: step 6337, loss 0.00598696, acc 1\n",
            "2019-01-22T07:09:59.510474: step 6338, loss 7.49357e-06, acc 1\n",
            "2019-01-22T07:10:00.184201: step 6339, loss 0.0882064, acc 0.984375\n",
            "2019-01-22T07:10:00.859646: step 6340, loss 1.44538e-06, acc 1\n",
            "2019-01-22T07:10:01.538347: step 6341, loss 6.18391e-07, acc 1\n",
            "2019-01-22T07:10:02.213336: step 6342, loss 8.53892e-05, acc 1\n",
            "2019-01-22T07:10:02.885468: step 6343, loss 3.46803e-06, acc 1\n",
            "2019-01-22T07:10:03.561527: step 6344, loss 2.80785e-05, acc 1\n",
            "2019-01-22T07:10:04.240446: step 6345, loss 0.0740393, acc 0.984375\n",
            "2019-01-22T07:10:04.914583: step 6346, loss 7.86424e-05, acc 1\n",
            "2019-01-22T07:10:05.585570: step 6347, loss 3.97286e-06, acc 1\n",
            "2019-01-22T07:10:06.261962: step 6348, loss 0.00056113, acc 1\n",
            "2019-01-22T07:10:06.940001: step 6349, loss 0.00136046, acc 1\n",
            "2019-01-22T07:10:07.619482: step 6350, loss 0.0101744, acc 1\n",
            "2019-01-22T07:10:08.297594: step 6351, loss 0.00491346, acc 1\n",
            "2019-01-22T07:10:08.977520: step 6352, loss 1.03003e-06, acc 1\n",
            "2019-01-22T07:10:09.651787: step 6353, loss 2.70176e-05, acc 1\n",
            "2019-01-22T07:10:10.325608: step 6354, loss 0.00042627, acc 1\n",
            "2019-01-22T07:10:11.011732: step 6355, loss 1.30385e-08, acc 1\n",
            "2019-01-22T07:10:11.685260: step 6356, loss 4.65661e-08, acc 1\n",
            "2019-01-22T07:10:12.356938: step 6357, loss 7.45057e-08, acc 1\n",
            "2019-01-22T07:10:13.039950: step 6358, loss 1.30385e-07, acc 1\n",
            "2019-01-22T07:10:13.711433: step 6359, loss 2.88485e-05, acc 1\n",
            "2019-01-22T07:10:14.394375: step 6360, loss 1.6205e-07, acc 1\n",
            "2019-01-22T07:10:15.071396: step 6361, loss 2.70082e-07, acc 1\n",
            "2019-01-22T07:10:15.746545: step 6362, loss 1.3269e-05, acc 1\n",
            "2019-01-22T07:10:16.421347: step 6363, loss 3.68802e-07, acc 1\n",
            "2019-01-22T07:10:17.100724: step 6364, loss 0.000181846, acc 1\n",
            "2019-01-22T07:10:17.776111: step 6365, loss 9.25004e-06, acc 1\n",
            "2019-01-22T07:10:18.446858: step 6366, loss 7.45058e-09, acc 1\n",
            "2019-01-22T07:10:19.119339: step 6367, loss 1.30385e-07, acc 1\n",
            "2019-01-22T07:10:19.795349: step 6368, loss 2.0431e-05, acc 1\n",
            "2019-01-22T07:10:20.202871: step 6369, loss 6.81196e-09, acc 1\n",
            "2019-01-22T07:10:20.876521: step 6370, loss 0.000955374, acc 1\n",
            "2019-01-22T07:10:21.552492: step 6371, loss 0.0157613, acc 0.984375\n",
            "2019-01-22T07:10:22.223805: step 6372, loss 0.000558473, acc 1\n",
            "2019-01-22T07:10:22.898269: step 6373, loss 3.40863e-07, acc 1\n",
            "2019-01-22T07:10:23.573652: step 6374, loss 2.30967e-07, acc 1\n",
            "2019-01-22T07:10:24.245812: step 6375, loss 2.10478e-07, acc 1\n",
            "2019-01-22T07:10:24.923673: step 6376, loss 4.47035e-08, acc 1\n",
            "2019-01-22T07:10:25.598897: step 6377, loss 3.64853e-06, acc 1\n",
            "2019-01-22T07:10:26.270921: step 6378, loss 2.99883e-07, acc 1\n",
            "2019-01-22T07:10:26.943958: step 6379, loss 7.45058e-09, acc 1\n",
            "2019-01-22T07:10:27.615834: step 6380, loss 2.40461e-06, acc 1\n",
            "2019-01-22T07:10:28.290796: step 6381, loss 3.17678e-05, acc 1\n",
            "2019-01-22T07:10:28.963328: step 6382, loss 0.00288963, acc 1\n",
            "2019-01-22T07:10:29.638581: step 6383, loss 5.2154e-08, acc 1\n",
            "2019-01-22T07:10:30.310234: step 6384, loss 1.86265e-09, acc 1\n",
            "2019-01-22T07:10:30.984676: step 6385, loss 1.11776e-05, acc 1\n",
            "2019-01-22T07:10:31.667033: step 6386, loss 2.58233e-05, acc 1\n",
            "2019-01-22T07:10:32.339454: step 6387, loss 1.63351e-06, acc 1\n",
            "2019-01-22T07:10:33.015387: step 6388, loss 8.96991e-06, acc 1\n",
            "2019-01-22T07:10:33.695333: step 6389, loss 2.96472e-05, acc 1\n",
            "2019-01-22T07:10:34.371798: step 6390, loss 0.0030987, acc 1\n",
            "2019-01-22T07:10:35.046283: step 6391, loss 4.22953e-06, acc 1\n",
            "2019-01-22T07:10:35.726644: step 6392, loss 9.12694e-08, acc 1\n",
            "2019-01-22T07:10:36.400428: step 6393, loss 0, acc 1\n",
            "2019-01-22T07:10:37.076845: step 6394, loss 3.16649e-08, acc 1\n",
            "2019-01-22T07:10:37.758084: step 6395, loss 0.000111942, acc 1\n",
            "2019-01-22T07:10:38.431653: step 6396, loss 2.66357e-07, acc 1\n",
            "2019-01-22T07:10:39.108781: step 6397, loss 1.3411e-07, acc 1\n",
            "2019-01-22T07:10:39.788346: step 6398, loss 0, acc 1\n",
            "2019-01-22T07:10:40.466897: step 6399, loss 8.00936e-08, acc 1\n",
            "2019-01-22T07:10:41.139268: step 6400, loss 5.4575e-07, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:10:41.499180: step 6400, loss 1.24778e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-6400\n",
            "\n",
            "2019-01-22T07:10:42.348331: step 6401, loss 0.00155451, acc 1\n",
            "2019-01-22T07:10:42.760507: step 6402, loss 0.000259488, acc 1\n",
            "2019-01-22T07:10:43.432590: step 6403, loss 1.51798e-06, acc 1\n",
            "2019-01-22T07:10:44.114447: step 6404, loss 4.1909e-07, acc 1\n",
            "2019-01-22T07:10:44.789507: step 6405, loss 5.4906e-06, acc 1\n",
            "2019-01-22T07:10:45.461718: step 6406, loss 2.61315e-05, acc 1\n",
            "2019-01-22T07:10:46.137689: step 6407, loss 0.00238378, acc 1\n",
            "2019-01-22T07:10:46.812747: step 6408, loss 1.91851e-07, acc 1\n",
            "2019-01-22T07:10:47.486653: step 6409, loss 0.000299157, acc 1\n",
            "2019-01-22T07:10:48.159415: step 6410, loss 2.60563e-06, acc 1\n",
            "2019-01-22T07:10:48.835810: step 6411, loss 1.78749e-05, acc 1\n",
            "2019-01-22T07:10:49.514285: step 6412, loss 1.11758e-07, acc 1\n",
            "2019-01-22T07:10:50.190377: step 6413, loss 0.000919506, acc 1\n",
            "2019-01-22T07:10:50.864759: step 6414, loss 1.49012e-08, acc 1\n",
            "2019-01-22T07:10:51.543819: step 6415, loss 2.97449e-06, acc 1\n",
            "2019-01-22T07:10:52.222540: step 6416, loss 0.00312549, acc 1\n",
            "2019-01-22T07:10:52.896305: step 6417, loss 1.86253e-06, acc 1\n",
            "2019-01-22T07:10:53.566853: step 6418, loss 4.94674e-06, acc 1\n",
            "2019-01-22T07:10:54.254583: step 6419, loss 2.2013e-05, acc 1\n",
            "2019-01-22T07:10:54.928746: step 6420, loss 1.95577e-07, acc 1\n",
            "2019-01-22T07:10:55.602129: step 6421, loss 5.31e-06, acc 1\n",
            "2019-01-22T07:10:56.280506: step 6422, loss 5.0129e-05, acc 1\n",
            "2019-01-22T07:10:56.963291: step 6423, loss 1.17346e-07, acc 1\n",
            "2019-01-22T07:10:57.638020: step 6424, loss 5.87334e-05, acc 1\n",
            "2019-01-22T07:10:58.309706: step 6425, loss 1.13621e-07, acc 1\n",
            "2019-01-22T07:10:58.984556: step 6426, loss 6.16525e-07, acc 1\n",
            "2019-01-22T07:10:59.664524: step 6427, loss 0.000262246, acc 1\n",
            "2019-01-22T07:11:00.336406: step 6428, loss 3.0361e-07, acc 1\n",
            "2019-01-22T07:11:01.010789: step 6429, loss 1.35596e-06, acc 1\n",
            "2019-01-22T07:11:01.693351: step 6430, loss 3.72529e-09, acc 1\n",
            "2019-01-22T07:11:02.365542: step 6431, loss 0, acc 1\n",
            "2019-01-22T07:11:03.039133: step 6432, loss 1.83648e-06, acc 1\n",
            "2019-01-22T07:11:03.712193: step 6433, loss 1.86265e-09, acc 1\n",
            "2019-01-22T07:11:04.387028: step 6434, loss 1.12501e-05, acc 1\n",
            "2019-01-22T07:11:04.796525: step 6435, loss 6.50535e-07, acc 1\n",
            "2019-01-22T07:11:05.470595: step 6436, loss 3.72529e-09, acc 1\n",
            "2019-01-22T07:11:06.145311: step 6437, loss 6.83576e-07, acc 1\n",
            "2019-01-22T07:11:06.820035: step 6438, loss 1.86265e-09, acc 1\n",
            "2019-01-22T07:11:07.493772: step 6439, loss 2.16239e-06, acc 1\n",
            "2019-01-22T07:11:08.165837: step 6440, loss 1.18476e-05, acc 1\n",
            "2019-01-22T07:11:08.840699: step 6441, loss 1.58324e-07, acc 1\n",
            "2019-01-22T07:11:09.522641: step 6442, loss 0, acc 1\n",
            "2019-01-22T07:11:10.197409: step 6443, loss 2.08236e-06, acc 1\n",
            "2019-01-22T07:11:10.869271: step 6444, loss 0, acc 1\n",
            "2019-01-22T07:11:11.544725: step 6445, loss 4.47035e-08, acc 1\n",
            "2019-01-22T07:11:12.225014: step 6446, loss 0.000853602, acc 1\n",
            "2019-01-22T07:11:12.898241: step 6447, loss 2.58906e-07, acc 1\n",
            "2019-01-22T07:11:13.569661: step 6448, loss 9.31322e-09, acc 1\n",
            "2019-01-22T07:11:14.242313: step 6449, loss 2.6077e-08, acc 1\n",
            "2019-01-22T07:11:14.918802: step 6450, loss 4.5819e-05, acc 1\n",
            "2019-01-22T07:11:15.592106: step 6451, loss 2.92088e-05, acc 1\n",
            "2019-01-22T07:11:16.264978: step 6452, loss 2.92434e-07, acc 1\n",
            "2019-01-22T07:11:16.948289: step 6453, loss 1.11759e-08, acc 1\n",
            "2019-01-22T07:11:17.620011: step 6454, loss 1.86265e-09, acc 1\n",
            "2019-01-22T07:11:18.293672: step 6455, loss 0.00215438, acc 1\n",
            "2019-01-22T07:11:18.975645: step 6456, loss 1.71173e-06, acc 1\n",
            "2019-01-22T07:11:19.657075: step 6457, loss 0.000193048, acc 1\n",
            "2019-01-22T07:11:20.343759: step 6458, loss 1.84259e-05, acc 1\n",
            "2019-01-22T07:11:21.022090: step 6459, loss 1.41299e-05, acc 1\n",
            "2019-01-22T07:11:21.698778: step 6460, loss 0.00195332, acc 1\n",
            "2019-01-22T07:11:22.375122: step 6461, loss 0.000619079, acc 1\n",
            "2019-01-22T07:11:23.054883: step 6462, loss 6.22372e-06, acc 1\n",
            "2019-01-22T07:11:23.734961: step 6463, loss 3.76251e-07, acc 1\n",
            "2019-01-22T07:11:24.412845: step 6464, loss 3.53902e-08, acc 1\n",
            "2019-01-22T07:11:25.084394: step 6465, loss 8.38778e-05, acc 1\n",
            "2019-01-22T07:11:25.758342: step 6466, loss 3.66217e-05, acc 1\n",
            "2019-01-22T07:11:26.432965: step 6467, loss 4.43255e-06, acc 1\n",
            "2019-01-22T07:11:26.843108: step 6468, loss 1.16121e-05, acc 1\n",
            "2019-01-22T07:11:27.517555: step 6469, loss 5.54539e-05, acc 1\n",
            "2019-01-22T07:11:28.203296: step 6470, loss 0.00380238, acc 1\n",
            "2019-01-22T07:11:28.877907: step 6471, loss 4.6895e-06, acc 1\n",
            "2019-01-22T07:11:29.551318: step 6472, loss 2.26298e-06, acc 1\n",
            "2019-01-22T07:11:30.227362: step 6473, loss 9.49946e-08, acc 1\n",
            "2019-01-22T07:11:30.908832: step 6474, loss 0.000173385, acc 1\n",
            "2019-01-22T07:11:31.583293: step 6475, loss 9.31322e-09, acc 1\n",
            "2019-01-22T07:11:32.257804: step 6476, loss 9.14533e-07, acc 1\n",
            "2019-01-22T07:11:32.928226: step 6477, loss 4.82419e-07, acc 1\n",
            "2019-01-22T07:11:33.603851: step 6478, loss 0.00109572, acc 1\n",
            "2019-01-22T07:11:34.279436: step 6479, loss 2.39541e-05, acc 1\n",
            "2019-01-22T07:11:34.956853: step 6480, loss 8.05945e-06, acc 1\n",
            "2019-01-22T07:11:35.633824: step 6481, loss 8.67511e-05, acc 1\n",
            "2019-01-22T07:11:36.309738: step 6482, loss 1.22934e-07, acc 1\n",
            "2019-01-22T07:11:36.984537: step 6483, loss 2.23517e-08, acc 1\n",
            "2019-01-22T07:11:37.658252: step 6484, loss 0.00276644, acc 1\n",
            "2019-01-22T07:11:38.344393: step 6485, loss 1.14735e-06, acc 1\n",
            "2019-01-22T07:11:39.020120: step 6486, loss 0.000528316, acc 1\n",
            "2019-01-22T07:11:39.694950: step 6487, loss 4.70008e-05, acc 1\n",
            "2019-01-22T07:11:40.371597: step 6488, loss 8.90329e-07, acc 1\n",
            "2019-01-22T07:11:41.050483: step 6489, loss 2.23517e-08, acc 1\n",
            "2019-01-22T07:11:41.721418: step 6490, loss 0.0218063, acc 0.984375\n",
            "2019-01-22T07:11:42.399005: step 6491, loss 2.79394e-07, acc 1\n",
            "2019-01-22T07:11:43.072273: step 6492, loss 1.86283e-05, acc 1\n",
            "2019-01-22T07:11:43.755427: step 6493, loss 0.000159324, acc 1\n",
            "2019-01-22T07:11:44.431087: step 6494, loss 3.35276e-08, acc 1\n",
            "2019-01-22T07:11:45.105040: step 6495, loss 1.08114e-05, acc 1\n",
            "2019-01-22T07:11:45.790644: step 6496, loss 0.00145498, acc 1\n",
            "2019-01-22T07:11:46.470965: step 6497, loss 4.10028e-05, acc 1\n",
            "2019-01-22T07:11:47.142718: step 6498, loss 1.82657e-05, acc 1\n",
            "2019-01-22T07:11:47.826447: step 6499, loss 1.3724e-05, acc 1\n",
            "2019-01-22T07:11:48.505914: step 6500, loss 0.00493689, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:11:48.864751: step 6500, loss 0.00166154, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-6500\n",
            "\n",
            "2019-01-22T07:11:49.415021: step 6501, loss 3.95092e-07, acc 1\n",
            "2019-01-22T07:11:50.091595: step 6502, loss 0.277269, acc 0.984375\n",
            "2019-01-22T07:11:50.766367: step 6503, loss 8.60092e-05, acc 1\n",
            "2019-01-22T07:11:51.437757: step 6504, loss 1.30385e-08, acc 1\n",
            "2019-01-22T07:11:52.110960: step 6505, loss 8.91849e-06, acc 1\n",
            "2019-01-22T07:11:52.790009: step 6506, loss 2.27233e-06, acc 1\n",
            "2019-01-22T07:11:53.462415: step 6507, loss 4.2654e-07, acc 1\n",
            "2019-01-22T07:11:54.132372: step 6508, loss 1.54033e-06, acc 1\n",
            "2019-01-22T07:11:54.814324: step 6509, loss 6.66813e-07, acc 1\n",
            "2019-01-22T07:11:55.489344: step 6510, loss 0.000565189, acc 1\n",
            "2019-01-22T07:11:56.166584: step 6511, loss 6.79851e-07, acc 1\n",
            "2019-01-22T07:11:56.839632: step 6512, loss 0.000119173, acc 1\n",
            "2019-01-22T07:11:57.518759: step 6513, loss 9.31321e-08, acc 1\n",
            "2019-01-22T07:11:58.191506: step 6514, loss 4.84287e-08, acc 1\n",
            "2019-01-22T07:11:58.866405: step 6515, loss 0.000607595, acc 1\n",
            "2019-01-22T07:11:59.538639: step 6516, loss 6.30937e-06, acc 1\n",
            "2019-01-22T07:12:00.217531: step 6517, loss 3.72529e-09, acc 1\n",
            "2019-01-22T07:12:00.892747: step 6518, loss 0.00149479, acc 1\n",
            "2019-01-22T07:12:01.569997: step 6519, loss 2.42144e-08, acc 1\n",
            "2019-01-22T07:12:02.255703: step 6520, loss 3.91155e-08, acc 1\n",
            "2019-01-22T07:12:02.933454: step 6521, loss 8.57407e-05, acc 1\n",
            "2019-01-22T07:12:03.604060: step 6522, loss 9.3602e-06, acc 1\n",
            "2019-01-22T07:12:04.285914: step 6523, loss 0.000143709, acc 1\n",
            "2019-01-22T07:12:04.961954: step 6524, loss 5.58793e-08, acc 1\n",
            "2019-01-22T07:12:05.634032: step 6525, loss 1.86265e-09, acc 1\n",
            "2019-01-22T07:12:06.310251: step 6526, loss 0.000198642, acc 1\n",
            "2019-01-22T07:12:06.987603: step 6527, loss 2.98022e-07, acc 1\n",
            "2019-01-22T07:12:07.664752: step 6528, loss 0.0646683, acc 0.984375\n",
            "2019-01-22T07:12:08.337577: step 6529, loss 1.81521e-05, acc 1\n",
            "2019-01-22T07:12:09.010518: step 6530, loss 1.81225e-06, acc 1\n",
            "2019-01-22T07:12:09.682043: step 6531, loss 1.13061e-06, acc 1\n",
            "2019-01-22T07:12:10.356821: step 6532, loss 2.39518e-06, acc 1\n",
            "2019-01-22T07:12:11.035430: step 6533, loss 0.00545959, acc 1\n",
            "2019-01-22T07:12:11.446496: step 6534, loss 7.49315e-08, acc 1\n",
            "2019-01-22T07:12:12.121668: step 6535, loss 6.418e-05, acc 1\n",
            "2019-01-22T07:12:12.797276: step 6536, loss 1.09896e-07, acc 1\n",
            "2019-01-22T07:12:13.475336: step 6537, loss 3.70665e-07, acc 1\n",
            "2019-01-22T07:12:14.149557: step 6538, loss 9.25708e-07, acc 1\n",
            "2019-01-22T07:12:14.826333: step 6539, loss 6.40815e-06, acc 1\n",
            "2019-01-22T07:12:15.506310: step 6540, loss 3.49958e-06, acc 1\n",
            "2019-01-22T07:12:16.182721: step 6541, loss 4.2053e-06, acc 1\n",
            "2019-01-22T07:12:16.857470: step 6542, loss 2.97587e-05, acc 1\n",
            "2019-01-22T07:12:17.532290: step 6543, loss 2.49593e-07, acc 1\n",
            "2019-01-22T07:12:18.212809: step 6544, loss 0.000120903, acc 1\n",
            "2019-01-22T07:12:18.891703: step 6545, loss 0.0450282, acc 0.96875\n",
            "2019-01-22T07:12:19.567753: step 6546, loss 5.58793e-08, acc 1\n",
            "2019-01-22T07:12:20.243279: step 6547, loss 1.62049e-07, acc 1\n",
            "2019-01-22T07:12:20.926602: step 6548, loss 0.00313155, acc 1\n",
            "2019-01-22T07:12:21.604404: step 6549, loss 1.29505e-05, acc 1\n",
            "2019-01-22T07:12:22.278810: step 6550, loss 0.002031, acc 1\n",
            "2019-01-22T07:12:22.953907: step 6551, loss 1.65953e-06, acc 1\n",
            "2019-01-22T07:12:23.640466: step 6552, loss 0.00264474, acc 1\n",
            "2019-01-22T07:12:24.319556: step 6553, loss 2.19792e-07, acc 1\n",
            "2019-01-22T07:12:24.994048: step 6554, loss 1.30384e-06, acc 1\n",
            "2019-01-22T07:12:25.669278: step 6555, loss 1.5599e-05, acc 1\n",
            "2019-01-22T07:12:26.349544: step 6556, loss 4.45897e-05, acc 1\n",
            "2019-01-22T07:12:27.027967: step 6557, loss 1.90187e-05, acc 1\n",
            "2019-01-22T07:12:27.703459: step 6558, loss 5.48645e-06, acc 1\n",
            "2019-01-22T07:12:28.376948: step 6559, loss 1.50874e-07, acc 1\n",
            "2019-01-22T07:12:29.067555: step 6560, loss 0.0478146, acc 0.984375\n",
            "2019-01-22T07:12:29.741819: step 6561, loss 3.10135e-05, acc 1\n",
            "2019-01-22T07:12:30.418751: step 6562, loss 8.54535e-06, acc 1\n",
            "2019-01-22T07:12:31.101063: step 6563, loss 8.56816e-08, acc 1\n",
            "2019-01-22T07:12:31.774919: step 6564, loss 1.32247e-07, acc 1\n",
            "2019-01-22T07:12:32.446026: step 6565, loss 2.98023e-08, acc 1\n",
            "2019-01-22T07:12:33.116747: step 6566, loss 0.000227103, acc 1\n",
            "2019-01-22T07:12:33.529410: step 6567, loss 6.13076e-08, acc 1\n",
            "2019-01-22T07:12:34.212824: step 6568, loss 1.86264e-08, acc 1\n",
            "2019-01-22T07:12:34.888438: step 6569, loss 9.3132e-08, acc 1\n",
            "2019-01-22T07:12:35.563237: step 6570, loss 7.15241e-07, acc 1\n",
            "2019-01-22T07:12:36.245467: step 6571, loss 3.72529e-09, acc 1\n",
            "2019-01-22T07:12:36.920780: step 6572, loss 4.33381e-06, acc 1\n",
            "2019-01-22T07:12:37.598137: step 6573, loss 6.14672e-08, acc 1\n",
            "2019-01-22T07:12:38.284752: step 6574, loss 4.14198e-06, acc 1\n",
            "2019-01-22T07:12:38.958537: step 6575, loss 1.43233e-06, acc 1\n",
            "2019-01-22T07:12:39.631098: step 6576, loss 0.00013419, acc 1\n",
            "2019-01-22T07:12:40.310657: step 6577, loss 4.91103e-06, acc 1\n",
            "2019-01-22T07:12:40.983925: step 6578, loss 2.16066e-07, acc 1\n",
            "2019-01-22T07:12:41.657955: step 6579, loss 1.58324e-07, acc 1\n",
            "2019-01-22T07:12:42.332461: step 6580, loss 1.30194e-06, acc 1\n",
            "2019-01-22T07:12:43.009786: step 6581, loss 7.63684e-08, acc 1\n",
            "2019-01-22T07:12:43.683961: step 6582, loss 0.000282614, acc 1\n",
            "2019-01-22T07:12:44.357420: step 6583, loss 6.27699e-07, acc 1\n",
            "2019-01-22T07:12:45.035151: step 6584, loss 5.84409e-05, acc 1\n",
            "2019-01-22T07:12:45.711812: step 6585, loss 0, acc 1\n",
            "2019-01-22T07:12:46.383768: step 6586, loss 1.11759e-08, acc 1\n",
            "2019-01-22T07:12:47.056442: step 6587, loss 2.27792e-06, acc 1\n",
            "2019-01-22T07:12:47.735427: step 6588, loss 2.0489e-07, acc 1\n",
            "2019-01-22T07:12:48.410118: step 6589, loss 1.13709e-05, acc 1\n",
            "2019-01-22T07:12:49.083025: step 6590, loss 2.98023e-08, acc 1\n",
            "2019-01-22T07:12:49.753597: step 6591, loss 2.12341e-07, acc 1\n",
            "2019-01-22T07:12:50.430417: step 6592, loss 1.61487e-06, acc 1\n",
            "2019-01-22T07:12:51.101906: step 6593, loss 2.03767e-06, acc 1\n",
            "2019-01-22T07:12:51.775688: step 6594, loss 9.12694e-08, acc 1\n",
            "2019-01-22T07:12:52.449797: step 6595, loss 2.42144e-08, acc 1\n",
            "2019-01-22T07:12:53.125554: step 6596, loss 0.000109346, acc 1\n",
            "2019-01-22T07:12:53.800543: step 6597, loss 2.37477e-06, acc 1\n",
            "2019-01-22T07:12:54.475309: step 6598, loss 2.35871e-05, acc 1\n",
            "2019-01-22T07:12:55.154178: step 6599, loss 9.31321e-08, acc 1\n",
            "2019-01-22T07:12:55.563642: step 6600, loss 0.000131063, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-01-22T07:12:55.925724: step 6600, loss 1.43206e-05, acc 1\n",
            "\n",
            "Saved model checkpoint to /content/runs/1548136699/checkpoints/model-6600\n",
            "\n",
            "\n",
            "Deleting Following keys...\n",
            "dev_sample_percentage\n",
            "positive_data_file\n",
            "negative_data_file\n",
            "embedding_dim\n",
            "filter_sizes\n",
            "num_filters\n",
            "dropout_keep_prob\n",
            "l2_reg_lambda\n",
            "batch_size\n",
            "num_epochs\n",
            "evaluate_every\n",
            "checkpoint_every\n",
            "num_checkpoints\n",
            "allow_soft_placement\n",
            "log_device_placement\n",
            "model_path\n",
            "\n",
            "Created Following keys...\n",
            "test_data_file\n",
            "checkpoint_dir\n",
            "eval_train\n",
            "batch_size\n",
            "allow_soft_placement\n",
            "log_device_placement\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/runs/1548136699/checkpoints/model-6600\n",
            "Saving evaluation to /content/prediction.csv\n",
            "Saving prediction descriptions to /content/prediction-description.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zCJW1smSJYvt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download prediction.csv file\n",
        "files.download(\"prediction.csv\")\n",
        "files.download(\"prediction-description.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6bXwM5P3aqDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}